{
  "Title": "[07]",
  "Content": "\nWhen a block is committed, its hash will be stored in `storedBlockHashes`:<br>\n[Executor.sol#L164](https://github.com/code-423n4/2022-10-zksync/blob/358f38736351a8a27e325dfcb665eeba5ec02bd5/ethereum/contracts/zksync/facets/Executor.sol#L164)<br>\n\nIf this block is reverted, it is not removed from `storedBlockHashes`:<br>\n[Executor.sol#L336](https://github.com/code-423n4/2022-10-zksync/blob/358f38736351a8a27e325dfcb665eeba5ec02bd5/ethereum/contracts/zksync/facets/Executor.sol#L336)<br>\n\nThe vulnerability is that in the `GettersFacet`, the function `storedBlockHash(...)` will return the hash of a reverted block if this block number is given as its parameter, while it should return 0.<br>\n[Getters.sol#L86](https://github.com/code-423n4/2022-10-zksync/blob/358f38736351a8a27e325dfcb665eeba5ec02bd5/ethereum/contracts/zksync/facets/Getters.sol#L86)<br>\n\n**[Alex the Entreprenerd (judge) commented](https://github.com/code-423n4/2022-10-zksync-findings/issues/49#issuecomment-1325835625):**\n > **The critical parameters in initialize(...) are not set safely:**<br>\n> \n> Logically equivalent to address(0) check, Low\n> \n> **approveEmergencyDiamondCutAsSecurityCouncilMemberBySignature**<br>\n>\n> Not sure what this means, but if it means one caller gets to approve emerengency cuts, this is a glaring security risk.\n> \n> **Better to have config facet, in case some update is needed in the config.sol**<br>\n>\n> Not convinced by this one either, ultimately it's called config but it's just a bunch of contact / base contract\n> \n> **L2_LOG_BYTES is not correct, it should be `L2_TO_L1_LOG_SERIALIZE_SIZE`**<br>\n>\n> Non-Critical\n> \n> **It is not needed to have modifier senderCanCallFunction for the function deposit in both L1ERC20Bridge and L1ETHBridge, because they call the function requestL2Transaction in the MailBox that has already such modifier.**<br>\n>\n> Not convinced in lack of detail, if you call contract X and contract X calls contract Y, then the check is necessary on both contracts\n> \n> **For each deposit of an ERC20 token, the information**<br>\n>\n> Unclear what you'd do and where the savings would be.\n> \n> **When a block is committed, its hash will be stored in storedBlockHashes:**\n>\n> See [`#204`](https://github.com/code-423n4/2022-10-zksync-findings/issues/204), Low\n\n**[Alex the Entreprenerd (judge) commented](https://github.com/code-423n4/2022-10-zksync-findings/issues/49#issuecomment-1335920135):**\n > 2 Low, 1 Non-Critical\n>\n > Report was pretty good, but the downgraded findings add a lot of points to this QA.\n> \n> While the Warden has sent a few false positives, I think the value they offered for this contest warrants them winning the best QA report.\n\n*(Note: please see warden's [original submission](https://github.com/code-423n4/2022-10-zksync-findings/issues/49) for links to the referenced downgraded findings)*\n\n**[miladpiri (zkSync) commented](https://github.com/code-423n4/2022-10-zksync-findings/issues/49#issuecomment-1336268291):**\n >  Thanks, @Alex the Entreprenerd for your comment, that we are now looking at this report more seriously.<br>\n>\n> IMO, No.2 “approveEmergencyDiamondCutAsSecurityCouncilMemberBySignature“ can be a security issue. This is what we are now implementing to make security council members be able to approve the proposal by signature.<br>\n>\n> The security issue is that in case the security council members do not have access to approve on-chain, the governor must wait for their approval to bypass the notice period. In case the proposal should be executed instantly (in case of a critical bug in our protocol), this delay can be dangerous to our protocol.<br>\n>\n> By having approval by signature, the security members can approve a proposal off-chain, and the governor can execute the instant upgrade on behalf of them by using their signatures, so all-time access to the chain is not necessary for security council members.<br>\n>\n> Moreover, in case a critical proposal should be executed silently, it is necessary to be able to approve by signature. Because, the governor, in one batch transaction, proposes the proposal, and approves the upgrade on behalf of the security council members by using their signatures, and then executes the proposal. Since all the above actions are done in one transaction, there is no security risk of vulnerability leak. But, in the current structure, when the governor proposes the proposal, should wait for security members’ approval, this delay can be transparent to a malicious user to investigate the governor’s proposal, and gets a clue what the vulnerability is and then exploits the protocol.<br>\n>\n> This report worths to be upgraded in terms of severity!\n> Thanks.\n\n**[Alex the Entreprenerd (judge) commented](https://github.com/code-423n4/2022-10-zksync-findings/issues/49#issuecomment-1340174006):**\n> Thank you for your insight, I have sent the contest to triage, took note of your feedback and am sharing with other Judges and Wardens.\n\n**[Alex the Entreprenerd (judge) commented](https://github.com/code-423n4/2022-10-zksync-findings/issues/49#issuecomment-1343557528):**\n > Similarly to [`#48`](https://github.com/code-423n4/2022-10-zksync-findings/issues/48), I have shared this finding with 2 Judges and a Top warden and we all agree that this is effectively the same finding, with similar impact.\n> \n> As such Low Severity is the most appropriate.\n\n**[miladpiri (zkSync) commented](https://github.com/code-423n4/2022-10-zksync-findings/issues/49#issuecomment-1343993682):**\n > Thanks for the follow-up!\n\n\n\n***\n\n",
  "Impact": "LOW",
  "Source": "https://code4rena.com/reports/2022-10-zksync",
  "Code": [
    {
      "filename": "ethereum/contracts/zksync/facets/Executor.sol",
      "content": "// SPDX-License-Identifier: MIT OR Apache-2.0\n\npragma solidity ^0.8.0;\n\nimport \"./Base.sol\";\nimport \"../Config.sol\";\nimport \"../interfaces/IExecutor.sol\";\nimport \"../libraries/PairingsBn254.sol\";\nimport \"../libraries/PriorityQueue.sol\";\nimport \"../../common/libraries/UncheckedMath.sol\";\nimport \"../../common/libraries/UnsafeBytes.sol\";\nimport \"../../common/L2ContractHelper.sol\";\n\n/// @title zkSync Executor contract capable of processing events emitted in the zkSync protocol.\n/// @author Matter Labs\ncontract ExecutorFacet is Base, IExecutor {\n    using UncheckedMath for uint256;\n    using PriorityQueue for PriorityQueue.Queue;\n\n    /// @dev Process one block commit using the previous block StoredBlockInfo\n    /// @dev returns new block StoredBlockInfo\n    /// @notice Does not change storage\n    function _commitOneBlock(StoredBlockInfo memory _previousBlock, CommitBlockInfo calldata _newBlock)\n        internal\n        view\n        returns (StoredBlockInfo memory storedNewBlock)\n    {\n        require(_newBlock.blockNumber == _previousBlock.blockNumber + 1, \"f\"); // only commit next block\n\n        // Check that block contain all meta information for L2 logs.\n        // Get the chained hash of priority transaction hashes.\n        (\n            uint256 expectedNumberOfLayer1Txs,\n            bytes32 expectedPriorityOperationsHash,\n            bytes32 previousBlockHash,\n            uint256 l2BlockTimestamp\n        ) = _processL2Logs(_newBlock);\n\n        require(_previousBlock.blockHash == previousBlockHash, \"l\");\n        // Check that the priority operation hash in the L2 logs is as expected\n        require(expectedPriorityOperationsHash == _newBlock.priorityOperationsHash, \"t\");\n        // Check that the number of processed priority operations is as expected\n        require(expectedNumberOfLayer1Txs == _newBlock.numberOfLayer1Txs);\n        // Check that the timestamp that came from the Bootloader is expected\n        require(l2BlockTimestamp == _newBlock.timestamp);\n\n        // Preventing \"stack too deep error\"\n        {\n            // Check the timestamp of the new block\n            bool timestampNotTooSmall = block.timestamp - COMMIT_TIMESTAMP_NOT_OLDER <= l2BlockTimestamp;\n            bool timestampNotTooBig = l2BlockTimestamp <= block.timestamp + COMMIT_TIMESTAMP_APPROXIMATION_DELTA;\n            require(timestampNotTooSmall, \"h\"); // New block timestamp is too small\n            require(timestampNotTooBig, \"h1\"); // New block timestamp is too big\n\n            // Check the index of repeated storage writes\n            uint256 newStorageChangesIndexes = uint256(uint32(bytes4(_newBlock.initialStorageChanges[:4])));\n            require(\n                _previousBlock.indexRepeatedStorageChanges + newStorageChangesIndexes ==\n                    _newBlock.indexRepeatedStorageChanges,\n                \"yq\"\n            );\n        }\n\n        // Create block commitment for the proof verification\n        bytes32 commitment = _createBlockCommitment(_newBlock);\n\n        return\n            StoredBlockInfo(\n                _newBlock.blockNumber,\n                _newBlock.newStateRoot,\n                _newBlock.indexRepeatedStorageChanges,\n                _newBlock.numberOfLayer1Txs,\n                _newBlock.priorityOperationsHash,\n                _newBlock.l2LogsTreeRoot,\n                _newBlock.timestamp,\n                commitment\n            );\n    }\n\n    function _calculateBlockHash(StoredBlockInfo memory _previousBlock, CommitBlockInfo calldata _newBlock)\n        internal\n        pure\n        returns (bytes32)\n    {\n        return keccak256(abi.encode(_previousBlock.blockHash, _newBlock.newStateRoot));\n    }\n\n    /// @dev Check that L2 logs are proper and block contain all meta information for them\n    function _processL2Logs(CommitBlockInfo calldata _newBlock)\n        internal\n        pure\n        returns (\n            uint256 numberOfLayer1Txs,\n            bytes32 chainedPriorityTxsHash,\n            bytes32 previousBlockHash,\n            uint256 blockTimestamp\n        )\n    {\n        // Copy L2 to L1 logs into memory.\n        bytes memory emittedL2Logs = _newBlock.l2Logs[4:];\n        bytes[] calldata l2Messages = _newBlock.l2ArbitraryLengthMessages;\n        uint256 currentMessage;\n        // Auxiliary variable that is needed to enforce that `previousBlockHash` and `blockTimestamp` was read exactly one time\n        bool isSystemContextLogProcessed;\n        bytes[] calldata factoryDeps = _newBlock.factoryDeps;\n        uint256 currentBytecode;\n\n        chainedPriorityTxsHash = EMPTY_STRING_KECCAK;\n\n        // linear traversal of the logs\n        for (uint256 i = 0; i < emittedL2Logs.length; i = i.uncheckedAdd(L2_TO_L1_LOG_SERIALIZE_SIZE)) {\n            (address logSender, ) = UnsafeBytes.readAddress(emittedL2Logs, i + 4);\n\n            // show preimage for hashed message stored in log\n            if (logSender == L2_TO_L1_MESSENGER) {\n                (bytes32 hashedMessage, ) = UnsafeBytes.readBytes32(emittedL2Logs, i + 56);\n                require(keccak256(l2Messages[currentMessage]) == hashedMessage, \"k2\");\n\n                currentMessage = currentMessage.uncheckedInc();\n            } else if (logSender == L2_BOOTLOADER_ADDRESS) {\n                (bytes32 canonicalTxHash, ) = UnsafeBytes.readBytes32(emittedL2Logs, i + 24);\n                chainedPriorityTxsHash = keccak256(abi.encode(chainedPriorityTxsHash, canonicalTxHash));\n\n                // Overflow is not realistic\n                numberOfLayer1Txs = numberOfLayer1Txs.uncheckedInc();\n            } else if (logSender == L2_SYSTEM_CONTEXT_ADDRESS) {\n                // Make sure that the system context log wasn't processed yet, to\n                // avoid accident double reading `blockTimestamp` and `previousBlockHash`\n                require(!isSystemContextLogProcessed, \"fx\");\n                (blockTimestamp, ) = UnsafeBytes.readUint256(emittedL2Logs, i + 24);\n                (previousBlockHash, ) = UnsafeBytes.readBytes32(emittedL2Logs, i + 56);\n                // Mark system context log as processed\n                isSystemContextLogProcessed = true;\n            } else if (logSender == L2_KNOWN_CODE_STORAGE_ADDRESS) {\n                (bytes32 bytecodeHash, ) = UnsafeBytes.readBytes32(emittedL2Logs, i + 24);\n                require(bytecodeHash == L2ContractHelper.hashL2Bytecode(factoryDeps[currentBytecode]), \"k3\");\n\n                currentBytecode = currentBytecode.uncheckedInc();\n            }\n        }\n        // To check that only relevant preimages have been included in the calldata\n        require(currentBytecode == factoryDeps.length, \"ym\");\n        require(currentMessage == l2Messages.length, \"pl\");\n        // `blockTimestamp` and `previousBlockHash` wasn't read from L2 logs\n        require(isSystemContextLogProcessed, \"by\");\n    }\n\n    /// @notice Commit block\n    /// @notice 1. Checks timestamp.\n    /// @notice 2. Process L2 logs.\n    /// @notice 3. Store block commitments.\n    function commitBlocks(StoredBlockInfo memory _lastCommittedBlockData, CommitBlockInfo[] calldata _newBlocksData)\n        external\n        override\n        nonReentrant\n        onlyValidator\n    {\n        // Check that we commit blocks after last committed block\n        require(s.storedBlockHashes[s.totalBlocksCommitted] == _hashStoredBlockInfo(_lastCommittedBlockData), \"i\"); // incorrect previous block data\n\n        uint256 blocksLength = _newBlocksData.length;\n        for (uint256 i = 0; i < blocksLength; i = i.uncheckedInc()) {\n            _lastCommittedBlockData = _commitOneBlock(_lastCommittedBlockData, _newBlocksData[i]);\n            s.storedBlockHashes[_lastCommittedBlockData.blockNumber] = _hashStoredBlockInfo(_lastCommittedBlockData);\n\n            emit BlockCommit(\n                _lastCommittedBlockData.blockNumber,\n                _lastCommittedBlockData.blockHash,\n                _lastCommittedBlockData.commitment\n            );\n        }\n\n        s.totalBlocksCommitted = s.totalBlocksCommitted + blocksLength;\n    }\n\n    /// @dev Pops the priority operations from the priority queue and returns a rolling hash of operations\n    function _collectOperationsFromPriorityQueue(uint256 _nPriorityOps) internal returns (bytes32 concatHash) {\n        concatHash = EMPTY_STRING_KECCAK;\n\n        for (uint256 i = 0; i < _nPriorityOps; i = i.uncheckedInc()) {\n            PriorityOperation memory priorityOp = s.priorityQueue.popFront();\n            concatHash = keccak256(abi.encode(concatHash, priorityOp.canonicalTxHash));\n        }\n    }\n\n    /// @dev Executes one block\n    /// @dev 1. Processes all pending operations (Complete priority requests)\n    /// @dev 2. Finalizes block on Ethereum\n    /// @dev _executedBlockIdx is an index in the array of the blocks that we want to execute together\n    function _executeOneBlock(StoredBlockInfo memory _storedBlock, uint256 _executedBlockIdx) internal {\n        uint256 currentBlockNumber = _storedBlock.blockNumber;\n        require(currentBlockNumber == s.totalBlocksExecuted + _executedBlockIdx + 1, \"k\"); // Execute blocks in order\n        require(\n            _hashStoredBlockInfo(_storedBlock) == s.storedBlockHashes[currentBlockNumber],\n            \"exe10\" // executing block should be committed\n        );\n\n        bytes32 priorityOperationsHash = _collectOperationsFromPriorityQueue(_storedBlock.numberOfLayer1Txs);\n        require(priorityOperationsHash == _storedBlock.priorityOperationsHash, \"x\"); // priority operations hash does not match to expected\n\n        // Save root hash of L2 -> L1 logs tree\n        s.l2LogsRootHashes[currentBlockNumber] = _storedBlock.l2LogsTreeRoot;\n    }\n\n    /// @notice Execute blocks, complete priority operations and process withdrawals.\n    /// @notice 1. Processes all pending operations (Complete priority requests)\n    /// @notice 2. Finalizes block on Ethereum\n    function executeBlocks(StoredBlockInfo[] calldata _blocksData) external nonReentrant onlyValidator {\n        uint256 nBlocks = _blocksData.length;\n        for (uint256 i = 0; i < nBlocks; i = i.uncheckedInc()) {\n            _executeOneBlock(_blocksData[i], i);\n            emit BlockExecution(_blocksData[i].blockNumber, _blocksData[i].blockHash, _blocksData[i].commitment);\n        }\n\n        s.totalBlocksExecuted = s.totalBlocksExecuted + nBlocks;\n        require(s.totalBlocksExecuted <= s.totalBlocksVerified, \"n\"); // Can't execute blocks more then committed and proven currently.\n    }\n\n    /// @notice Blocks commitment verification.\n    /// @notice Only verifies block commitments without any other processing\n    function proveBlocks(\n        StoredBlockInfo calldata _prevBlock,\n        StoredBlockInfo[] calldata _committedBlocks,\n        ProofInput calldata _proof\n    ) external nonReentrant onlyValidator {\n        // Save the variables into the stack to save gas on reading them later\n        uint256 currentTotalBlocksVerified = s.totalBlocksVerified;\n        uint256 committedBlocksLength = _committedBlocks.length;\n\n        // Save the variable from the storage to memory to save gas\n        VerifierParams memory verifierParams = s.verifierParams;\n\n        // Initialize the array, that will be used as public input to the ZKP\n        uint256[] memory proofPublicInput = new uint256[](committedBlocksLength);\n\n        // Check that the block passed by the validator is indeed the first unverified block\n        require(_hashStoredBlockInfo(_prevBlock) == s.storedBlockHashes[currentTotalBlocksVerified], \"t1\");\n\n        bytes32 prevBlockCommitment = _prevBlock.commitment;\n        for (uint256 i = 0; i < committedBlocksLength; i = i.uncheckedInc()) {\n            require(\n                _hashStoredBlockInfo(_committedBlocks[i]) ==\n                    s.storedBlockHashes[currentTotalBlocksVerified.uncheckedInc()],\n                \"o1\"\n            );\n\n            bytes32 currentBlockCommitment = _committedBlocks[i].commitment;\n            proofPublicInput[i] = _getBlockProofPublicInput(\n                prevBlockCommitment,\n                currentBlockCommitment,\n                _proof,\n                verifierParams\n            );\n\n            prevBlockCommitment = currentBlockCommitment;\n            currentTotalBlocksVerified = currentTotalBlocksVerified.uncheckedInc();\n        }\n\n        // #if DUMMY_VERIFIER == false\n        bool successVerifyProof = s.verifier.verify_serialized_proof(proofPublicInput, _proof.serializedProof);\n        require(successVerifyProof, \"p\"); // Proof verification fail\n\n        // Verify the recursive part that was given to us through the public input\n        bool successProofAggregation = _verifyRecursivePartOfProof(_proof.recurisiveAggregationInput);\n        require(successProofAggregation, \"hh\"); // Proof aggregation must be valid\n        // #endif\n\n        require(currentTotalBlocksVerified <= s.totalBlocksCommitted, \"q\");\n        emit BlocksVerification(s.totalBlocksVerified, currentTotalBlocksVerified);\n        s.totalBlocksVerified = currentTotalBlocksVerified;\n    }\n\n    /// @dev Gets zk proof public input\n    function _getBlockProofPublicInput(\n        bytes32 _prevBlockCommitment,\n        bytes32 _currentBlockCommitment,\n        ProofInput calldata _proof,\n        VerifierParams memory _verifierParams\n    ) internal pure returns (uint256) {\n        return\n            uint256(\n                keccak256(\n                    abi.encodePacked(\n                        _prevBlockCommitment,\n                        _currentBlockCommitment,\n                        _verifierParams.recursionNodeLevelVkHash,\n                        _verifierParams.recursionLeafLevelVkHash,\n                        _verifierParams.recursionCircuitsSetVksHash,\n                        _proof.recurisiveAggregationInput\n                    )\n                )\n            ) & INPUT_MASK;\n    }\n\n    /// @dev Verify a part of the zkp, that is responsible for the aggregation\n    function _verifyRecursivePartOfProof(uint256[] calldata _recurisiveAggregationInput) internal view returns (bool) {\n        require(_recurisiveAggregationInput.length == 4);\n\n        PairingsBn254.G1Point memory pairWithGen = PairingsBn254.new_g1_checked(\n            _recurisiveAggregationInput[0],\n            _recurisiveAggregationInput[1]\n        );\n        PairingsBn254.G1Point memory pairWithX = PairingsBn254.new_g1_checked(\n            _recurisiveAggregationInput[2],\n            _recurisiveAggregationInput[3]\n        );\n\n        PairingsBn254.G2Point memory g2Gen = PairingsBn254.new_g2(\n            [\n                0x198e9393920d483a7260bfb731fb5d25f1aa493335a9e71297e485b7aef312c2,\n                0x1800deef121f1e76426a00665e5c4479674322d4f75edadd46debd5cd992f6ed\n            ],\n            [\n                0x090689d0585ff075ec9e99ad690c3395bc4b313370b38ef355acdadcd122975b,\n                0x12c85ea5db8c6deb4aab71808dcb408fe3d1e7690c43d37b4ce6cc0166fa7daa\n            ]\n        );\n        PairingsBn254.G2Point memory g2X = PairingsBn254.new_g2(\n            [\n                0x260e01b251f6f1c7e7ff4e580791dee8ea51d87a358e038b4efe30fac09383c1,\n                0x0118c4d5b837bcc2bc89b5b398b5974e9f5944073b32078b7e231fec938883b0\n            ],\n            [\n                0x04fc6369f7110fe3d25156c1bb9a72859cf2a04641f99ba4ee413c80da6a5fe4,\n                0x22febda3c0c0632a56475b4214e5615e11e6dd3f96e6cea2854a87d4dacc5e55\n            ]\n        );\n\n        return PairingsBn254.pairingProd2(pairWithGen, g2Gen, pairWithX, g2X);\n    }\n\n    /// @notice Reverts unexecuted blocks\n    /// @param _newLastBlock block number after which blocks should be reverted\n    /// NOTE: Doesn't delete the stored data about blocks, but only decreases\n    /// counters that are responsible for the number of blocks\n    function revertBlocks(uint256 _newLastBlock) external nonReentrant onlyValidator {\n        require(s.totalBlocksCommitted > _newLastBlock, \"v1\"); // the last committed block is less new last block\n        uint256 newTotalBlocksCommitted = _maxU256(_newLastBlock, s.totalBlocksExecuted);\n\n        if (newTotalBlocksCommitted < s.totalBlocksVerified) {\n            s.totalBlocksVerified = newTotalBlocksCommitted;\n        }\n        s.totalBlocksCommitted = newTotalBlocksCommitted;\n\n        emit BlocksRevert(s.totalBlocksCommitted, s.totalBlocksVerified, s.totalBlocksExecuted);\n    }\n\n    /// @notice Returns larger of two values\n    function _maxU256(uint256 a, uint256 b) internal pure returns (uint256) {\n        return a < b ? b : a;\n    }\n\n    /// @dev Creates block commitment from its data\n    function _createBlockCommitment(CommitBlockInfo calldata _newBlockData) internal view returns (bytes32) {\n        bytes32 passThroughDataHash = keccak256(_blockPassThroughData(_newBlockData));\n        bytes32 metadataHash = keccak256(_blockMetaParameters());\n        bytes32 auxiliaryOutputHash = keccak256(_blockAuxilaryOutput(_newBlockData));\n\n        return keccak256(abi.encode(passThroughDataHash, metadataHash, auxiliaryOutputHash));\n    }\n\n    function _blockPassThroughData(CommitBlockInfo calldata _block) internal pure returns (bytes memory) {\n        return\n            abi.encodePacked(\n                _block.indexRepeatedStorageChanges,\n                _block.newStateRoot,\n                uint64(0), // index repeated storage changes in zkPorter\n                bytes32(0) // zkPorter block hash\n            );\n    }\n\n    function _blockMetaParameters() internal view returns (bytes memory) {\n        return abi.encodePacked(s.zkPorterIsAvailable, s.l2BootloaderBytecodeHash, s.l2DefaultAccountBytecodeHash);\n    }\n\n    function _blockAuxilaryOutput(CommitBlockInfo calldata _block) internal pure returns (bytes memory) {\n        bytes32 initialStorageChangesHash = keccak256(_block.initialStorageChanges);\n        bytes32 repeatedStorageChangesHash = keccak256(_block.repeatedStorageChanges);\n        bytes32 l2ToL1LogsHash = keccak256(_block.l2Logs);\n\n        return abi.encode(_block.l2LogsTreeRoot, l2ToL1LogsHash, initialStorageChangesHash, repeatedStorageChangesHash);\n    }\n\n    /// @notice Returns the keccak hash of the ABI-encoded StoredBlockInfo\n    function _hashStoredBlockInfo(StoredBlockInfo memory _storedBlockInfo) internal pure returns (bytes32) {\n        return keccak256(abi.encode(_storedBlockInfo));\n    }\n}"
    },
    {
      "filename": "ethereum/contracts/zksync/facets/Executor.sol",
      "content": "// SPDX-License-Identifier: MIT OR Apache-2.0\n\npragma solidity ^0.8.0;\n\nimport \"./Base.sol\";\nimport \"../Config.sol\";\nimport \"../interfaces/IExecutor.sol\";\nimport \"../libraries/PairingsBn254.sol\";\nimport \"../libraries/PriorityQueue.sol\";\nimport \"../../common/libraries/UncheckedMath.sol\";\nimport \"../../common/libraries/UnsafeBytes.sol\";\nimport \"../../common/L2ContractHelper.sol\";\n\n/// @title zkSync Executor contract capable of processing events emitted in the zkSync protocol.\n/// @author Matter Labs\ncontract ExecutorFacet is Base, IExecutor {\n    using UncheckedMath for uint256;\n    using PriorityQueue for PriorityQueue.Queue;\n\n    /// @dev Process one block commit using the previous block StoredBlockInfo\n    /// @dev returns new block StoredBlockInfo\n    /// @notice Does not change storage\n    function _commitOneBlock(StoredBlockInfo memory _previousBlock, CommitBlockInfo calldata _newBlock)\n        internal\n        view\n        returns (StoredBlockInfo memory storedNewBlock)\n    {\n        require(_newBlock.blockNumber == _previousBlock.blockNumber + 1, \"f\"); // only commit next block\n\n        // Check that block contain all meta information for L2 logs.\n        // Get the chained hash of priority transaction hashes.\n        (\n            uint256 expectedNumberOfLayer1Txs,\n            bytes32 expectedPriorityOperationsHash,\n            bytes32 previousBlockHash,\n            uint256 l2BlockTimestamp\n        ) = _processL2Logs(_newBlock);\n\n        require(_previousBlock.blockHash == previousBlockHash, \"l\");\n        // Check that the priority operation hash in the L2 logs is as expected\n        require(expectedPriorityOperationsHash == _newBlock.priorityOperationsHash, \"t\");\n        // Check that the number of processed priority operations is as expected\n        require(expectedNumberOfLayer1Txs == _newBlock.numberOfLayer1Txs);\n        // Check that the timestamp that came from the Bootloader is expected\n        require(l2BlockTimestamp == _newBlock.timestamp);\n\n        // Preventing \"stack too deep error\"\n        {\n            // Check the timestamp of the new block\n            bool timestampNotTooSmall = block.timestamp - COMMIT_TIMESTAMP_NOT_OLDER <= l2BlockTimestamp;\n            bool timestampNotTooBig = l2BlockTimestamp <= block.timestamp + COMMIT_TIMESTAMP_APPROXIMATION_DELTA;\n            require(timestampNotTooSmall, \"h\"); // New block timestamp is too small\n            require(timestampNotTooBig, \"h1\"); // New block timestamp is too big\n\n            // Check the index of repeated storage writes\n            uint256 newStorageChangesIndexes = uint256(uint32(bytes4(_newBlock.initialStorageChanges[:4])));\n            require(\n                _previousBlock.indexRepeatedStorageChanges + newStorageChangesIndexes ==\n                    _newBlock.indexRepeatedStorageChanges,\n                \"yq\"\n            );\n        }\n\n        // Create block commitment for the proof verification\n        bytes32 commitment = _createBlockCommitment(_newBlock);\n\n        return\n            StoredBlockInfo(\n                _newBlock.blockNumber,\n                _newBlock.newStateRoot,\n                _newBlock.indexRepeatedStorageChanges,\n                _newBlock.numberOfLayer1Txs,\n                _newBlock.priorityOperationsHash,\n                _newBlock.l2LogsTreeRoot,\n                _newBlock.timestamp,\n                commitment\n            );\n    }\n\n    function _calculateBlockHash(StoredBlockInfo memory _previousBlock, CommitBlockInfo calldata _newBlock)\n        internal\n        pure\n        returns (bytes32)\n    {\n        return keccak256(abi.encode(_previousBlock.blockHash, _newBlock.newStateRoot));\n    }\n\n    /// @dev Check that L2 logs are proper and block contain all meta information for them\n    function _processL2Logs(CommitBlockInfo calldata _newBlock)\n        internal\n        pure\n        returns (\n            uint256 numberOfLayer1Txs,\n            bytes32 chainedPriorityTxsHash,\n            bytes32 previousBlockHash,\n            uint256 blockTimestamp\n        )\n    {\n        // Copy L2 to L1 logs into memory.\n        bytes memory emittedL2Logs = _newBlock.l2Logs[4:];\n        bytes[] calldata l2Messages = _newBlock.l2ArbitraryLengthMessages;\n        uint256 currentMessage;\n        // Auxiliary variable that is needed to enforce that `previousBlockHash` and `blockTimestamp` was read exactly one time\n        bool isSystemContextLogProcessed;\n        bytes[] calldata factoryDeps = _newBlock.factoryDeps;\n        uint256 currentBytecode;\n\n        chainedPriorityTxsHash = EMPTY_STRING_KECCAK;\n\n        // linear traversal of the logs\n        for (uint256 i = 0; i < emittedL2Logs.length; i = i.uncheckedAdd(L2_TO_L1_LOG_SERIALIZE_SIZE)) {\n            (address logSender, ) = UnsafeBytes.readAddress(emittedL2Logs, i + 4);\n\n            // show preimage for hashed message stored in log\n            if (logSender == L2_TO_L1_MESSENGER) {\n                (bytes32 hashedMessage, ) = UnsafeBytes.readBytes32(emittedL2Logs, i + 56);\n                require(keccak256(l2Messages[currentMessage]) == hashedMessage, \"k2\");\n\n                currentMessage = currentMessage.uncheckedInc();\n            } else if (logSender == L2_BOOTLOADER_ADDRESS) {\n                (bytes32 canonicalTxHash, ) = UnsafeBytes.readBytes32(emittedL2Logs, i + 24);\n                chainedPriorityTxsHash = keccak256(abi.encode(chainedPriorityTxsHash, canonicalTxHash));\n\n                // Overflow is not realistic\n                numberOfLayer1Txs = numberOfLayer1Txs.uncheckedInc();\n            } else if (logSender == L2_SYSTEM_CONTEXT_ADDRESS) {\n                // Make sure that the system context log wasn't processed yet, to\n                // avoid accident double reading `blockTimestamp` and `previousBlockHash`\n                require(!isSystemContextLogProcessed, \"fx\");\n                (blockTimestamp, ) = UnsafeBytes.readUint256(emittedL2Logs, i + 24);\n                (previousBlockHash, ) = UnsafeBytes.readBytes32(emittedL2Logs, i + 56);\n                // Mark system context log as processed\n                isSystemContextLogProcessed = true;\n            } else if (logSender == L2_KNOWN_CODE_STORAGE_ADDRESS) {\n                (bytes32 bytecodeHash, ) = UnsafeBytes.readBytes32(emittedL2Logs, i + 24);\n                require(bytecodeHash == L2ContractHelper.hashL2Bytecode(factoryDeps[currentBytecode]), \"k3\");\n\n                currentBytecode = currentBytecode.uncheckedInc();\n            }\n        }\n        // To check that only relevant preimages have been included in the calldata\n        require(currentBytecode == factoryDeps.length, \"ym\");\n        require(currentMessage == l2Messages.length, \"pl\");\n        // `blockTimestamp` and `previousBlockHash` wasn't read from L2 logs\n        require(isSystemContextLogProcessed, \"by\");\n    }\n\n    /// @notice Commit block\n    /// @notice 1. Checks timestamp.\n    /// @notice 2. Process L2 logs.\n    /// @notice 3. Store block commitments.\n    function commitBlocks(StoredBlockInfo memory _lastCommittedBlockData, CommitBlockInfo[] calldata _newBlocksData)\n        external\n        override\n        nonReentrant\n        onlyValidator\n    {\n        // Check that we commit blocks after last committed block\n        require(s.storedBlockHashes[s.totalBlocksCommitted] == _hashStoredBlockInfo(_lastCommittedBlockData), \"i\"); // incorrect previous block data\n\n        uint256 blocksLength = _newBlocksData.length;\n        for (uint256 i = 0; i < blocksLength; i = i.uncheckedInc()) {\n            _lastCommittedBlockData = _commitOneBlock(_lastCommittedBlockData, _newBlocksData[i]);\n            s.storedBlockHashes[_lastCommittedBlockData.blockNumber] = _hashStoredBlockInfo(_lastCommittedBlockData);\n\n            emit BlockCommit(\n                _lastCommittedBlockData.blockNumber,\n                _lastCommittedBlockData.blockHash,\n                _lastCommittedBlockData.commitment\n            );\n        }\n\n        s.totalBlocksCommitted = s.totalBlocksCommitted + blocksLength;\n    }\n\n    /// @dev Pops the priority operations from the priority queue and returns a rolling hash of operations\n    function _collectOperationsFromPriorityQueue(uint256 _nPriorityOps) internal returns (bytes32 concatHash) {\n        concatHash = EMPTY_STRING_KECCAK;\n\n        for (uint256 i = 0; i < _nPriorityOps; i = i.uncheckedInc()) {\n            PriorityOperation memory priorityOp = s.priorityQueue.popFront();\n            concatHash = keccak256(abi.encode(concatHash, priorityOp.canonicalTxHash));\n        }\n    }\n\n    /// @dev Executes one block\n    /// @dev 1. Processes all pending operations (Complete priority requests)\n    /// @dev 2. Finalizes block on Ethereum\n    /// @dev _executedBlockIdx is an index in the array of the blocks that we want to execute together\n    function _executeOneBlock(StoredBlockInfo memory _storedBlock, uint256 _executedBlockIdx) internal {\n        uint256 currentBlockNumber = _storedBlock.blockNumber;\n        require(currentBlockNumber == s.totalBlocksExecuted + _executedBlockIdx + 1, \"k\"); // Execute blocks in order\n        require(\n            _hashStoredBlockInfo(_storedBlock) == s.storedBlockHashes[currentBlockNumber],\n            \"exe10\" // executing block should be committed\n        );\n\n        bytes32 priorityOperationsHash = _collectOperationsFromPriorityQueue(_storedBlock.numberOfLayer1Txs);\n        require(priorityOperationsHash == _storedBlock.priorityOperationsHash, \"x\"); // priority operations hash does not match to expected\n\n        // Save root hash of L2 -> L1 logs tree\n        s.l2LogsRootHashes[currentBlockNumber] = _storedBlock.l2LogsTreeRoot;\n    }\n\n    /// @notice Execute blocks, complete priority operations and process withdrawals.\n    /// @notice 1. Processes all pending operations (Complete priority requests)\n    /// @notice 2. Finalizes block on Ethereum\n    function executeBlocks(StoredBlockInfo[] calldata _blocksData) external nonReentrant onlyValidator {\n        uint256 nBlocks = _blocksData.length;\n        for (uint256 i = 0; i < nBlocks; i = i.uncheckedInc()) {\n            _executeOneBlock(_blocksData[i], i);\n            emit BlockExecution(_blocksData[i].blockNumber, _blocksData[i].blockHash, _blocksData[i].commitment);\n        }\n\n        s.totalBlocksExecuted = s.totalBlocksExecuted + nBlocks;\n        require(s.totalBlocksExecuted <= s.totalBlocksVerified, \"n\"); // Can't execute blocks more then committed and proven currently.\n    }\n\n    /// @notice Blocks commitment verification.\n    /// @notice Only verifies block commitments without any other processing\n    function proveBlocks(\n        StoredBlockInfo calldata _prevBlock,\n        StoredBlockInfo[] calldata _committedBlocks,\n        ProofInput calldata _proof\n    ) external nonReentrant onlyValidator {\n        // Save the variables into the stack to save gas on reading them later\n        uint256 currentTotalBlocksVerified = s.totalBlocksVerified;\n        uint256 committedBlocksLength = _committedBlocks.length;\n\n        // Save the variable from the storage to memory to save gas\n        VerifierParams memory verifierParams = s.verifierParams;\n\n        // Initialize the array, that will be used as public input to the ZKP\n        uint256[] memory proofPublicInput = new uint256[](committedBlocksLength);\n\n        // Check that the block passed by the validator is indeed the first unverified block\n        require(_hashStoredBlockInfo(_prevBlock) == s.storedBlockHashes[currentTotalBlocksVerified], \"t1\");\n\n        bytes32 prevBlockCommitment = _prevBlock.commitment;\n        for (uint256 i = 0; i < committedBlocksLength; i = i.uncheckedInc()) {\n            require(\n                _hashStoredBlockInfo(_committedBlocks[i]) ==\n                    s.storedBlockHashes[currentTotalBlocksVerified.uncheckedInc()],\n                \"o1\"\n            );\n\n            bytes32 currentBlockCommitment = _committedBlocks[i].commitment;\n            proofPublicInput[i] = _getBlockProofPublicInput(\n                prevBlockCommitment,\n                currentBlockCommitment,\n                _proof,\n                verifierParams\n            );\n\n            prevBlockCommitment = currentBlockCommitment;\n            currentTotalBlocksVerified = currentTotalBlocksVerified.uncheckedInc();\n        }\n\n        // #if DUMMY_VERIFIER == false\n        bool successVerifyProof = s.verifier.verify_serialized_proof(proofPublicInput, _proof.serializedProof);\n        require(successVerifyProof, \"p\"); // Proof verification fail\n\n        // Verify the recursive part that was given to us through the public input\n        bool successProofAggregation = _verifyRecursivePartOfProof(_proof.recurisiveAggregationInput);\n        require(successProofAggregation, \"hh\"); // Proof aggregation must be valid\n        // #endif\n\n        require(currentTotalBlocksVerified <= s.totalBlocksCommitted, \"q\");\n        emit BlocksVerification(s.totalBlocksVerified, currentTotalBlocksVerified);\n        s.totalBlocksVerified = currentTotalBlocksVerified;\n    }\n\n    /// @dev Gets zk proof public input\n    function _getBlockProofPublicInput(\n        bytes32 _prevBlockCommitment,\n        bytes32 _currentBlockCommitment,\n        ProofInput calldata _proof,\n        VerifierParams memory _verifierParams\n    ) internal pure returns (uint256) {\n        return\n            uint256(\n                keccak256(\n                    abi.encodePacked(\n                        _prevBlockCommitment,\n                        _currentBlockCommitment,\n                        _verifierParams.recursionNodeLevelVkHash,\n                        _verifierParams.recursionLeafLevelVkHash,\n                        _verifierParams.recursionCircuitsSetVksHash,\n                        _proof.recurisiveAggregationInput\n                    )\n                )\n            ) & INPUT_MASK;\n    }\n\n    /// @dev Verify a part of the zkp, that is responsible for the aggregation\n    function _verifyRecursivePartOfProof(uint256[] calldata _recurisiveAggregationInput) internal view returns (bool) {\n        require(_recurisiveAggregationInput.length == 4);\n\n        PairingsBn254.G1Point memory pairWithGen = PairingsBn254.new_g1_checked(\n            _recurisiveAggregationInput[0],\n            _recurisiveAggregationInput[1]\n        );\n        PairingsBn254.G1Point memory pairWithX = PairingsBn254.new_g1_checked(\n            _recurisiveAggregationInput[2],\n            _recurisiveAggregation"
    }
  ]
}