{
  "Title": "[38] Chunking the pubdata should be made more efficient",
  "Content": "\n### Proof of Concept\n\nTake a look at https://github.com/code-423n4/2024-03-zksync/blob/4f0ba34f34a864c354c7e8c47643ed8f4a250e13/code/system-contracts/contracts/PubdataChunkPublisher.sol#L21-L57\n\n```solidity\n    function chunkAndPublishPubdata(bytes calldata _pubdata) external onlyCallFrom(address(L1_MESSENGER_CONTRACT)) {\n        require(_pubdata.length <= BLOB_SIZE_BYTES * MAX_NUMBER_OF_BLOBS, \"pubdata should fit in 2 blobs\");\n\n        bytes32[] memory blobHashes = new bytes32[](MAX_NUMBER_OF_BLOBS);\n\n        // We allocate to the full size of MAX_NUMBER_OF_BLOBS * BLOB_SIZE_BYTES because we need to pad\n        // the data on the right with 0s if it doesn't take up the full blob\n        bytes memory totalBlobs = new bytes(BLOB_SIZE_BYTES * MAX_NUMBER_OF_BLOBS);\n\n        assembly {\n            // The pointer to the allocated memory above. We skip 32 bytes to avoid overwriting the length.\n            let ptr := add(totalBlobs, 0x20)\n            calldatacopy(ptr, _pubdata.offset, _pubdata.length)\n        }\n\n        for (uint256 i = 0; i < MAX_NUMBER_OF_BLOBS; i++) {\n            uint256 start = BLOB_SIZE_BYTES * i;\n\n            // We break if the pubdata isn't enough to cover 2 blobs. On L1 it is expected that the hash\n            // will be bytes32(0) if a blob isn't going to be used.\n            if (start >= _pubdata.length) {\n                break;\n            }\n\n            bytes32 blobHash;\n            assembly {\n                // The pointer to the allocated memory above skipping the length.\n                let ptr := add(totalBlobs, 0x20)\n                blobHash := keccak256(add(ptr, start), BLOB_SIZE_BYTES)\n            }\n\n            blobHashes[i] = blobHash;\n        }\n\n        SystemContractHelper.toL1(true, bytes32(uint256(SystemLogKey.BLOB_ONE_HASH_KEY)), blobHashes[0]);\n        SystemContractHelper.toL1(true, bytes32(uint256(SystemLogKey.BLOB_TWO_HASH_KEY)), blobHashes[1]);\n    }\n```\n\nFunction is used to chunk pubdata into pieces that can fit into blobs, case is that it queries to `constant variables` and then multiplies, when this value can just be hardcoded to the codebase, i.e this line `require(_pubdata.length <= BLOB_SIZE_BYTES * MAX_NUMBER_OF_BLOBS, \"pubdata should fit in 2 blobs\");`, both `BLOB_SIZE_BYTES` and `MAX_NUMBER_OF_BLOBS` have fixed numbers of `2` and `126976` respectively as gotten from https://github.com/code-423n4/2024-03-zksync/blob/4f0ba34f34a864c354c7e8c47643ed8f4a250e13/code/system-contracts/contracts/Constants.sol\n\nThis means that the check could be changed to `require(_pubdata.length <= 253_952 \"pubdata should fit in 2 blobs\");`\n\n### Impact\n\nWastage of gas.\n\n### Recommended Mitigation Steps\n\nConsider making the change in as much as the values are going to be constants.\n\n",
  "Impact": "LOW",
  "Source": "https://code4rena.com/reports/2024-03-zksync",
  "Code": [
    {
      "filename": "code/system-contracts/contracts/PubdataChunkPublisher.sol",
      "content": "// SPDX-License-Identifier: MIT\npragma solidity 0.8.20;\n\nimport {IPubdataChunkPublisher} from \"./interfaces/IPubdataChunkPublisher.sol\";\nimport {ISystemContract} from \"./interfaces/ISystemContract.sol\";\nimport {L1_MESSENGER_CONTRACT, BLOB_SIZE_BYTES, MAX_NUMBER_OF_BLOBS} from \"./Constants.sol\";\nimport {EfficientCall} from \"./libraries/EfficientCall.sol\";\nimport {SystemContractHelper} from \"./libraries/SystemContractHelper.sol\";\nimport {SystemLogKey} from \"./Constants.sol\";\n\n/**\n * @author Matter Labs\n * @custom:security-contact security@matterlabs.dev\n * @notice Smart contract for chunking pubdata into the appropriate size for EIP-4844 blobs.\n */\ncontract PubdataChunkPublisher is IPubdataChunkPublisher, ISystemContract {\n    /// @notice Chunks pubdata into pieces that can fit into blobs.\n    /// @param _pubdata The total l2 to l1 pubdata that will be sent via L1 blobs.\n    /// @dev Note: This is an early implementation, in the future we plan to support up to 16 blobs per l1 batch.\n    /// @dev We always publish 2 system logs even if our pubdata fits into a single blob. This makes processing logs on L1 easier.\n    function chunkAndPublishPubdata(bytes calldata _pubdata) external onlyCallFrom(address(L1_MESSENGER_CONTRACT)) {\n        require(_pubdata.length <= BLOB_SIZE_BYTES * MAX_NUMBER_OF_BLOBS, \"pubdata should fit in 2 blobs\");\n\n        bytes32[] memory blobHashes = new bytes32[](MAX_NUMBER_OF_BLOBS);\n\n        // We allocate to the full size of MAX_NUMBER_OF_BLOBS * BLOB_SIZE_BYTES because we need to pad\n        // the data on the right with 0s if it doesn't take up the full blob\n        bytes memory totalBlobs = new bytes(BLOB_SIZE_BYTES * MAX_NUMBER_OF_BLOBS);\n\n        assembly {\n            // The pointer to the allocated memory above. We skip 32 bytes to avoid overwriting the length.\n            let ptr := add(totalBlobs, 0x20)\n            calldatacopy(ptr, _pubdata.offset, _pubdata.length)\n        }\n\n        for (uint256 i = 0; i < MAX_NUMBER_OF_BLOBS; i++) {\n            uint256 start = BLOB_SIZE_BYTES * i;\n\n            // We break if the pubdata isn't enough to cover 2 blobs. On L1 it is expected that the hash\n            // will be bytes32(0) if a blob isn't going to be used.\n            if (start >= _pubdata.length) {\n                break;\n            }\n\n            bytes32 blobHash;\n            assembly {\n                // The pointer to the allocated memory above skipping the length.\n                let ptr := add(totalBlobs, 0x20)\n                blobHash := keccak256(add(ptr, start), BLOB_SIZE_BYTES)\n            }\n\n            blobHashes[i] = blobHash;\n        }\n\n        SystemContractHelper.toL1(true, bytes32(uint256(SystemLogKey.BLOB_ONE_HASH_KEY)), blobHashes[0]);\n        SystemContractHelper.toL1(true, bytes32(uint256(SystemLogKey.BLOB_TWO_HASH_KEY)), blobHashes[1]);\n    }\n}"
    }
  ]
}