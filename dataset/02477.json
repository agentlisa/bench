{
  "Title": "M-6: Crafted p2p spam can render nodes permanently unable to process L2 blocks",
  "Content": "# Issue M-6: Crafted p2p spam can render nodes permanently unable to process L2 blocks \n\nSource: https://github.com/sherlock-audit/2023-01-optimism-judging/issues/177 \n\n## Found by \nusmannk\n\n## Summary\n\nThe constraints in `BuildBlocksValidator` are meant to prevent p2p spam from bogging down the network. However they are not sufficient for stopping spam, but are strong enough to render nodes unable to catch up.\n\n## Vulnerability Detail\n\nThe `p2p.BuildBlocksValidator` function has several constraints that must be met before a gossiped block is considered valid. If a block is considered valid then it is gossiped to peers and ingested into the current node.\n\nThe constraints on a gossiped block `block` are, in order:\n- `block` must be <10MB\n- `block` must be compressed as valid snappy data\n- `block` must be signed by the sequencer\n- `block` must be a valid SSZ encoded block\n- `block`.timestamp must within 60 seconds in the past and 5 seconds in the future\n- `block.BlockHash` must be correctly calculated\n- `block.BlockNumber` must not be in the set of the previous 100 unique block numbers that reached this step\n\nThese constraints are applied here:\n\nhttps://github.com/sherlock-audit/2023-01-optimism/blob/main/optimism/op-node/p2p/gossip.go#L208\n\nHowever, it is often the case that there are over 100 valid L2 blocks emitted per minute. For example, the 253 blocks between block 71404110 and 71404363 were emitted within 60 seconds. \n\nIn this example, an attacker would take the 200 blocks from 71404110-71404310 and replay them in order to a node for 60 seconds. Every block would be valid because they each pass all constraints and the 100 block LRU cache for marking `seen` blocks would be evicted twice over during the process. \n\nAfter 60 seconds, block `71404310 + 1` would be invalid to this node because its timestamp would be too old. At this point, for any new L2 block the node would either:\n- mark it as invalid because it is too old\n- or ingest it and throw it away because it is waiting for block `71404311`\n\nAs a result, the node will no longer be able to successfully process any L2 blocks from the p2p network.\n\nFurthermore, because all 200 blocks are marked as valid several times, the node would gossip all of these blocks (potentially several times) to its peers who would suffer the same effect and impact their peers.\n\n## Impact\n\nAttackers may halt targeted nodes.\n\nFurther knock-on potential halting of the node's peers, peers of peers, and so on.\n\n## Code Snippet\n\n## Tool used\n\nManual Review\n\n## Recommendation\n\nIncrease the size of the `blockHeightLRU` cache to more effectively catch and reject previously seen nodes. It would be reasonable to set it to 1000 instead of 100. Even at 1000 the cache should occupy less than 500kb in memory.\n\n## Discussion\n\n**rcstanciu**\n\nComment from Optimism\n\n---\n\n **Description:** P2P buffer is too small\n\n\n **Reason:** This is a valid attack, wherein the attacker replays blocks and eclipses a node from the P2P network.\n\n\n **Action:** Increase buffer size\n\n\n\n\n\n",
  "Impact": "MEDIUM",
  "Source": "https://app.sherlock.xyz/audits/contests/38",
  "Code": [
    {
      "filename": "optimism/op-node/p2p/gossip.go",
      "content": "package p2p\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"crypto/sha256\"\n\t\"encoding/binary\"\n\t\"fmt\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/golang/snappy\"\n\tlru \"github.com/hashicorp/golang-lru\"\n\tpubsub \"github.com/libp2p/go-libp2p-pubsub\"\n\tpb \"github.com/libp2p/go-libp2p-pubsub/pb\"\n\t\"github.com/libp2p/go-libp2p/core/host\"\n\t\"github.com/libp2p/go-libp2p/core/peer\"\n\n\t\"github.com/ethereum/go-ethereum/common\"\n\t\"github.com/ethereum/go-ethereum/crypto\"\n\t\"github.com/ethereum/go-ethereum/log\"\n\n\t\"github.com/ethereum-optimism/optimism/op-node/eth\"\n\t\"github.com/ethereum-optimism/optimism/op-node/rollup\"\n)\n\nconst (\n\t// maxGossipSize limits the total size of gossip RPC containers as well as decompressed individual messages.\n\tmaxGossipSize = 10 * (1 << 20)\n\t// minGossipSize is used to make sure that there is at least some data to validate the signature against.\n\tminGossipSize          = 66\n\tmaxOutboundQueue       = 256\n\tmaxValidateQueue       = 256\n\tglobalValidateThrottle = 512\n\tgossipHeartbeat        = 500 * time.Millisecond\n\t// seenMessagesTTL limits the duration that message IDs are remembered for gossip deduplication purposes\n\tseenMessagesTTL = 80 * gossipHeartbeat\n)\n\n// Message domains, the msg id function uncompresses to keep data monomorphic,\n// but invalid compressed data will need a unique different id.\n\nvar MessageDomainInvalidSnappy = [4]byte{0, 0, 0, 0}\nvar MessageDomainValidSnappy = [4]byte{1, 0, 0, 0}\n\ntype GossipRuntimeConfig interface {\n\tP2PSequencerAddress() common.Address\n}\n\ntype GossipMetricer interface {\n\tRecordGossipEvent(evType int32)\n}\n\nfunc blocksTopicV1(cfg *rollup.Config) string {\n\treturn fmt.Sprintf(\"/optimism/%s/0/blocks\", cfg.L2ChainID.String())\n}\n\n// BuildSubscriptionFilter builds a simple subscription filter,\n// to help protect against peers spamming useless subscriptions.\nfunc BuildSubscriptionFilter(cfg *rollup.Config) pubsub.SubscriptionFilter {\n\treturn pubsub.NewAllowlistSubscriptionFilter(blocksTopicV1(cfg)) // add more topics here in the future, if any.\n}\n\nvar msgBufPool = sync.Pool{New: func() any {\n\t// note: the topic validator concurrency is limited, so pool won't blow up, even with large pre-allocation.\n\tx := make([]byte, 0, maxGossipSize)\n\treturn &x\n}}\n\n// BuildMsgIdFn builds a generic message ID function for gossipsub that can handle compressed payloads,\n// mirroring the eth2 p2p gossip spec.\nfunc BuildMsgIdFn(cfg *rollup.Config) pubsub.MsgIdFunction {\n\treturn func(pmsg *pb.Message) string {\n\t\tvalid := false\n\t\tvar data []byte\n\t\t// If it's a valid compressed snappy data, then hash the uncompressed contents.\n\t\t// The validator can throw away the message later when recognized as invalid,\n\t\t// and the unique hash helps detect duplicates.\n\t\tdLen, err := snappy.DecodedLen(pmsg.Data)\n\t\tif err == nil && dLen <= maxGossipSize {\n\t\t\tres := msgBufPool.Get().(*[]byte)\n\t\t\tdefer msgBufPool.Put(res)\n\t\t\tif data, err = snappy.Decode((*res)[:0], pmsg.Data); err == nil {\n\t\t\t\t*res = data // if we ended up growing the slice capacity, fine, keep the larger one.\n\t\t\t\tvalid = true\n\t\t\t}\n\t\t}\n\t\tif data == nil {\n\t\t\tdata = pmsg.Data\n\t\t}\n\t\th := sha256.New()\n\t\tif valid {\n\t\t\th.Write(MessageDomainValidSnappy[:])\n\t\t} else {\n\t\t\th.Write(MessageDomainInvalidSnappy[:])\n\t\t}\n\t\t// The chain ID is part of the gossip topic, making the msg id unique\n\t\ttopic := pmsg.GetTopic()\n\t\tvar topicLen [8]byte\n\t\tbinary.LittleEndian.PutUint64(topicLen[:], uint64(len(topic)))\n\t\th.Write(topicLen[:])\n\t\th.Write([]byte(topic))\n\t\th.Write(data)\n\t\t// the message ID is shortened to save space, a lot of these may be gossiped.\n\t\treturn string(h.Sum(nil)[:20])\n\t}\n}\n\nfunc BuildGlobalGossipParams(cfg *rollup.Config) pubsub.GossipSubParams {\n\tparams := pubsub.DefaultGossipSubParams()\n\tparams.D = 8                               // topic stable mesh target count\n\tparams.Dlo = 6                             // topic stable mesh low watermark\n\tparams.Dhi = 12                            // topic stable mesh high watermark\n\tparams.Dlazy = 6                           // gossip target\n\tparams.HeartbeatInterval = gossipHeartbeat // interval of heartbeat\n\tparams.FanoutTTL = 24 * time.Second        // ttl for fanout maps for topics we are not subscribed to but have published to\n\tparams.HistoryLength = 12                  // number of windows to retain full messages in cache for IWANT responses\n\tparams.HistoryGossip = 3                   // number of windows to gossip about\n\n\treturn params\n}\n\nfunc NewGossipSub(p2pCtx context.Context, h host.Host, cfg *rollup.Config, m GossipMetricer) (*pubsub.PubSub, error) {\n\tdenyList, err := pubsub.NewTimeCachedBlacklist(30 * time.Second)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn pubsub.NewGossipSub(p2pCtx, h,\n\t\tpubsub.WithMaxMessageSize(maxGossipSize),\n\t\tpubsub.WithMessageIdFn(BuildMsgIdFn(cfg)),\n\t\tpubsub.WithNoAuthor(),\n\t\tpubsub.WithMessageSignaturePolicy(pubsub.StrictNoSign),\n\t\tpubsub.WithSubscriptionFilter(BuildSubscriptionFilter(cfg)),\n\t\tpubsub.WithValidateQueueSize(maxValidateQueue),\n\t\tpubsub.WithPeerOutboundQueueSize(maxOutboundQueue),\n\t\tpubsub.WithValidateThrottle(globalValidateThrottle),\n\t\tpubsub.WithSeenMessagesTTL(seenMessagesTTL),\n\t\tpubsub.WithPeerExchange(false),\n\t\tpubsub.WithBlacklist(denyList),\n\t\tpubsub.WithGossipSubParams(BuildGlobalGossipParams(cfg)),\n\t\tpubsub.WithEventTracer(&gossipTracer{m: m}),\n\t)\n\t// TODO: pubsub.WithPeerScoreInspect(inspect, InspectInterval) to update peerstore scores with gossip scores\n}\n\nfunc validationResultString(v pubsub.ValidationResult) string {\n\tswitch v {\n\tcase pubsub.ValidationAccept:\n\t\treturn \"ACCEPT\"\n\tcase pubsub.ValidationIgnore:\n\t\treturn \"IGNORE\"\n\tcase pubsub.ValidationReject:\n\t\treturn \"REJECT\"\n\tdefault:\n\t\treturn fmt.Sprintf(\"UNKNOWN_%d\", v)\n\t}\n}\n\nfunc logValidationResult(self peer.ID, msg string, log log.Logger, fn pubsub.ValidatorEx) pubsub.ValidatorEx {\n\treturn func(ctx context.Context, id peer.ID, message *pubsub.Message) pubsub.ValidationResult {\n\t\tres := fn(ctx, id, message)\n\t\tvar src any\n\t\tsrc = id\n\t\tif id == self {\n\t\t\tsrc = \"self\"\n\t\t}\n\t\tlog.Debug(msg, \"result\", validationResultString(res), \"from\", src)\n\t\treturn res\n\t}\n}\n\nfunc guardGossipValidator(log log.Logger, fn pubsub.ValidatorEx) pubsub.ValidatorEx {\n\treturn func(ctx context.Context, id peer.ID, message *pubsub.Message) (result pubsub.ValidationResult) {\n\t\tdefer func() {\n\t\t\tif err := recover(); err != nil {\n\t\t\t\tlog.Error(\"gossip validation panic\", \"err\", err, \"peer\", id)\n\t\t\t\tresult = pubsub.ValidationReject\n\t\t\t}\n\t\t}()\n\t\treturn fn(ctx, id, message)\n\t}\n}\n\ntype seenBlocks struct {\n\tsync.Mutex\n\tblockHashes []common.Hash\n}\n\n// hasSeen checks if the hash has been marked as seen, and how many have been seen.\nfunc (sb *seenBlocks) hasSeen(h common.Hash) (count int, hasSeen bool) {\n\tsb.Lock()\n\tdefer sb.Unlock()\n\tfor _, prev := range sb.blockHashes {\n\t\tif prev == h {\n\t\t\treturn len(sb.blockHashes), true\n\t\t}\n\t}\n\treturn len(sb.blockHashes), false\n}\n\n// markSeen marks the block hash as seen\nfunc (sb *seenBlocks) markSeen(h common.Hash) {\n\tsb.Lock()\n\tdefer sb.Unlock()\n\tsb.blockHashes = append(sb.blockHashes, h)\n}\n\nfunc BuildBlocksValidator(log log.Logger, cfg *rollup.Config, runCfg GossipRuntimeConfig) pubsub.ValidatorEx {\n\n\t// Seen block hashes per block height\n\t// uint64 -> *seenBlocks\n\tblockHeightLRU, err := lru.New(100)\n\tif err != nil {\n\t\tpanic(fmt.Errorf(\"failed to set up block height LRU cache: %w\", err))\n\t}\n\n\treturn func(ctx context.Context, id peer.ID, message *pubsub.Message) pubsub.ValidationResult {\n\t\t// [REJECT] if the compression is not valid\n\t\toutLen, err := snappy.DecodedLen(message.Data)\n\t\tif err != nil {\n\t\t\tlog.Warn(\"invalid snappy compression length data\", \"err\", err, \"peer\", id)\n\t\t\treturn pubsub.ValidationReject\n\t\t}\n\t\tif outLen > maxGossipSize {\n\t\t\tlog.Warn(\"possible snappy zip bomb, decoded length is too large\", \"decoded_length\", outLen, \"peer\", id)\n\t\t\treturn pubsub.ValidationReject\n\t\t}\n\t\tif outLen < minGossipSize {\n\t\t\tlog.Warn(\"rejecting undersized gossip payload\")\n\t\t\treturn pubsub.ValidationReject\n\t\t}\n\n\t\tres := msgBufPool.Get().(*[]byte)\n\t\tdefer msgBufPool.Put(res)\n\t\tdata, err := snappy.Decode((*res)[:0], message.Data)\n\t\tif err != nil {\n\t\t\tlog.Warn(\"invalid snappy compression\", \"err\", err, \"peer\", id)\n\t\t\treturn pubsub.ValidationReject\n\t\t}\n\t\t*res = data // if we ended up growing the slice capacity, fine, keep the larger one.\n\n\t\t// message starts with compact-encoding secp256k1 encoded signature\n\t\tsignatureBytes, payloadBytes := data[:65], data[65:]\n\n\t\t// [REJECT] if the signature by the sequencer is not valid\n\t\tsigningHash, err := BlockSigningHash(cfg, payloadBytes)\n\t\tif err != nil {\n\t\t\tlog.Warn(\"failed to compute block signing hash\", \"err\", err, \"peer\", id)\n\t\t\treturn pubsub.ValidationReject\n\t\t}\n\n\t\tpub, err := crypto.SigToPub(signingHash[:], signatureBytes)\n\t\tif err != nil {\n\t\t\tlog.Warn(\"invalid block signature\", \"err\", err, \"peer\", id)\n\t\t\treturn pubsub.ValidationReject\n\t\t}\n\t\taddr := crypto.PubkeyToAddress(*pub)\n\n\t\t// In the future we may load & validate block metadata before checking the signature.\n\t\t// And then check the signer based on the metadata, to support e.g. multiple p2p signers at the same time.\n\t\t// For now we only have one signer at a time and thus check the address directly.\n\t\t// This means we may drop old payloads upon key rotation,\n\t\t// but this can be recovered from like any other missed unsafe payload.\n\t\tif expected := runCfg.P2PSequencerAddress(); expected == (common.Address{}) {\n\t\t\tlog.Warn(\"no configured p2p sequencer address, ignoring gossiped block\", \"peer\", id, \"addr\", addr)\n\t\t\treturn pubsub.ValidationIgnore\n\t\t} else if addr != expected {\n\t\t\tlog.Warn(\"unexpected block author\", \"err\", err, \"peer\", id, \"addr\", addr, \"expected\", expected)\n\t\t\treturn pubsub.ValidationReject\n\t\t}\n\n\t\t// [REJECT] if the block encoding is not valid\n\t\tvar payload eth.ExecutionPayload\n\t\tif err := payload.UnmarshalSSZ(uint32(len(payloadBytes)), bytes.NewReader(payloadBytes)); err != nil {\n\t\t\tlog.Warn(\"invalid payload\", \"err\", err, \"peer\", id)\n\t\t\treturn pubsub.ValidationReject\n\t\t}\n\n\t\t// rounding down to seconds is fine here.\n\t\tnow := uint64(time.Now().Unix())\n\n\t\t// [REJECT] if the `payload.timestamp` is older than 60 seconds in the past\n\t\tif uint64(payload.Timestamp) < now-60 {\n\t\t\tlog.Warn(\"payload is too old\", \"timestamp\", uint64(payload.Timestamp))\n\t\t\treturn pubsub.ValidationReject\n\t\t}\n\n\t\t// [REJECT] if the `payload.timestamp` is more than 5 seconds into the future\n\t\tif uint64(payload.Timestamp) > now+5 {\n\t\t\tlog.Warn(\"payload is too new\", \"timestamp\", uint64(payload.Timestamp))\n\t\t\treturn pubsub.ValidationReject\n\t\t}\n\n\t\t// [REJECT] if the `block_hash` in the `payload` is not valid\n\t\tif actual, ok := payload.CheckBlockHash(); !ok {\n\t\t\tlog.Warn(\"payload has bad block hash\", \"bad_hash\", payload.BlockHash.String(), \"actual\", actual.String())\n\t\t\treturn pubsub.ValidationReject\n\t\t}\n\n\t\tseen, ok := blockHeightLRU.Get(uint64(payload.BlockNumber))\n\t\tif !ok {\n\t\t\tseen = new(seenBlocks)\n\t\t\tblockHeightLRU.Add(uint64(payload.BlockNumber), seen)\n\t\t}\n\n\t\tif count, hasSeen := seen.(*seenBlocks).hasSeen(payload.BlockHash); count > 5 {\n\t\t\t// [REJECT] if more than 5 blocks have been seen with the same block height\n\t\t\tlog.Warn(\"seen too many different blocks at same height\", \"height\", payload.BlockNumber)\n\t\t\treturn pubsub.ValidationReject\n\t\t} else if hasSeen {\n\t\t\t// [IGNORE] if the block has already been seen\n\t\t\tlog.Warn(\"validated already seen message again\")\n\t\t\treturn pubsub.ValidationIgnore\n\t\t}\n\n\t\t// mark it as seen. (note: with concurrent validation more than 5 blocks may be marked as seen still,\n\t\t// but validator concurrency is limited anyway)\n\t\tseen.(*seenBlocks).markSeen(payload.BlockHash)\n\n\t\t// remember the decoded payload for later usage in topic subscriber.\n\t\tmessage.ValidatorData = &payload\n\t\treturn pubsub.ValidationAccept\n\t}\n}\n\ntype GossipIn interface {\n\tOnUnsafeL2Payload(ctx context.Context, from peer.ID, msg *eth.ExecutionPayload) error\n}\n\ntype GossipTopicInfo interface {\n\tBlocksTopicPeers() []peer.ID\n}\n\ntype GossipOut interface {\n\tGossipTopicInfo\n\tPublishL2Payload(ctx context.Context, msg *eth.ExecutionPayload, signer Signer) error\n\tClose() error\n}\n\ntype publisher struct {\n\tlog         log.Logger\n\tcfg         *rollup.Config\n\tblocksTopic *pubsub.Topic\n\trunCfg      GossipRuntimeConfig\n}\n\nvar _ GossipOut = (*publisher)(nil)\n\nfunc (p *publisher) BlocksTopicPeers() []peer.ID {\n\treturn p.blocksTopic.ListPeers()\n}\n\nfunc (p *publisher) PublishL2Payload(ctx context.Context, payload *eth.ExecutionPayload, signer Signer) error {\n\tres := msgBufPool.Get().(*[]byte)\n\tbuf := bytes.NewBuffer((*res)[:0])\n\tdefer func() {\n\t\t*res = buf.Bytes()\n\t\tdefer msgBufPool.Put(res)\n\t}()\n\n\tbuf.Write(make([]byte, 65))\n\tif _, err := payload.MarshalSSZ(buf); err != nil {\n\t\treturn fmt.Errorf(\"failed to encoded execution payload to publish: %w\", err)\n\t}\n\tdata := buf.Bytes()\n\tpayloadData := data[65:]\n\tsig, err := signer.Sign(ctx, SigningDomainBlocksV1, p.cfg.L2ChainID, payloadData)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to sign execution payload with signer: %w\", err)\n\t}\n\tcopy(data[:65], sig[:])\n\n\t// compress the full message\n\t// This also copies the data, freeing up the original buffer to go back into the pool\n\tout := snappy.Encode(nil, data)\n\n\treturn p.blocksTopic.Publish(ctx, out)\n}\n\nfunc (p *publisher) Close() error {\n\treturn p.blocksTopic.Close()\n}\n\nfunc JoinGossip(p2pCtx context.Context, self peer.ID, ps *pubsub.PubSub, log log.Logger, cfg *rollup.Config, runCfg GossipRuntimeConfig, gossipIn GossipIn) (GossipOut, error) {\n\tval := guardGossipValidator(log, logValidationResult(self, \"validated block\", log, BuildBlocksValidator(log, cfg, runCfg)))\n\tblocksTopicName := blocksTopicV1(cfg)\n\terr := ps.RegisterTopicValidator(blocksTopicName,\n\t\tval,\n\t\tpubsub.WithValidatorTimeout(3*time.Second),\n\t\tpubsub.WithValidatorConcurrency(4))\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to register blocks gossip topic: %w\", err)\n\t}\n\tblocksTopic, err := ps.Join(blocksTopicName)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to join blocks gossip topic: %w\", err)\n\t}\n\tblocksTopicEvents, err := blocksTopic.EventHandler()\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to create blocks gossip topic handler: %w\", err)\n\t}\n\tgo LogTopicEvents(p2pCtx, log.New(\"topic\", \"blocks\"), blocksTopicEvents)\n\n\t// TODO: block topic scoring parameters\n\t// See prysm: https://github.com/prysmaticlabs/prysm/blob/develop/beacon-chain/p2p/gossip_scoring_params.go\n\t// And research from lighthouse: https://gist.github.com/blacktemplar/5c1862cb3f0e32a1a7fb0b25e79e6e2c\n\t// And docs: https://github.com/libp2p/specs/blob/master/pubsub/gossipsub/gossipsub-v1.1.md#topic-parameter-calculation-and-decay\n\t//err := blocksTopic.SetScoreParams(&pubsub.TopicScoreParams{......})\n\n\tsubscription, err := blocksTopic.Subscribe()\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to subscribe to blocks gossip topic: %w\", err)\n\t}\n\n\tsubscriber := MakeSubscriber(log, BlocksHandler(gossipIn.OnUnsafeL2Payload))\n\tgo subscriber(p2pCtx, subscription)\n\n\treturn &publisher{log: log, cfg: cfg, blocksTopic: blocksTopic, runCfg: runCfg}, nil\n}\n\ntype TopicSubscriber func(ctx context.Context, sub *pubsub.Subscription)\ntype MessageHandler func(ctx context.Context, from peer.ID, msg any) error\n\nfunc BlocksHandler(onBlock func(ctx context.Context, from peer.ID, msg *eth.ExecutionPayload) error) MessageHandler {\n\treturn func(ctx context.Context, from peer.ID, msg any) error {\n\t\tpayload, ok := msg.(*eth.ExecutionPayload)\n\t\tif !ok {\n\t\t\treturn fmt.Errorf(\"expected topic validator to parse and validate data into execution payload, but got %T\", msg)\n\t\t}\n\t\treturn onBlock(ctx, from, payload)\n\t}\n}\n\nfunc MakeSubscriber(log log.Logger, msgHandler MessageHandler) TopicSubscriber {\n\treturn func(ctx context.Context, sub *pubsub.Subscription) {\n\t\ttopicLog := log.New(\"topic\", sub.Topic())\n\t\tfor {\n\t\t\tmsg, err := sub.Next(ctx)\n\t\t\tif err != nil { // ctx was closed, or subscription was closed\n\t\t\t\ttopicLog.Debug(\"stopped subscriber\")\n\t\t\t\treturn\n\t\t\t}\n\t\t\tif msg.ValidatorData == nil {\n\t\t\t\ttopicLog.Error(\"gossip message with no data\", \"from\", msg.ReceivedFrom)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif err := msgHandler(ctx, msg.ReceivedFrom, msg.ValidatorData); err != nil {\n\t\t\t\ttopicLog.Error(\"failed to process gossip message\", \"err\", err)\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc LogTopicEvents(ctx context.Context, log log.Logger, evHandler *pubsub.TopicEventHandler) {\n\tdefer evHandler.Cancel()\n\tfor {\n\t\tev, err := evHandler.NextPeerEvent(ctx)\n\t\tif err != nil {\n\t\t\treturn // ctx closed\n\t\t}\n\t\tswitch ev.Type {\n\t\tcase pubsub.PeerJoin:\n\t\t\tlog.Debug(\"peer joined topic\", \"peer\", ev.Peer)\n\t\tcase pubsub.PeerLeave:\n\t\t\tlog.Debug(\"peer left topic\", \"peer\", ev.Peer)\n\t\tdefault:\n\t\t\tlog.Warn(\"unrecognized topic event\", \"ev\", ev)\n\t\t}\n\t}\n}\n\ntype gossipTracer struct {\n\tm GossipMetricer\n}\n\nfunc (g *gossipTracer) Trace(evt *pb.TraceEvent) {\n\tif g.m != nil {\n\t\tg.m.RecordGossipEvent(int32(*evt.Type))\n\t}\n}"
    }
  ]
}