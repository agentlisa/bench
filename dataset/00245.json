{
  "Title": "RequestL2Range Does Not Return Error if Channel Is Full",
  "Content": "The `RequestL2Range` function queues a range of L2 blocks and [returns early if the channel is full](https://github.com/mantlenetworkio/mantle-v2/blob/bb0ff7002520ee936101c4c263ac02a66e7e3c96/op-node/sources/sync_client.go#L123-L125). However, it does not return an error in this case which means that the partial data will be processed.\n\n\nConsider returning an error in case the channel is full in order to make callers aware.\n\n\n***Update:** Acknowledged, not resolved. The Mantle team stated:*\n\n\n\n> *Not a valid issue.*\n\n\n",
  "Impact": "LOW",
  "Source": "",
  "Code": [
    {
      "filename": "op-node/sources/sync_client.go",
      "content": "package sources\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/ethereum-optimism/optimism/op-node/client\"\n\t\"github.com/ethereum-optimism/optimism/op-node/eth\"\n\t\"github.com/ethereum-optimism/optimism/op-node/rollup\"\n\t\"github.com/ethereum-optimism/optimism/op-node/sources/caching\"\n\t\"github.com/ethereum-optimism/optimism/op-service/backoff\"\n\n\t\"github.com/ethereum/go-ethereum/log\"\n\t\"github.com/libp2p/go-libp2p/core/peer\"\n)\n\nconst (\n\trequestsChannelBufferSize = 1024\n)\n\nvar ErrNoUnsafeL2PayloadChannel = errors.New(\"unsafeL2Payloads channel must not be nil\")\n\n// RpcSyncPeer is a mock PeerID for the RPC sync client.\nvar RpcSyncPeer peer.ID = \"ALT_RPC_SYNC\"\n\n// receivePayload queues the received payload for processing.\n// This may return an error if there's no capacity for the payload.\ntype receivePayload = func(ctx context.Context, from peer.ID, payload *eth.ExecutionPayload) error\n\ntype RPCSync interface {\n\tio.Closer\n\t// Start starts an additional worker syncing job\n\tStart() error\n\t// RequestL2Range signals that the given range should be fetched, implementing the alt-sync interface.\n\tRequestL2Range(ctx context.Context, start uint64, end eth.L2BlockRef) error\n}\n\n// SyncClient implements the driver AltSync interface, including support for fetching an open-ended chain of L2 blocks.\ntype SyncClient struct {\n\t*L2Client\n\n\trequests chan uint64\n\n\tresCtx    context.Context\n\tresCancel context.CancelFunc\n\n\treceivePayload receivePayload\n\twg             sync.WaitGroup\n}\n\ntype SyncClientConfig struct {\n\tL2ClientConfig\n}\n\nfunc SyncClientDefaultConfig(config *rollup.Config, trustRPC bool) *SyncClientConfig {\n\treturn &SyncClientConfig{\n\t\t*L2ClientDefaultConfig(config, trustRPC),\n\t}\n}\n\nfunc NewSyncClient(receiver receivePayload, client client.RPC, log log.Logger, metrics caching.Metrics, config *SyncClientConfig) (*SyncClient, error) {\n\tl2Client, err := NewL2Client(client, log, metrics, &config.L2ClientConfig)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\t// This resource context is shared between all workers that may be started\n\tresCtx, resCancel := context.WithCancel(context.Background())\n\treturn &SyncClient{\n\t\tL2Client:       l2Client,\n\t\tresCtx:         resCtx,\n\t\tresCancel:      resCancel,\n\t\trequests:       make(chan uint64, requestsChannelBufferSize),\n\t\treceivePayload: receiver,\n\t}, nil\n}\n\n// Start starts the syncing background work. This may not be called after Close().\nfunc (s *SyncClient) Start() error {\n\t// TODO(CLI-3635): we can start multiple event loop runners as workers, to parallelize the work\n\ts.wg.Add(1)\n\tgo s.eventLoop()\n\treturn nil\n}\n\n// Close sends a signal to close all concurrent syncing work.\nfunc (s *SyncClient) Close() error {\n\ts.resCancel()\n\ts.wg.Wait()\n\treturn nil\n}\n\nfunc (s *SyncClient) RequestL2Range(ctx context.Context, start, end eth.L2BlockRef) error {\n\t// Drain previous requests now that we have new information\n\tfor len(s.requests) > 0 {\n\t\tselect { // in case requests is being read at the same time, don't block on draining it.\n\t\tcase <-s.requests:\n\t\tdefault:\n\t\t\tbreak\n\t\t}\n\t}\n\n\tendNum := end.Number\n\tif end == (eth.L2BlockRef{}) {\n\t\tn, err := s.rollupCfg.TargetBlockNumber(uint64(time.Now().Unix()))\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif n <= start.Number {\n\t\t\treturn nil\n\t\t}\n\t\tendNum = n\n\t}\n\n\t// TODO(CLI-3635): optimize the by-range fetching with the Engine API payloads-by-range method.\n\n\ts.log.Info(\"Scheduling to fetch trailing missing payloads from backup RPC\", \"start\", start, \"end\", endNum, \"size\", endNum-start.Number-1)\n\n\tfor i := start.Number + 1; i < endNum; i++ {\n\t\tif len(s.requests) == requestsChannelBufferSize {\n\t\t\treturn nil\n\t\t}\n\t\tselect {\n\t\tcase s.requests <- i:\n\t\tcase <-ctx.Done():\n\t\t\treturn ctx.Err()\n\t\t}\n\t}\n\treturn nil\n}\n\n// eventLoop is the main event loop for the sync client.\nfunc (s *SyncClient) eventLoop() {\n\tdefer s.wg.Done()\n\ts.log.Info(\"Starting sync client event loop\")\n\n\tbackoffStrategy := &backoff.ExponentialStrategy{\n\t\tMin:       1000,\n\t\tMax:       20_000,\n\t\tMaxJitter: 250,\n\t}\n\n\tfor {\n\t\tselect {\n\t\tcase <-s.resCtx.Done():\n\t\t\ts.log.Debug(\"Shutting down RPC sync worker\")\n\t\t\treturn\n\t\tcase reqNum := <-s.requests:\n\t\t\ts.log.Debug(\"Sync client left request quantity\", \"quantity\", len(s.requests))\n\t\t\terr := backoff.DoCtx(s.resCtx, 5, backoffStrategy, func() error {\n\t\t\t\t// Limit the maximum time for fetching payloads\n\t\t\t\tctx, cancel := context.WithTimeout(s.resCtx, time.Second*10)\n\t\t\t\tdefer cancel()\n\t\t\t\t// We are only fetching one block at a time here.\n\t\t\t\treturn s.fetchUnsafeBlockFromRpc(ctx, reqNum)\n\t\t\t})\n\t\t\tif err != nil {\n\t\t\t\tif err == s.resCtx.Err() {\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t\ts.log.Error(\"failed syncing L2 block via RPC\", \"err\", err, \"num\", reqNum)\n\t\t\t\t// Reschedule at end of queue\n\t\t\t\tselect {\n\t\t\t\tcase s.requests <- reqNum:\n\t\t\t\tdefault:\n\t\t\t\t\t// drop syncing job if we are too busy with sync jobs already.\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\n\n// fetchUnsafeBlockFromRpc attempts to fetch an unsafe execution payload from the backup unsafe sync RPC.\n// WARNING: This function fails silently (aside from warning logs).\n//\n// Post Shanghai hardfork, the engine API's `PayloadBodiesByRange` method will be much more efficient, but for now,\n// the `eth_getBlockByNumber` method is more widely available.\nfunc (s *SyncClient) fetchUnsafeBlockFromRpc(ctx context.Context, blockNumber uint64) error {\n\ts.log.Info(\"Requesting unsafe payload from backup RPC\", \"block number\", blockNumber)\n\n\tpayload, err := s.PayloadByNumber(ctx, blockNumber)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to fetch payload by number (%d): %w\", blockNumber, err)\n\t}\n\t// Note: the underlying RPC client used for syncing verifies the execution payload blockhash, if set to untrusted.\n\n\ts.log.Info(\"Received unsafe payload from backup RPC\", \"payload\", payload.ID())\n\n\t// Send the retrieved payload to the `unsafeL2Payloads` channel.\n\tif err = s.receivePayload(ctx, RpcSyncPeer, payload); err != nil {\n\t\treturn fmt.Errorf(\"failed to send payload %s into the driver's unsafeL2Payloads channel: %w\", payload.ID(), err)\n\t} else {\n\t\ts.log.Debug(\"Sent received payload into the driver's unsafeL2Payloads channel\", \"payload\", payload.ID())\n\t\treturn nil\n\t}\n}"
    }
  ]
}