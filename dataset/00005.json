{
  "Title": "Ambiguous PubdataPricingMode Configuration",
  "Content": "The [`setValidiumMode` function](https://github.com/matter-labs/era-contracts/blob/d5c5d7a7c3a458f131e5de72c6c134a172e6763d/l1-contracts/contracts/state-transition/chain-deps/facets/Admin.sol#L89) of the `Admin` facet allows the admin role to toggle the pubdata pricing mode between `Rollup` and `Validium` as long as no batches are committed. While the function name only suggests that the mode can be changed from `Rollup` to `Validium`, the mode can freely be set (e.g., from `Rollup` to `Validium` and back to `Rollup`). Furthermore, the comment *\"Validium mode can be set only before the first batch is committed\"* is wrong for two reasons:\n\n\n1. Batches can be committed but then [reverted](https://github.com/matter-labs/era-contracts/blob/d5c5d7a7c3a458f131e5de72c6c134a172e6763d/l1-contracts/contracts/state-transition/chain-deps/facets/Executor.sol#L472) again to the initial state to set the mode differently.\n2. The `PubdataPricingMode` can be set through the [`changeFeeParams` function](https://github.com/matter-labs/era-contracts/blob/d5c5d7a7c3a458f131e5de72c6c134a172e6763d/l1-contracts/contracts/state-transition/chain-deps/facets/Admin.sol#L67) at any time.\n\n\nThus, when users request an L2 transaction through the `Mailbox` facet, they sometimes [could get charged for L1 PubData](https://github.com/matter-labs/era-contracts/blob/d5c5d7a7c3a458f131e5de72c6c134a172e6763d/l1-contracts/contracts/state-transition/chain-deps/facets/Mailbox.sol#L162), and sometimes not. Moreover, if the `Validium` mode is motivated by [enterprise or privacy reasons](https://docs.zksync.io/zk-stack/concepts/validiums.html#potential-use-cases), a mode change to `Rollup` would entail leaking sensitive data.\n\n\nConsider clarifying the intention of when and how the `PubdataPricingMode` may be changed. For example, ensure that the `PubdataPricingMode` can only be changed until the first batch is written to [`storedBatchHashes`](https://github.com/matter-labs/era-contracts/blob/d5c5d7a7c3a458f131e5de72c6c134a172e6763d/l1-contracts/contracts/state-transition/chain-deps/ZkSyncStateTransitionStorage.sol#L86), which is not deleted during a batch revert. In addition, consider checking that the mode is not changed during `changeFeeParams`.\n\n\n***Update:** Resolved in [pull request #292](https://github.com/matter-labs/era-contracts/pull/292) and [pull request #298](https://github.com/matter-labs/era-contracts/pull/298/files#diff-092566068f0ff90bac8444b21b086d15fbd1507d052fd62f4b141a03b1c87796). The Matter Labs team stated:*\n\n\n\n> *`PubdataPricingMode` can be changed only before the first batch is processed, this is also reflected in the comment and in the doc-comments introduced with the fix in L-06.*\n\n\n",
  "Impact": "MEDIUM",
  "Source": "",
  "Code": [
    {
      "filename": "l1-contracts/contracts/state-transition/chain-deps/facets/Admin.sol",
      "content": "// SPDX-License-Identifier: MIT\n\npragma solidity 0.8.20;\n\nimport {IAdmin} from \"../../chain-interfaces/IAdmin.sol\";\nimport {Diamond} from \"../../libraries/Diamond.sol\";\nimport {MAX_GAS_PER_TRANSACTION} from \"../../../common/Config.sol\";\nimport {FeeParams, PubdataPricingMode} from \"../ZkSyncStateTransitionStorage.sol\";\nimport {ZkSyncStateTransitionBase} from \"./ZkSyncStateTransitionBase.sol\";\nimport {IStateTransitionManager} from \"../../IStateTransitionManager.sol\";\n\n// While formally the following import is not used, it is needed to inherit documentation from it\nimport {IZkSyncStateTransitionBase} from \"../../chain-interfaces/IZkSyncStateTransitionBase.sol\";\n\n/// @title Admin Contract controls access rights for contract management.\n/// @author Matter Labs\n/// @custom:security-contact security@matterlabs.dev\ncontract AdminFacet is ZkSyncStateTransitionBase, IAdmin {\n    /// @inheritdoc IZkSyncStateTransitionBase\n    string public constant override getName = \"AdminFacet\";\n\n    /// @inheritdoc IAdmin\n    function setPendingAdmin(address _newPendingAdmin) external onlyAdmin {\n        // Save previous value into the stack to put it into the event later\n        address oldPendingAdmin = s.pendingAdmin;\n        // Change pending admin\n        s.pendingAdmin = _newPendingAdmin;\n        emit NewPendingAdmin(oldPendingAdmin, _newPendingAdmin);\n    }\n\n    /// @inheritdoc IAdmin\n    function acceptAdmin() external {\n        address pendingAdmin = s.pendingAdmin;\n        require(msg.sender == pendingAdmin, \"n4\"); // Only proposed by current admin address can claim the admin rights\n\n        address previousAdmin = s.admin;\n        s.admin = pendingAdmin;\n        delete s.pendingAdmin;\n\n        emit NewPendingAdmin(pendingAdmin, address(0));\n        emit NewAdmin(previousAdmin, pendingAdmin);\n    }\n\n    /// @inheritdoc IAdmin\n    function setValidator(address _validator, bool _active) external onlyStateTransitionManager {\n        s.validators[_validator] = _active;\n        emit ValidatorStatusUpdate(_validator, _active);\n    }\n\n    /// @inheritdoc IAdmin\n    function setPorterAvailability(bool _zkPorterIsAvailable) external onlyStateTransitionManager {\n        // Change the porter availability\n        s.zkPorterIsAvailable = _zkPorterIsAvailable;\n        emit IsPorterAvailableStatusUpdate(_zkPorterIsAvailable);\n    }\n\n    /// @inheritdoc IAdmin\n    function setPriorityTxMaxGasLimit(uint256 _newPriorityTxMaxGasLimit) external onlyStateTransitionManager {\n        require(_newPriorityTxMaxGasLimit <= MAX_GAS_PER_TRANSACTION, \"n5\");\n\n        uint256 oldPriorityTxMaxGasLimit = s.priorityTxMaxGasLimit;\n        s.priorityTxMaxGasLimit = _newPriorityTxMaxGasLimit;\n        emit NewPriorityTxMaxGasLimit(oldPriorityTxMaxGasLimit, _newPriorityTxMaxGasLimit);\n    }\n\n    /// @inheritdoc IAdmin\n    function changeFeeParams(FeeParams calldata _newFeeParams) external onlyAdminOrStateTransitionManager {\n        // Double checking that the new fee params are valid, i.e.\n        // the maximal pubdata per batch is not less than the maximal pubdata per priority transaction.\n        require(_newFeeParams.maxPubdataPerBatch >= _newFeeParams.priorityTxMaxPubdata, \"n6\");\n\n        FeeParams memory oldFeeParams = s.feeParams;\n        s.feeParams = _newFeeParams;\n\n        emit NewFeeParams(oldFeeParams, _newFeeParams);\n    }\n\n    /// @inheritdoc IAdmin\n    function setTokenMultiplier(uint128 _nominator, uint128 _denominator) external onlyAdminOrStateTransitionManager {\n        uint128 oldNominator = s.baseTokenGasPriceMultiplierNominator;\n        uint128 oldDenominator = s.baseTokenGasPriceMultiplierDenominator;\n\n        s.baseTokenGasPriceMultiplierNominator = _nominator;\n        s.baseTokenGasPriceMultiplierDenominator = _denominator;\n\n        emit NewBaseTokenMultiplier(oldNominator, oldDenominator, _nominator, _denominator);\n    }\n\n    function setValidiumMode(PubdataPricingMode _validiumMode) external onlyAdmin {\n        require(s.totalBatchesCommitted == 0, \"AdminFacet: set validium only after genesis\"); // Validium mode can be set only before the first batch is committed\n        s.feeParams.pubdataPricingMode = _validiumMode;\n        emit ValidiumModeStatusUpdate(_validiumMode);\n    }\n\n    /*//////////////////////////////////////////////////////////////\n                            UPGRADE EXECUTION\n    //////////////////////////////////////////////////////////////*/\n\n    /// upgrade a specific chain\n    function upgradeChainFromVersion(\n        uint256 _oldProtocolVersion,\n        Diamond.DiamondCutData calldata _diamondCut\n    ) external onlyAdminOrStateTransitionManager {\n        bytes32 cutHashInput = keccak256(abi.encode(_diamondCut));\n        require(\n            cutHashInput == IStateTransitionManager(s.stateTransitionManager).upgradeCutHash(_oldProtocolVersion),\n            \"StateTransition: cutHash mismatch\"\n        );\n\n        require(\n            s.protocolVersion == _oldProtocolVersion,\n            \"StateTransition: protocolVersion mismatch in STC when upgrading\"\n        );\n        Diamond.diamondCut(_diamondCut);\n        emit ExecuteUpgrade(_diamondCut);\n        require(\n            s.protocolVersion > _oldProtocolVersion,\n            \"StateTransition: protocolVersion mismatch in STC after upgrading\"\n        );\n    }\n\n    /// @inheritdoc IAdmin\n    function executeUpgrade(Diamond.DiamondCutData calldata _diamondCut) external onlyStateTransitionManager {\n        Diamond.diamondCut(_diamondCut);\n        emit ExecuteUpgrade(_diamondCut);\n    }\n\n    /*//////////////////////////////////////////////////////////////\n                            CONTRACT FREEZING\n    //////////////////////////////////////////////////////////////*/\n\n    /// @inheritdoc IAdmin\n    function freezeDiamond() external onlyAdminOrStateTransitionManager {\n        Diamond.DiamondStorage storage diamondStorage = Diamond.getDiamondStorage();\n\n        require(!diamondStorage.isFrozen, \"a9\"); // diamond proxy is frozen already\n        diamondStorage.isFrozen = true;\n\n        emit Freeze();\n    }\n\n    /// @inheritdoc IAdmin\n    function unfreezeDiamond() external onlyAdminOrStateTransitionManager {\n        Diamond.DiamondStorage storage diamondStorage = Diamond.getDiamondStorage();\n\n        require(diamondStorage.isFrozen, \"a7\"); // diamond proxy is not frozen\n        diamondStorage.isFrozen = false;\n\n        emit Unfreeze();\n    }\n}"
    },
    {
      "filename": "l1-contracts/contracts/state-transition/chain-deps/facets/Executor.sol",
      "content": "// SPDX-License-Identifier: MIT\n\npragma solidity 0.8.20;\n\nimport {ZkSyncStateTransitionBase} from \"./ZkSyncStateTransitionBase.sol\";\nimport {COMMIT_TIMESTAMP_NOT_OLDER, COMMIT_TIMESTAMP_APPROXIMATION_DELTA, EMPTY_STRING_KECCAK, L2_TO_L1_LOG_SERIALIZE_SIZE, MAX_L2_TO_L1_LOGS_COMMITMENT_BYTES, PACKED_L2_BLOCK_TIMESTAMP_MASK, PUBLIC_INPUT_SHIFT, POINT_EVALUATION_PRECOMPILE_ADDR} from \"../../../common/Config.sol\";\nimport {IExecutor, L2_LOG_ADDRESS_OFFSET, L2_LOG_KEY_OFFSET, L2_LOG_VALUE_OFFSET, SystemLogKey, LogProcessingOutput, PubdataSource, BLS_MODULUS, PUBDATA_COMMITMENT_SIZE, PUBDATA_COMMITMENT_CLAIMED_VALUE_OFFSET, PUBDATA_COMMITMENT_COMMITMENT_OFFSET, MAX_NUMBER_OF_BLOBS, TOTAL_BLOBS_IN_COMMITMENT} from \"../../chain-interfaces/IExecutor.sol\";\nimport {PriorityQueue, PriorityOperation} from \"../../libraries/PriorityQueue.sol\";\nimport {UncheckedMath} from \"../../../common/libraries/UncheckedMath.sol\";\nimport {UnsafeBytes} from \"../../../common/libraries/UnsafeBytes.sol\";\nimport {VerifierParams} from \"../../chain-interfaces/IVerifier.sol\";\nimport {L2_BOOTLOADER_ADDRESS, L2_TO_L1_MESSENGER_SYSTEM_CONTRACT_ADDR, L2_SYSTEM_CONTEXT_SYSTEM_CONTRACT_ADDR, L2_PUBDATA_CHUNK_PUBLISHER_ADDR} from \"../../../common/L2ContractAddresses.sol\";\nimport {PubdataPricingMode} from \"../ZkSyncStateTransitionStorage.sol\";\nimport {IStateTransitionManager} from \"../../IStateTransitionManager.sol\";\n\n// While formally the following import is not used, it is needed to inherit documentation from it\nimport {IZkSyncStateTransitionBase} from \"../../chain-interfaces/IZkSyncStateTransitionBase.sol\";\n\n/// @title zkSync hyperchain Executor contract capable of processing events emitted in the zkSync hyperchain protocol.\n/// @author Matter Labs\n/// @custom:security-contact security@matterlabs.dev\ncontract ExecutorFacet is ZkSyncStateTransitionBase, IExecutor {\n    using UncheckedMath for uint256;\n    using PriorityQueue for PriorityQueue.Queue;\n\n    /// @inheritdoc IZkSyncStateTransitionBase\n    string public constant override getName = \"ExecutorFacet\";\n\n    /// @dev Process one batch commit using the previous batch StoredBatchInfo\n    /// @dev returns new batch StoredBatchInfo\n    /// @notice Does not change storage\n    function _commitOneBatch(\n        StoredBatchInfo memory _previousBatch,\n        CommitBatchInfo calldata _newBatch,\n        bytes32 _expectedSystemContractUpgradeTxHash\n    ) internal view returns (StoredBatchInfo memory) {\n        require(_newBatch.batchNumber == _previousBatch.batchNumber + 1, \"f\"); // only commit next batch\n\n        uint8 pubdataSource = uint8(bytes1(_newBatch.pubdataCommitments[0]));\n        require(pubdataSource == uint8(PubdataSource.Calldata) || pubdataSource == uint8(PubdataSource.Blob), \"us\");\n\n        // Check that batch contain all meta information for L2 logs.\n        // Get the chained hash of priority transaction hashes.\n        LogProcessingOutput memory logOutput = _processL2Logs(_newBatch, _expectedSystemContractUpgradeTxHash);\n\n        bytes32[] memory blobCommitments = new bytes32[](MAX_NUMBER_OF_BLOBS);\n        bytes32[] memory blobHashes = new bytes32[](MAX_NUMBER_OF_BLOBS);\n        if (s.feeParams.pubdataPricingMode == PubdataPricingMode.Validium) {\n            // skipping data validation for validium, we just check that the data is empty\n            require(logOutput.pubdataHash == 0x00, \"v0h\");\n            require(_newBatch.pubdataCommitments.length == 1);\n        } else if (pubdataSource == uint8(PubdataSource.Blob)) {\n            // We want only want to include the actual blob linear hashes when we send pubdata via blobs.\n            // Otherwise we should be using bytes32(0)\n            blobHashes[0] = logOutput.blob1Hash;\n            blobHashes[1] = logOutput.blob2Hash;\n            // In this scenario, pubdataCommitments is a list of: opening point (16 bytes) || claimed value (32 bytes) || commitment (48 bytes) || proof (48 bytes)) = 144 bytes\n            blobCommitments = _verifyBlobInformation(_newBatch.pubdataCommitments[1:], blobHashes);\n        } else if (pubdataSource == uint8(PubdataSource.Calldata)) {\n            // In this scenario pubdataCommitments is actual pubdata consisting of l2 to l1 logs, l2 to l1 message, compressed smart contract bytecode, and compressed state diffs\n            require(\n                logOutput.pubdataHash ==\n                    keccak256(_newBatch.pubdataCommitments[1:_newBatch.pubdataCommitments.length - 32]),\n                \"wp\"\n            );\n            blobHashes[0] = logOutput.blob1Hash;\n            blobCommitments[0] = bytes32(\n                _newBatch.pubdataCommitments[_newBatch.pubdataCommitments.length - 32:_newBatch\n                    .pubdataCommitments\n                    .length]\n            );\n        }\n\n        require(_previousBatch.batchHash == logOutput.previousBatchHash, \"l\");\n        // Check that the priority operation hash in the L2 logs is as expected\n        require(logOutput.chainedPriorityTxsHash == _newBatch.priorityOperationsHash, \"t\");\n        // Check that the number of processed priority operations is as expected\n        require(logOutput.numberOfLayer1Txs == _newBatch.numberOfLayer1Txs, \"ta\");\n\n        // Check the timestamp of the new batch\n        _verifyBatchTimestamp(logOutput.packedBatchAndL2BlockTimestamp, _newBatch.timestamp, _previousBatch.timestamp);\n\n        // Create batch commitment for the proof verification\n        bytes32 commitment = _createBatchCommitment(_newBatch, logOutput.stateDiffHash, blobCommitments, blobHashes);\n\n        return\n            StoredBatchInfo(\n                _newBatch.batchNumber,\n                _newBatch.newStateRoot,\n                _newBatch.indexRepeatedStorageChanges,\n                _newBatch.numberOfLayer1Txs,\n                _newBatch.priorityOperationsHash,\n                logOutput.l2LogsTreeRoot,\n                _newBatch.timestamp,\n                commitment\n            );\n    }\n\n    /// @notice checks that the timestamps of both the new batch and the new L2 block are correct.\n    /// @param _packedBatchAndL2BlockTimestamp - packed batch and L2 block timestamp in a format of batchTimestamp * 2**128 + l2BatchTimestamp\n    /// @param _expectedBatchTimestamp - expected batch timestamp\n    /// @param _previousBatchTimestamp - the timestamp of the previous batch\n    function _verifyBatchTimestamp(\n        uint256 _packedBatchAndL2BlockTimestamp,\n        uint256 _expectedBatchTimestamp,\n        uint256 _previousBatchTimestamp\n    ) internal view {\n        // Check that the timestamp that came from the system context is expected\n        uint256 batchTimestamp = _packedBatchAndL2BlockTimestamp >> 128;\n        require(batchTimestamp == _expectedBatchTimestamp, \"tb\");\n\n        // While the fact that _previousBatchTimestamp < batchTimestamp is already checked on L2,\n        // we double check it here for clarity\n        require(_previousBatchTimestamp < batchTimestamp, \"h3\");\n\n        uint256 lastL2BlockTimestamp = _packedBatchAndL2BlockTimestamp & PACKED_L2_BLOCK_TIMESTAMP_MASK;\n\n        // All L2 blocks have timestamps within the range of [batchTimestamp, lastL2BlockTimestamp].\n        // So here we need to only double check that:\n        // - The timestamp of the batch is not too small.\n        // - The timestamp of the last L2 block is not too big.\n        require(block.timestamp - COMMIT_TIMESTAMP_NOT_OLDER <= batchTimestamp, \"h1\"); // New batch timestamp is too small\n        require(lastL2BlockTimestamp <= block.timestamp + COMMIT_TIMESTAMP_APPROXIMATION_DELTA, \"h2\"); // The last L2 block timestamp is too big\n    }\n\n    /// @dev Check that L2 logs are proper and batch contain all meta information for them\n    /// @dev The logs processed here should line up such that only one log for each key from the\n    ///      SystemLogKey enum in Constants.sol is processed per new batch.\n    /// @dev Data returned from here will be used to form the batch commitment.\n    function _processL2Logs(\n        CommitBatchInfo calldata _newBatch,\n        bytes32 _expectedSystemContractUpgradeTxHash\n    ) internal pure returns (LogProcessingOutput memory logOutput) {\n        // Copy L2 to L1 logs into memory.\n        bytes memory emittedL2Logs = _newBatch.systemLogs;\n\n        // Used as bitmap to set/check log processing happens exactly once.\n        // See SystemLogKey enum in Constants.sol for ordering.\n        uint256 processedLogs;\n\n        // linear traversal of the logs\n        for (uint256 i = 0; i < emittedL2Logs.length; i = i.uncheckedAdd(L2_TO_L1_LOG_SERIALIZE_SIZE)) {\n            // Extract the values to be compared to/used such as the log sender, key, and value\n            (address logSender, ) = UnsafeBytes.readAddress(emittedL2Logs, i + L2_LOG_ADDRESS_OFFSET);\n            (uint256 logKey, ) = UnsafeBytes.readUint256(emittedL2Logs, i + L2_LOG_KEY_OFFSET);\n            (bytes32 logValue, ) = UnsafeBytes.readBytes32(emittedL2Logs, i + L2_LOG_VALUE_OFFSET);\n\n            // Ensure that the log hasn't been processed already\n            require(!_checkBit(processedLogs, uint8(logKey)), \"kp\");\n            processedLogs = _setBit(processedLogs, uint8(logKey));\n\n            // Need to check that each log was sent by the correct address.\n            if (logKey == uint256(SystemLogKey.L2_TO_L1_LOGS_TREE_ROOT_KEY)) {\n                require(logSender == L2_TO_L1_MESSENGER_SYSTEM_CONTRACT_ADDR, \"lm\");\n                logOutput.l2LogsTreeRoot = logValue;\n            } else if (logKey == uint256(SystemLogKey.TOTAL_L2_TO_L1_PUBDATA_KEY)) {\n                require(logSender == L2_TO_L1_MESSENGER_SYSTEM_CONTRACT_ADDR, \"ln\");\n                logOutput.pubdataHash = logValue;\n            } else if (logKey == uint256(SystemLogKey.STATE_DIFF_HASH_KEY)) {\n                require(logSender == L2_TO_L1_MESSENGER_SYSTEM_CONTRACT_ADDR, \"lb\");\n                logOutput.stateDiffHash = logValue;\n            } else if (logKey == uint256(SystemLogKey.PACKED_BATCH_AND_L2_BLOCK_TIMESTAMP_KEY)) {\n                require(logSender == L2_SYSTEM_CONTEXT_SYSTEM_CONTRACT_ADDR, \"sc\");\n                logOutput.packedBatchAndL2BlockTimestamp = uint256(logValue);\n            } else if (logKey == uint256(SystemLogKey.PREV_BATCH_HASH_KEY)) {\n                require(logSender == L2_SYSTEM_CONTEXT_SYSTEM_CONTRACT_ADDR, \"sv\");\n                logOutput.previousBatchHash = logValue;\n            } else if (logKey == uint256(SystemLogKey.CHAINED_PRIORITY_TXN_HASH_KEY)) {\n                require(logSender == L2_BOOTLOADER_ADDRESS, \"bl\");\n                logOutput.chainedPriorityTxsHash = logValue;\n            } else if (logKey == uint256(SystemLogKey.NUMBER_OF_LAYER_1_TXS_KEY)) {\n                require(logSender == L2_BOOTLOADER_ADDRESS, \"bk\");\n                logOutput.numberOfLayer1Txs = uint256(logValue);\n            } else if (logKey == uint256(SystemLogKey.BLOB_ONE_HASH_KEY)) {\n                require(logSender == L2_PUBDATA_CHUNK_PUBLISHER_ADDR, \"pc\");\n                logOutput.blob1Hash = logValue;\n            } else if (logKey == uint256(SystemLogKey.BLOB_TWO_HASH_KEY)) {\n                require(logSender == L2_PUBDATA_CHUNK_PUBLISHER_ADDR, \"pd\");\n                logOutput.blob2Hash = logValue;\n            } else if (logKey == uint256(SystemLogKey.EXPECTED_SYSTEM_CONTRACT_UPGRADE_TX_HASH_KEY)) {\n                require(logSender == L2_BOOTLOADER_ADDRESS, \"bu\");\n                require(_expectedSystemContractUpgradeTxHash == logValue, \"ut\");\n            } else {\n                revert(\"ul\");\n            }\n        }\n\n        // We only require 9 logs to be checked, the 10th is if we are expecting a protocol upgrade\n        // Without the protocol upgrade we expect 9 logs: 2^9 - 1 = 511\n        // With the protocol upgrade we expect 8 logs: 2^10 - 1 = 1023\n        if (_expectedSystemContractUpgradeTxHash == bytes32(0)) {\n            require(processedLogs == 511, \"b7\");\n        } else {\n            require(processedLogs == 1023, \"b8\");\n        }\n    }\n\n    /// @inheritdoc IExecutor\n    function commitBatches(\n        StoredBatchInfo memory _lastCommittedBatchData,\n        CommitBatchInfo[] calldata _newBatchesData\n    ) external nonReentrant onlyValidator {\n        _commitBatches(_lastCommittedBatchData, _newBatchesData);\n    }\n\n    /// @inheritdoc IExecutor\n    function commitBatchesSharedBridge(\n        uint256, // _chainId\n        StoredBatchInfo memory _lastCommittedBatchData,\n        CommitBatchInfo[] calldata _newBatchesData\n    ) external nonReentrant onlyValidator {\n        _commitBatches(_lastCommittedBatchData, _newBatchesData);\n    }\n\n    function _commitBatches(\n        StoredBatchInfo memory _lastCommittedBatchData,\n        CommitBatchInfo[] calldata _newBatchesData\n    ) internal {\n        // check that we have the right protocol version\n        // three comments:\n        // 1. A chain has to keep their protocol version up to date, as processing a block requires the latest or previous protocol version\n        // to solve this we will need to add the feature to create batches with only the protocol upgrade tx, without any other txs.\n        // 2. A chain might become out of sync if it launches while we are in the middle of a protocol upgrade. This would mean they cannot process their genesis upgrade\n        // as their protocolversion would be outdated, and they also cannot process the protocol upgrade tx as they have a pending upgrade.\n        // 3. The protocol upgrade is increased in the BaseZkSyncUpgrade, in the executor only the systemContractsUpgradeTxHash is checked\n        require(\n            IStateTransitionManager(s.stateTransitionManager).protocolVersion() == s.protocolVersion,\n            \"Executor facet: wrong protocol version\"\n        );\n        // With the new changes for EIP-4844, namely the restriction on number of blobs per block, we only allow for a single batch to be committed at a time.\n        require(_newBatchesData.length == 1, \"e4\");\n        // Check that we commit batches after last committed batch\n        require(s.storedBatchHashes[s.totalBatchesCommitted] == _hashStoredBatchInfo(_lastCommittedBatchData), \"i\"); // incorrect previous batch data\n\n        bytes32 systemContractsUpgradeTxHash = s.l2SystemContractsUpgradeTxHash;\n        // Upgrades are rarely done so we optimize a case with no active system contracts upgrade.\n        if (systemContractsUpgradeTxHash == bytes32(0) || s.l2SystemContractsUpgradeBatchNumber != 0) {\n            _commitBatchesWithoutSystemContractsUpgrade(_lastCommittedBatchData, _newBatchesData);\n        } else {\n            _commitBatchesWithSystemContractsUpgrade(\n                _lastCommittedBatchData,\n                _newBatchesData,\n                systemContractsUpgradeTxHash\n            );\n        }\n\n        s.totalBatchesCommitted = s.totalBatchesCommitted + _newBatchesData.length;\n    }\n\n    /// @dev Commits new batches without any system contracts upgrade.\n    /// @param _lastCommittedBatchData The data of the last committed batch.\n    /// @param _newBatchesData An array of batch data that needs to be committed.\n    function _commitBatchesWithoutSystemContractsUpgrade(\n        StoredBatchInfo memory _lastCommittedBatchData,\n        CommitBatchInfo[] calldata _newBatchesData\n    ) internal {\n        for (uint256 i = 0; i < _newBatchesData.length; i = i.uncheckedInc()) {\n            _lastCommittedBatchData = _commitOneBatch(_lastCommittedBatchData, _newBatchesData[i], bytes32(0));\n\n            s.storedBatchHashes[_lastCommittedBatchData.batchNumber] = _hashStoredBatchInfo(_lastCommittedBatchData);\n            emit BlockCommit(\n                _lastCommittedBatchData.batchNumber,\n                _lastCommittedBatchData.batchHash,\n                _lastCommittedBatchData.commitment\n            );\n        }\n    }\n\n    /// @dev Commits new batches with a system contracts upgrade transaction.\n    /// @param _lastCommittedBatchData The data of the last committed batch.\n    /// @param _newBatchesData An array of batch data that needs to be committed.\n    /// @param _systemContractUpgradeTxHash The transaction hash of the system contract upgrade.\n    function _commitBatchesWithSystemContractsUpgrade(\n        StoredBatchInfo memory _lastCommittedBatchData,\n        CommitBatchInfo[] calldata _newBatchesData,\n        bytes32 _systemContractUpgradeTxHash\n    ) internal {\n        // The system contract upgrade is designed to be executed atomically with the new bootloader, a default account,\n        // ZKP verifier, and other system parameters. Hence, we ensure that the upgrade transaction is\n        // carried out within the first batch committed after the upgrade.\n\n        // While the logic of the contract ensures that the s.l2SystemContractsUpgradeBatchNumber is 0 when this function is called,\n        // this check is added just in case. Since it is a hot read, it does not encure noticeable gas cost.\n        require(s.l2SystemContractsUpgradeBatchNumber == 0, \"ik\");\n\n        // Save the batch number where the upgrade transaction was executed.\n        s.l2SystemContractsUpgradeBatchNumber = _newBatchesData[0].batchNumber;\n\n        for (uint256 i = 0; i < _newBatchesData.length; i = i.uncheckedInc()) {\n            // The upgrade transaction must only be included in the first batch.\n            bytes32 expectedUpgradeTxHash = i == 0 ? _systemContractUpgradeTxHash : bytes32(0);\n            _lastCommittedBatchData = _commitOneBatch(\n                _lastCommittedBatchData,\n                _newBatchesData[i],\n                expectedUpgradeTxHash\n            );\n\n            s.storedBatchHashes[_lastCommittedBatchData.batchNumber] = _hashStoredBatchInfo(_lastCommittedBatchData);\n            emit BlockCommit(\n                _lastCommittedBatchData.batchNumber,\n                _lastCommittedBatchData.batchHash,\n                _lastCommittedBatchData.commitment\n            );\n        }\n    }\n\n    /// @dev Pops the priority operations from the priority queue and returns a rolling hash of operations\n    function _collectOperationsFromPriorityQueue(uint256 _nPriorityOps) internal returns (bytes32 concatHash) {\n        concatHash = EMPTY_STRING_KECCAK;\n\n        for (uint256 i = 0; i < _nPriorityOps; i = i.uncheckedInc()) {\n            PriorityOperation memory priorityOp = s.priorityQueue.popFront();\n            concatHash = keccak256(abi.encode(concatHash, priorityOp.canonicalTxHash));\n        }\n    }\n\n    /// @dev Executes one batch\n    /// @dev 1. Processes all pending operations (Complete priority requests)\n    /// @dev 2. Finalizes batch on Ethereum\n    /// @dev _executedBatchIdx is an index in the array of the batches that we want to execute together\n    function _executeOneBatch(StoredBatchInfo memory _storedBatch, uint256 _executedBatchIdx) internal {\n        uint256 currentBatchNumber = _storedBatch.batchNumber;\n        require(currentBatchNumber == s.totalBatchesExecuted + _executedBatchIdx + 1, \"k\"); // Execute batches in order\n        require(\n            _hashStoredBatchInfo(_storedBatch) == s.storedBatchHashes[currentBatchNumber],\n            \"exe10\" // executing batch should be committed\n        );\n\n        bytes32 priorityOperationsHash = _collectOperationsFromPriorityQueue(_storedBatch.numberOfLayer1Txs);\n        require(priorityOperationsHash == _storedBatch.priorityOperationsHash, \"x\"); // priority operations hash does not match to expected\n\n        // Save root hash of L2 -> L1 logs tree\n        s.l2LogsRootHashes[currentBatchNumber] = _storedBatch.l2LogsTreeRoot;\n    }\n\n    /// @inheritdoc IExecutor\n    function executeBatchesSharedBridge(\n        uint256,\n        StoredBatchInfo[] calldata _batchesData\n    ) external nonReentrant onlyValidator {\n        _executeBatches(_batchesData);\n    }\n\n    /// @inheritdoc IExecutor\n    function executeBatches(StoredBatchInfo[] calldata _batchesData) external nonReentrant onlyValidator {\n        _executeBatches(_batchesData);\n    }\n\n    function _executeBatches(StoredBatchInfo[] calldata _batchesData) internal {\n        uint256 nBatches = _batchesData.length;\n        for (uint256 i = 0; i < nBatches; i = i.uncheckedInc()) {\n            _executeOneBatch(_batchesData[i], i);\n            emit BlockExecution(_batchesData[i].batchNumber, _batchesData[i].batchHash, _batchesData[i].commitment);\n        }\n\n        uint256 newTotalBatchesExecuted = s.totalBatchesExecuted + nBatches;\n        s.totalBatchesExecuted = newTotalBatchesExecuted;\n        require(newTotalBatchesExecuted <= s.totalBatchesVerified, \"n\"); // Can't execute batches more than committed and proven currently.\n\n        uint256 batchWhenUpgradeHappened = s.l2SystemContractsUpgradeBatchNumber;\n        if (batchWhenUpgradeHappened != 0 && batchWhenUpgradeHappened <= newTotalBatchesExecuted) {\n            delete s.l2SystemContractsUpgradeTxHash;\n            delete s.l2SystemContractsUpgradeBatchNumber;\n        }\n    }\n\n    /// @inheritdoc IExecutor\n    function proveBatches(\n        StoredBatchInfo calldata _prevBatch,\n        StoredBatchInfo[] calldata _committedBatches,\n        ProofInput calldata _proof\n    ) external nonReentrant onlyValidator {\n        _proveBatches(_prevBatch, _committedBatches, _proof);\n    }\n\n    /// @inheritdoc IExecutor\n    function proveBatchesSharedBridge(\n        uint256, // _chainId\n        StoredBatchInfo calldata _prevBatch,\n        StoredBatchInfo[] calldata _committedBatches,\n        ProofInput calldata _proof\n    ) external nonReentrant onlyValidator {\n        _proveBatches(_prevBatch, _committedBatches, _proof);\n    }\n\n    function _proveBatches(\n        StoredBatchInfo calldata _prevBatch,\n        StoredBatchInfo[] calldata _committedBatches,\n        ProofInput calldata _proof\n    ) internal {\n        // Save the variables into the stack to save gas on reading them later\n        uint256 currentTotalBatchesVerified = s.totalBatchesVerified;\n        uint256 committedBatchesLength = _committedBatches.length;\n\n        // Save the variable from the storage to memory to save gas\n        VerifierParams memory verifierParams = s.verifierParams;\n\n        // Initialize the array, that will be used as public input to the ZKP\n        uint256[] memory proofPublicInput = new uint256[](committedBatchesLength);\n\n        // Check that the batch passed by the validator is indeed the first unverified batch\n        require(_hashStoredBatchInfo(_prevBatch) == s.storedBatchHashes[currentTotalBatchesVerified], \"t1\");\n\n        bytes32 prevBatchCommitment = _prevBatch.commitment;\n        for (uint256 i = 0; i < committedBatchesLength; i = i.uncheckedInc()) {\n            currentTotalBatchesVerified = currentTotalBatchesVerified.uncheckedInc();\n            require(\n                _hashStoredBatchInfo(_committedBatches[i]) == s.storedBatchHashes[currentTotalBatchesVerified],\n                \"o1\"\n            );\n\n            bytes32 currentBatchCommitment = _committedBatches[i].commitment;\n            proofPublicInput[i] = _getBatchProofPublicInput(\n                prevBatchCommitment,\n                currentBatchCommitment,\n                verifierParams\n            );\n\n            prevBatchCommitment = currentBatchCommitment;\n        }\n        require(currentTotalBatchesVerified <= s.totalBatchesCommitted, \"q\");\n\n        // #if DUMMY_VERIFIER\n\n        // Additional level of protection for the mainnet\n        assert(block.chainid != 1);\n        // We allow skipping the zkp verification for the test(net) environment\n        // If the proof is not empty, verify it, otherwise, skip the verification\n        if (_proof.serializedProof.length > 0) {\n            _verifyProof(proofPublicInput, _proof);\n        }\n        // #else\n        _verifyProof(proofPublicInput, _proof);\n        // #endif\n\n        emit BlocksVerification(s.totalBatchesVerified, currentTotalBatchesVerified);\n        s.totalBatchesVerified = currentTotalBatchesVerified;\n    }\n\n    function _verifyProof(uint256[] memory proofPublicInput, ProofInput calldata _proof) internal view {\n        // We can only process 1 batch proof at a time.\n        require(proofPublicInput.length == 1, \"t4\");\n\n        bool successVerifyProof = s.verifier.verify(\n            proofPublicInput,\n            _proof.serializedProof,\n            _proof.recursiveAggregationInput\n        );\n        require(successVerifyProof, \"p\"); // Proof verification fail\n    }\n\n    /// @dev Gets zk proof public input\n    function _getBatchProofPublicInput(\n        bytes32 _prevBatchCommitment,\n        bytes32 _currentBatchCommitment,\n        VerifierParams memory _verifierParams\n    ) internal pure returns (uint256) {\n        return\n            uint256(\n                keccak256(\n                    abi.encodePacked(\n                        _prevBatchCommitment,\n                        _currentBatchCommitment,\n                        _verifierParams.recursionNodeLevelVkHash,\n                        _verifierParams.recursionLeafLevelVkHash\n                    )\n                )\n            ) >> PUBLIC_INPUT_SHIFT;\n    }\n\n    /// @inheritdoc IExecutor\n    function revertBatches(uint256 _newLastBatch) external nonReentrant onlyValidatorOrStateTransitionManager {\n        _revertBatches(_newLastBatch);\n    }\n\n    /// @inheritdoc IExecutor\n    function revertBatchesSharedBridge(uint256, uint256 _newLastBatch) external nonReentrant onlyValidator {\n        _revertBatches(_newLastBatch);\n    }\n\n    function _revertBatches(uint256 _newLastBatch) internal {\n        require(s.totalBatchesCommitted > _newLastBatch, \"v1\"); // The last committed batch is less than new last batch\n        require(_newLastBatch >= s.totalBatchesExecuted, \"v2\"); // Already executed batches cannot be reverted\n\n        if (_newLastBatch < s.totalBatchesVerified) {\n            s.totalBatchesVerified = _newLastBatch;\n        }\n        s.totalBatchesCommitted = _newLastBatch;\n\n        // Reset the batch number of the executed system contracts upgrade transaction if the batch\n        // where the system contracts upgrade was committed is among the reverted batches.\n        if (s.l2SystemContractsUpgradeBatchNumber > _newLastBatch) {\n            delete s.l2SystemContractsUpgradeBatchNumber;\n        }\n\n        emit BlocksRevert(s.totalBatchesCommitted, s.totalBatchesVerified,"
    }
  ]
}