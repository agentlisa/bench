{
  "Title": "The Mantle DA Status Is Not Cleared on OP-Batcher Start",
  "Content": "Upon starting the OP-Batcher, [the state is cleaned](https://github.com/mantlenetworkio/mantle-v2/blob/e29d360904db5e5ec81888885f7b7250f8255895/op-batcher/batcher/driver.go#L179) but the Mantle DA status is not.\n\n\nConsider calling the [`clearMantleDAStatus`](https://github.com/mantlenetworkio/mantle-v2/blob/e29d360904db5e5ec81888885f7b7250f8255895/op-batcher/batcher/channel_manager.go#L142-L148) function to clear the Mantle DA status.\n\n\n***Update:** Acknowledged, not resolved.*\n\n\n",
  "Impact": "LOW",
  "Source": "",
  "Code": [
    {
      "filename": "op-batcher/batcher/driver.go",
      "content": "package batcher\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"math/big\"\n\t_ \"net/http/pprof\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/Layr-Labs/datalayr/common/graphView\"\n\t\"github.com/Layr-Labs/datalayr/common/logging\"\n\n\t\"github.com/ethereum/go-ethereum/common\"\n\t\"github.com/ethereum/go-ethereum/core\"\n\t\"github.com/ethereum/go-ethereum/core/types\"\n\t\"github.com/ethereum/go-ethereum/log\"\n\n\t\"github.com/ethereum-optimism/optimism/op-batcher/metrics\"\n\t\"github.com/ethereum-optimism/optimism/op-bindings/bindings\"\n\t\"github.com/ethereum-optimism/optimism/op-node/eth\"\n\t\"github.com/ethereum-optimism/optimism/op-node/rollup/derive\"\n\topclient \"github.com/ethereum-optimism/optimism/op-service/client\"\n\t\"github.com/ethereum-optimism/optimism/op-service/txmgr\"\n)\n\n// BatchSubmitter encapsulates a service responsible for submitting L2 tx\n// batches to L1 for availability.\ntype BatchSubmitter struct {\n\tConfig // directly embed the config + sources\n\n\ttxMgr txmgr.TxManager\n\twg    sync.WaitGroup\n\n\tshutdownCtx       context.Context\n\tcancelShutdownCtx context.CancelFunc\n\tkillCtx           context.Context\n\tcancelKillCtx     context.CancelFunc\n\n\tmutex   sync.Mutex\n\trunning bool\n\n\t// lastStoredBlock is the last block loaded into `state`. If it is empty it should be set to the l2 safe head.\n\tlastStoredBlock eth.BlockID\n\tlastL1Tip       eth.L1BlockRef\n\n\tstate *channelManager\n}\n\n// NewBatchSubmitterFromCLIConfig initializes the BatchSubmitter, gathering any resources\n// that will be needed during operation.\nfunc NewBatchSubmitterFromCLIConfig(cfg CLIConfig, l log.Logger, m metrics.Metricer) (*BatchSubmitter, error) {\n\tctx := context.Background()\n\n\t// Connect to L1 and L2 providers. Perform these last since they are the\n\t// most expensive.\n\tl1Client, err := opclient.DialEthClientWithTimeout(ctx, cfg.L1EthRpc, opclient.DefaultDialTimeout)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tl2Client, err := opclient.DialEthClientWithTimeout(ctx, cfg.L2EthRpc, opclient.DefaultDialTimeout)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\trollupClient, err := opclient.DialRollupClientWithTimeout(ctx, cfg.RollupRpc, opclient.DefaultDialTimeout)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\trcfg, err := rollupClient.RollupConfig(ctx)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"querying rollup config: %w\", err)\n\t}\n\n\ttxManager, err := txmgr.NewSimpleTxManager(\"batcher\", l, m, cfg.TxMgrConfig)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tbatcherCfg := Config{\n\t\tL1Client:               l1Client,\n\t\tL2Client:               l2Client,\n\t\tRollupNode:             rollupClient,\n\t\tPollInterval:           cfg.PollInterval,\n\t\tMaxPendingTransactions: cfg.MaxPendingTransactions,\n\t\tNetworkTimeout:         cfg.TxMgrConfig.NetworkTimeout,\n\t\tTxManager:              txManager,\n\t\tDisperserSocket:        cfg.DisperserSocket,\n\t\tDisperserTimeout:       cfg.DisperserTimeout,\n\t\tDataStoreDuration:      cfg.DataStoreDuration,\n\t\tGraphPollingDuration:   cfg.GraphPollingDuration,\n\t\tRollupMaxSize:          cfg.RollupMaxSize,\n\t\tMantleDaNodes:          cfg.MantleDaNodes,\n\t\tRollup:                 rcfg,\n\t\tChannel: ChannelConfig{\n\t\t\tSeqWindowSize:      rcfg.SeqWindowSize,\n\t\t\tChannelTimeout:     rcfg.ChannelTimeout,\n\t\t\tMaxChannelDuration: cfg.MaxChannelDuration,\n\t\t\tSubSafetyMargin:    cfg.SubSafetyMargin,\n\t\t\tMaxFrameSize:       cfg.MaxL1TxSize - 1, // subtract 1 byte for version\n\t\t\tCompressorConfig:   cfg.CompressorConfig.Config(),\n\t\t},\n\t}\n\n\t// Validate the batcher config\n\tif err := batcherCfg.Check(); err != nil {\n\t\treturn nil, err\n\t}\n\tif rcfg.MantleDaSwitch {\n\t\tif common.HexToAddress(rcfg.DataLayrServiceManagerAddr) == (common.Address{}) {\n\t\t\treturn nil, fmt.Errorf(\"rollup type %t , datalayrcontract address is 0\", rcfg.MantleDaSwitch)\n\t\t}\n\t\tdataLayrServiceManagerAddress := common.HexToAddress(rcfg.DataLayrServiceManagerAddr)\n\n\t\tif len(cfg.GraphProvider) == 0 {\n\t\t\treturn nil, fmt.Errorf(\"graph node provider url is empty\")\n\t\t}\n\t\tdataLayrContract, err := bindings.NewContractDataLayrServiceManager(dataLayrServiceManagerAddress, l1Client)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tparsed, err := bindings.ContractDataLayrServiceManagerMetaData.GetAbi()\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\teigenLogger, err := logging.GetLogger(cfg.EigenLogConfig)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\tgraphClient := graphView.NewGraphClient(cfg.GraphProvider, eigenLogger)\n\t\tbatcherCfg.DataLayrServiceManagerAddr = dataLayrServiceManagerAddress\n\t\tbatcherCfg.DataLayrServiceManagerContract = dataLayrContract\n\t\tbatcherCfg.DataLayrServiceManagerABI = parsed\n\t\tbatcherCfg.GraphClient = graphClient\n\n\t}\n\treturn NewBatchSubmitter(ctx, batcherCfg, l, m)\n}\n\n// NewBatchSubmitter initializes the BatchSubmitter, gathering any resources\n// that will be needed during operation.\nfunc NewBatchSubmitter(ctx context.Context, cfg Config, l log.Logger, m metrics.Metricer) (*BatchSubmitter, error) {\n\tbalance, err := cfg.L1Client.BalanceAt(ctx, cfg.TxManager.From(), nil)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tcfg.log = l\n\tcfg.log.Info(\"creating batch submitter\", \"submitter_addr\", cfg.TxManager.From(), \"submitter_bal\", balance)\n\n\tcfg.metr = m\n\n\treturn &BatchSubmitter{\n\t\tConfig: cfg,\n\t\ttxMgr:  cfg.TxManager,\n\t\tstate:  NewChannelManager(l, m, cfg.Channel),\n\t}, nil\n\n}\n\nfunc (l *BatchSubmitter) Start() error {\n\tl.log.Info(\"Starting Batch Submitter\")\n\n\tl.mutex.Lock()\n\tdefer l.mutex.Unlock()\n\n\tif l.running {\n\t\treturn errors.New(\"batcher is already running\")\n\t}\n\tl.running = true\n\n\tl.shutdownCtx, l.cancelShutdownCtx = context.WithCancel(context.Background())\n\tl.killCtx, l.cancelKillCtx = context.WithCancel(context.Background())\n\tl.state.Clear()\n\tl.lastStoredBlock = eth.BlockID{}\n\n\tl.wg.Add(1)\n\n\tif !l.Rollup.MantleDaSwitch {\n\t\tgo l.loop()\n\t} else {\n\t\tgo l.mantleDALoop()\n\t}\n\n\tl.log.Info(\"Batch Submitter started\")\n\n\treturn nil\n}\n\nfunc (l *BatchSubmitter) StopIfRunning(ctx context.Context) {\n\t_ = l.Stop(ctx)\n}\n\nfunc (l *BatchSubmitter) Stop(ctx context.Context) error {\n\tl.log.Info(\"Stopping Batch Submitter\")\n\n\tl.mutex.Lock()\n\tdefer l.mutex.Unlock()\n\n\tif !l.running {\n\t\treturn errors.New(\"batcher is not running\")\n\t}\n\tl.running = false\n\n\t// go routine will call cancelKill() if the passed in ctx is ever Done\n\tcancelKill := l.cancelKillCtx\n\twrapped, cancel := context.WithCancel(ctx)\n\tdefer cancel()\n\tgo func() {\n\t\t<-wrapped.Done()\n\t\tcancelKill()\n\t}()\n\n\tl.cancelShutdownCtx()\n\tl.wg.Wait()\n\tl.cancelKillCtx()\n\n\tl.log.Info(\"Batch Submitter stopped\")\n\n\treturn nil\n}\n\n// loadBlocksIntoState loads all blocks since the previous stored block\n// It does the following:\n// 1. Fetch the sync status of the sequencer\n// 2. Check if the sync status is valid or if we are all the way up to date\n// 3. Check if it needs to initialize state OR it is lagging (todo: lagging just means race condition?)\n// 4. Load all new blocks into the local state.\n// If there is a reorg, it will reset the last stored block but not clear the internal state so\n// the state can be flushed to L1.\nfunc (l *BatchSubmitter) loadBlocksIntoState(ctx context.Context) error {\n\tstart, end, err := l.calculateL2BlockRangeToStore(ctx)\n\tif err != nil {\n\t\tl.log.Warn(\"Error calculating L2 block range\", \"err\", err)\n\t\treturn err\n\t} else if start.Number >= end.Number {\n\t\treturn errors.New(\"start number is >= end number\")\n\t}\n\n\tvar latestBlock *types.Block\n\t// Add all blocks to \"state\"\n\tfor i := start.Number + 1; i < end.Number+1; i++ {\n\t\tblock, err := l.loadBlockIntoState(ctx, i)\n\t\tif errors.Is(err, ErrReorg) {\n\t\t\tl.log.Warn(\"Found L2 reorg\", \"block_number\", i)\n\t\t\tl.lastStoredBlock = eth.BlockID{}\n\t\t\treturn err\n\t\t} else if err != nil {\n\t\t\tl.log.Warn(\"failed to load block into state\", \"err\", err)\n\t\t\treturn err\n\t\t}\n\t\tl.lastStoredBlock = eth.ToBlockID(block)\n\t\tlatestBlock = block\n\t}\n\n\tl2ref, err := derive.L2BlockToBlockRef(latestBlock, &l.Rollup.Genesis)\n\tif err != nil {\n\t\tl.log.Warn(\"Invalid L2 block loaded into state\", \"err\", err)\n\t\treturn err\n\t}\n\n\tl.metr.RecordL2BlocksLoaded(l2ref)\n\treturn nil\n}\n\n// loadBlockIntoState fetches & stores a single block into `state`. It returns the block it loaded.\nfunc (l *BatchSubmitter) loadBlockIntoState(ctx context.Context, blockNumber uint64) (*types.Block, error) {\n\tctx, cancel := context.WithTimeout(ctx, l.NetworkTimeout)\n\tdefer cancel()\n\tblock, err := l.L2Client.BlockByNumber(ctx, new(big.Int).SetUint64(blockNumber))\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"getting L2 block: %w\", err)\n\t}\n\n\tif err := l.state.AddL2Block(block); err != nil {\n\t\treturn nil, fmt.Errorf(\"adding L2 block to state: %w\", err)\n\t}\n\n\tl.log.Info(\"added L2 block to local state\", \"block\", eth.ToBlockID(block), \"tx_count\", len(block.Transactions()), \"time\", block.Time())\n\treturn block, nil\n}\n\n// calculateL2BlockRangeToStore determines the range (start,end] that should be loaded into the local state.\n// It also takes care of initializing some local state (i.e. will modify l.lastStoredBlock in certain conditions)\nfunc (l *BatchSubmitter) calculateL2BlockRangeToStore(ctx context.Context) (eth.BlockID, eth.BlockID, error) {\n\tctx, cancel := context.WithTimeout(ctx, l.NetworkTimeout)\n\tdefer cancel()\n\tsyncStatus, err := l.RollupNode.SyncStatus(ctx)\n\t// Ensure that we have the sync status\n\tif err != nil {\n\t\treturn eth.BlockID{}, eth.BlockID{}, fmt.Errorf(\"failed to get sync status: %w\", err)\n\t}\n\tif syncStatus.HeadL1 == (eth.L1BlockRef{}) {\n\t\treturn eth.BlockID{}, eth.BlockID{}, errors.New(\"empty sync status\")\n\t}\n\n\t// Check last stored to see if it needs to be set on startup OR set if is lagged behind.\n\t// It lagging implies that the op-node processed some batches that were submitted prior to the current instance of the batcher being alive.\n\tif l.lastStoredBlock == (eth.BlockID{}) {\n\t\tl.log.Info(\"Starting batch-submitter work at safe-head\", \"safe\", syncStatus.SafeL2)\n\t\tl.lastStoredBlock = syncStatus.SafeL2.ID()\n\t} else if l.lastStoredBlock.Number < syncStatus.SafeL2.Number {\n\t\tl.log.Warn(\"last submitted block lagged behind L2 safe head: batch submission will continue from the safe head now\", \"last\", l.lastStoredBlock, \"safe\", syncStatus.SafeL2)\n\t\tl.lastStoredBlock = syncStatus.SafeL2.ID()\n\t}\n\n\t// Check if we should even attempt to load any blocks. TODO: May not need this check\n\tif syncStatus.SafeL2.Number >= syncStatus.UnsafeL2.Number {\n\t\treturn eth.BlockID{}, eth.BlockID{}, errors.New(\"L2 safe head ahead of L2 unsafe head\")\n\t}\n\n\treturn l.lastStoredBlock, syncStatus.UnsafeL2.ID(), nil\n}\n\n// The following things occur:\n// New L2 block (reorg or not)\n// L1 transaction is confirmed\n//\n// What the batcher does:\n// Ensure that channels are created & submitted as frames for an L2 range\n//\n// Error conditions:\n// Submitted batch, but it is not valid\n// Missed L2 block somehow.\n\nfunc (l *BatchSubmitter) loop() {\n\tdefer l.wg.Done()\n\n\tticker := time.NewTicker(l.PollInterval)\n\tdefer ticker.Stop()\n\n\treceiptsCh := make(chan txmgr.TxReceipt[txData])\n\tqueue := txmgr.NewQueue[txData](l.killCtx, l.txMgr, l.MaxPendingTransactions)\n\n\tfor {\n\t\tselect {\n\t\tcase <-ticker.C:\n\t\t\tif err := l.loadBlocksIntoState(l.shutdownCtx); errors.Is(err, ErrReorg) {\n\t\t\t\terr := l.state.Close()\n\t\t\t\tif err != nil {\n\t\t\t\t\tl.log.Error(\"error closing the channel manager to handle a L2 reorg\", \"err\", err)\n\t\t\t\t}\n\t\t\t\tl.publishStateToL1(queue, receiptsCh, true)\n\t\t\t\tl.state.Clear()\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tl.publishStateToL1(queue, receiptsCh, false)\n\t\tcase r := <-receiptsCh:\n\t\t\tl.handleReceipt(r)\n\t\tcase <-l.shutdownCtx.Done():\n\t\t\terr := l.state.Close()\n\t\t\tif err != nil {\n\t\t\t\tl.log.Error(\"error closing the channel manager\", \"err\", err)\n\t\t\t}\n\t\t\tl.publishStateToL1(queue, receiptsCh, true)\n\t\t\treturn\n\t\t}\n\t}\n}\n\n// publishStateToL1 loops through the block data loaded into `state` and\n// submits the associated data to the L1 in the form of channel frames.\nfunc (l *BatchSubmitter) publishStateToL1(queue *txmgr.Queue[txData], receiptsCh chan txmgr.TxReceipt[txData], drain bool) {\n\ttxDone := make(chan struct{})\n\t// send/wait and receipt reading must be on a separate goroutines to avoid deadlocks\n\tgo func() {\n\t\tdefer func() {\n\t\t\tif drain {\n\t\t\t\t// if draining, we wait for all transactions to complete\n\t\t\t\tqueue.Wait()\n\t\t\t}\n\t\t\tclose(txDone)\n\t\t}()\n\t\tfor {\n\t\t\terr := l.publishTxToL1(l.killCtx, queue, receiptsCh)\n\t\t\tif err != nil {\n\t\t\t\tif drain && err != io.EOF {\n\t\t\t\t\tl.log.Error(\"error sending tx while draining state\", \"err\", err)\n\t\t\t\t}\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}()\n\n\tfor {\n\t\tselect {\n\t\tcase r := <-receiptsCh:\n\t\t\tl.handleReceipt(r)\n\t\tcase <-txDone:\n\t\t\treturn\n\t\t}\n\t}\n}\n\n// publishTxToL1 submits a single state tx to the L1\nfunc (l *BatchSubmitter) publishTxToL1(ctx context.Context, queue *txmgr.Queue[txData], receiptsCh chan txmgr.TxReceipt[txData]) error {\n\t// send all available transactions\n\tl1tip, err := l.l1Tip(ctx)\n\tif err != nil {\n\t\tl.log.Error(\"Failed to query L1 tip\", \"error\", err)\n\t\treturn err\n\t}\n\tl.recordL1Tip(l1tip)\n\n\t// Collect next transaction data\n\ttxdata, err := l.state.TxData(l1tip.ID())\n\tif err == io.EOF {\n\t\tl.log.Trace(\"no transaction data available\")\n\t\treturn err\n\t} else if err != nil {\n\t\tl.log.Error(\"unable to get tx data\", \"err\", err)\n\t\treturn err\n\t}\n\n\tl.sendTransaction(txdata, queue, receiptsCh)\n\treturn nil\n}\n\n// sendTransaction creates & submits a transaction to the batch inbox address with the given `data`.\n// It currently uses the underlying `txmgr` to handle transaction sending & price management.\n// This is a blocking method. It should not be called concurrently.\nfunc (l *BatchSubmitter) sendTransaction(txdata txData, queue *txmgr.Queue[txData], receiptsCh chan txmgr.TxReceipt[txData]) {\n\t// Do the gas estimation offline. A value of 0 will cause the [txmgr] to estimate the gas limit.\n\tdata := txdata.Bytes()\n\tintrinsicGas, err := core.IntrinsicGas(data, nil, false, true, true, false)\n\tif err != nil {\n\t\tl.log.Error(\"Failed to calculate intrinsic gas\", \"error\", err)\n\t\treturn\n\t}\n\n\tcandidate := txmgr.TxCandidate{\n\t\tTo:       &l.Rollup.BatchInboxAddress,\n\t\tTxData:   data,\n\t\tGasLimit: intrinsicGas,\n\t}\n\tqueue.Send(txdata, candidate, receiptsCh)\n}\n\nfunc (l *BatchSubmitter) handleReceipt(r txmgr.TxReceipt[txData]) {\n\t// Record TX Status\n\tif r.Err != nil {\n\t\tl.log.Warn(\"unable to publish tx\", \"err\", r.Err, \"data_size\", r.ID.Len())\n\t\tl.recordFailedTx(r.ID.ID(), r.Err)\n\t} else {\n\t\tl.log.Info(\"tx successfully published\", \"tx_hash\", r.Receipt.TxHash, \"data_size\", r.ID.Len())\n\t\tl.recordConfirmedTx(r.ID.ID(), r.Receipt)\n\t}\n}\n\nfunc (l *BatchSubmitter) recordL1Tip(l1tip eth.L1BlockRef) {\n\tif l.lastL1Tip == l1tip {\n\t\treturn\n\t}\n\tl.lastL1Tip = l1tip\n\tl.metr.RecordLatestL1Block(l1tip)\n}\n\nfunc (l *BatchSubmitter) recordFailedTx(id txID, err error) {\n\tl.log.Warn(\"Failed to send transaction\", \"err\", err)\n\tl.state.TxFailed(id)\n}\n\nfunc (l *BatchSubmitter) recordConfirmedTx(id txID, receipt *types.Receipt) {\n\tl.log.Info(\"Transaction confirmed\", \"tx_hash\", receipt.TxHash, \"status\", receipt.Status, \"block_hash\", receipt.BlockHash, \"block_number\", receipt.BlockNumber)\n\tl1block := eth.BlockID{Number: receipt.BlockNumber.Uint64(), Hash: receipt.BlockHash}\n\tl.state.TxConfirmed(id, l1block)\n}\n\n// l1Tip gets the current L1 tip as a L1BlockRef. The passed context is assumed\n// to be a lifetime context, so it is internally wrapped with a network timeout.\nfunc (l *BatchSubmitter) l1Tip(ctx context.Context) (eth.L1BlockRef, error) {\n\ttctx, cancel := context.WithTimeout(ctx, l.NetworkTimeout)\n\tdefer cancel()\n\thead, err := l.L1Client.HeaderByNumber(tctx, nil)\n\tif err != nil {\n\t\treturn eth.L1BlockRef{}, fmt.Errorf(\"getting latest L1 block: %w\", err)\n\t}\n\treturn eth.InfoToL1BlockRef(eth.HeaderBlockInfo(head)), nil\n}"
    },
    {
      "filename": "op-batcher/batcher/channel_manager.go",
      "content": "package batcher\n\nimport (\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"math\"\n\n\tbcommon \"github.com/ethereum-optimism/optimism/op-batcher/common\"\n\t\"github.com/ethereum-optimism/optimism/op-batcher/metrics\"\n\t\"github.com/ethereum-optimism/optimism/op-node/eth\"\n\t\"github.com/ethereum-optimism/optimism/op-node/rollup/derive\"\n\n\t\"github.com/ethereum/go-ethereum/common\"\n\t\"github.com/ethereum/go-ethereum/core/types\"\n\t\"github.com/ethereum/go-ethereum/log\"\n)\n\nvar ErrReorg = errors.New(\"block does not extend existing chain\")\n\n// channelManager stores a contiguous set of blocks & turns them into channels.\n// Upon receiving tx confirmation (or a tx failure), it does channel error handling.\n//\n// For simplicity, it only creates a single pending channel at a time & waits for\n// the channel to either successfully be submitted or timeout before creating a new\n// channel.\n// Functions on channelManager are not safe for concurrent access.ErrReorg\ntype channelManager struct {\n\tlog  log.Logger\n\tmetr metrics.Metricer\n\tcfg  ChannelConfig\n\n\t// All blocks since the last request for new tx data.\n\tblocks []*types.Block\n\t// last block hash - for reorg detection\n\ttip common.Hash\n\n\t// Pending data returned by TxData waiting on Tx Confirmed/Failed\n\n\t// pending channel builder\n\tpendingChannel *channelBuilder\n\t// Set of unconfirmed txID -> frame data. For tx resubmission\n\tpendingTransactions map[txID]txData\n\t// Set of confirmed txID -> inclusion block. For determining if the channel is timed out\n\tconfirmedTransactions map[txID]eth.BlockID\n\t// Set of txID -> frame data. For rollup to MantleDa\n\tdaPendingTxData map[txID]txData\n\t//Set of unconfirmed txID\n\tdaUnConfirmedTxID []txID\n\t// params of initStoreData on MantleDA\n\tparams *bcommon.StoreParams\n\t//receipt of initStoreData transaction on L1\n\tinitStoreDataReceipt *types.Receipt\n\t// if set to true, prevents production of any new channel frames\n\tclosed bool\n}\n\nfunc NewChannelManager(log log.Logger, metr metrics.Metricer, cfg ChannelConfig) *channelManager {\n\treturn &channelManager{\n\t\tlog:  log,\n\t\tmetr: metr,\n\t\tcfg:  cfg,\n\n\t\tpendingTransactions:   make(map[txID]txData),\n\t\tconfirmedTransactions: make(map[txID]eth.BlockID),\n\t\tdaPendingTxData:       make(map[txID]txData),\n\t}\n}\n\n// Clear clears the entire state of the channel manager.\n// It is intended to be used after an L2 reorg.\nfunc (s *channelManager) Clear() {\n\ts.log.Trace(\"clearing channel manager state\")\n\ts.blocks = s.blocks[:0]\n\ts.tip = common.Hash{}\n\ts.closed = false\n\ts.clearPendingChannel()\n}\n\n// TxFailed records a transaction as failed. It will attempt to resubmit the data\n// in the failed transaction.\nfunc (s *channelManager) TxFailed(id txID) {\n\tif data, ok := s.pendingTransactions[id]; ok {\n\t\ts.log.Trace(\"marked transaction as failed\", \"id\", id)\n\t\t// Note: when the batcher is changed to send multiple frames per tx,\n\t\t// this needs to be changed to iterate over all frames of the tx data\n\t\t// and re-queue them.\n\t\ts.pendingChannel.PushFrame(data.Frame())\n\t\tdelete(s.pendingTransactions, id)\n\t} else {\n\t\ts.log.Warn(\"unknown transaction marked as failed\", \"id\", id)\n\t}\n\n\ts.metr.RecordBatchTxFailed()\n\tif s.closed && len(s.confirmedTransactions) == 0 && len(s.pendingTransactions) == 0 && s.pendingChannel != nil {\n\t\ts.log.Info(\"Channel has no submitted transactions, clearing for shutdown\", \"chID\", s.pendingChannel.ID())\n\t\ts.clearPendingChannel()\n\t}\n}\n\n// TxConfirmed marks a transaction as confirmed on L1. Unfortunately even if all frames in\n// a channel have been marked as confirmed on L1 the channel may be invalid & need to be\n// resubmitted.\n// This function may reset the pending channel if the pending channel has timed out.\nfunc (s *channelManager) TxConfirmed(id txID, inclusionBlock eth.BlockID) {\n\ts.metr.RecordBatchTxSubmitted()\n\ts.log.Debug(\"marked transaction as confirmed\", \"id\", id, \"block\", inclusionBlock)\n\tif _, ok := s.pendingTransactions[id]; !ok {\n\t\ts.log.Warn(\"unknown transaction marked as confirmed\", \"id\", id, \"block\", inclusionBlock)\n\t\t// TODO: This can occur if we clear the channel while there are still pending transactions\n\t\t// We need to keep track of stale transactions instead\n\t\treturn\n\t}\n\tdelete(s.pendingTransactions, id)\n\ts.confirmedTransactions[id] = inclusionBlock\n\ts.pendingChannel.FramePublished(inclusionBlock.Number)\n\n\t// If this channel timed out, put the pending blocks back into the local saved blocks\n\t// and then reset this state so it can try to build a new channel.\n\tif s.pendingChannelIsTimedOut() {\n\t\ts.metr.RecordChannelTimedOut(s.pendingChannel.ID())\n\t\ts.log.Warn(\"Channel timed out\", \"id\", s.pendingChannel.ID())\n\t\ts.blocks = append(s.pendingChannel.Blocks(), s.blocks...)\n\t\ts.clearPendingChannel()\n\t}\n\t// If we are done with this channel, record that.\n\tif s.pendingChannelIsFullySubmitted() {\n\t\ts.metr.RecordChannelFullySubmitted(s.pendingChannel.ID())\n\t\ts.log.Info(\"Channel is fully submitted\", \"id\", s.pendingChannel.ID())\n\t\ts.clearPendingChannel()\n\t}\n}\n\n// clearPendingChannel resets all pending state back to an initialized but empty state.\n// TODO: Create separate \"pending\" state\nfunc (s *channelManager) clearPendingChannel() {\n\ts.pendingChannel = nil\n\ts.pendingTransactions = make(map[txID]txData)\n\ts.confirmedTransactions = make(map[txID]eth.BlockID)\n}\n\nfunc (s *channelManager) clearMantleDAStatus() {\n\ts.params = nil\n\ts.initStoreDataReceipt = nil\n\ts.daPendingTxData = make(map[txID]txData)\n\ts.daUnConfirmedTxID = s.daUnConfirmedTxID[:0]\n\ts.metr.RecordRollupRetry(0)\n}\n\n// pendingChannelIsTimedOut returns true if submitted channel has timed out.\n// A channel has timed out if the difference in L1 Inclusion blocks between\n// the first & last included block is greater than or equal to the channel timeout.\nfunc (s *channelManager) pendingChannelIsTimedOut() bool {\n\tif s.pendingChannel == nil {\n\t\treturn false // no channel to be timed out\n\t}\n\t// No confirmed transactions => not timed out\n\tif len(s.confirmedTransactions) == 0 {\n\t\treturn false\n\t}\n\t// If there are confirmed transactions, find the first + last confirmed block numbers\n\tmin := uint64(math.MaxUint64)\n\tmax := uint64(0)\n\tfor _, inclusionBlock := range s.confirmedTransactions {\n\t\tif inclusionBlock.Number < min {\n\t\t\tmin = inclusionBlock.Number\n\t\t}\n\t\tif inclusionBlock.Number > max {\n\t\t\tmax = inclusionBlock.Number\n\t\t}\n\t}\n\treturn max-min >= s.cfg.ChannelTimeout\n}\n\n// pendingChannelIsFullySubmitted returns true if the channel has been fully submitted.\nfunc (s *channelManager) pendingChannelIsFullySubmitted() bool {\n\tif s.pendingChannel == nil {\n\t\treturn false // todo: can decide either way here. Nonsensical answer though\n\t}\n\treturn s.pendingChannel.IsFull() && len(s.pendingTransactions)+s.pendingChannel.NumFrames() == 0\n}\n\n// nextTxData pops off s.datas & handles updating the internal state\nfunc (s *channelManager) nextTxData() (txData, error) {\n\tif s.pendingChannel == nil || !s.pendingChannel.HasFrame() {\n\t\ts.log.Trace(\"no next tx data\")\n\t\treturn txData{}, io.EOF // TODO: not enough data error instead\n\t}\n\n\tframe := s.pendingChannel.NextFrame()\n\ttxdata := txData{frame}\n\tid := txdata.ID()\n\n\ts.log.Trace(\"returning next tx data\", \"id\", id)\n\ts.pendingTransactions[id] = txdata\n\treturn txdata, nil\n}\n\n// TxData returns the next tx data that should be submitted to L1.\n//\n// It currently only uses one frame per transaction. If the pending channel is\n// full, it only returns the remaining frames of this channel until it got\n// successfully fully sent to L1. It returns io.EOF if there's no pending frame.\nfunc (s *channelManager) TxData(l1Head eth.BlockID) (txData, error) {\n\tdataPending := s.pendingChannel != nil && s.pendingChannel.HasFrame()\n\ts.log.Debug(\"Requested tx data\", \"l1Head\", l1Head, \"data_pending\", dataPending, \"blocks_pending\", len(s.blocks))\n\n\t// Short circuit if there is a pending frame or the channel manager is closed.\n\tif dataPending || s.closed {\n\t\treturn s.nextTxData()\n\t}\n\n\t// No pending frame, so we have to add new blocks to the channel\n\n\t// If we have no saved blocks, we will not be able to create valid frames\n\tif len(s.blocks) == 0 {\n\t\treturn txData{}, io.EOF\n\t}\n\n\t// we have blocks, but we cannot add them to the channel right now\n\tif s.pendingChannel != nil && s.pendingChannel.IsFull() {\n\t\treturn txData{}, io.EOF\n\t}\n\n\tif err := s.ensurePendingChannel(l1Head); err != nil {\n\t\treturn txData{}, err\n\t}\n\n\tif err := s.processBlocks(); err != nil {\n\t\treturn txData{}, err\n\t}\n\n\t// Register current L1 head only after all pending blocks have been\n\t// processed. Even if a timeout will be triggered now, it is better to have\n\t// all pending blocks be included in this channel for submission.\n\ts.registerL1Block(l1Head)\n\n\tif err := s.outputFrames(); err != nil {\n\t\treturn txData{}, err\n\t}\n\n\treturn s.nextTxData()\n}\n\nfunc (s *channelManager) ensurePendingChannel(l1Head eth.BlockID) error {\n\tif s.pendingChannel != nil {\n\t\treturn nil\n\t}\n\n\tcb, err := newChannelBuilder(s.cfg)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"creating new channel: %w\", err)\n\t}\n\ts.pendingChannel = cb\n\ts.log.Info(\"Created channel\",\n\t\t\"id\", cb.ID(),\n\t\t\"l1Head\", l1Head,\n\t\t\"blocks_pending\", len(s.blocks))\n\ts.metr.RecordChannelOpened(cb.ID(), len(s.blocks))\n\n\treturn nil\n}\n\n// registerL1Block registers the given block at the pending channel.\nfunc (s *channelManager) registerL1Block(l1Head eth.BlockID) {\n\ts.pendingChannel.RegisterL1Block(l1Head.Number)\n\ts.log.Debug(\"new L1-block registered at channel builder\",\n\t\t\"l1Head\", l1Head,\n\t\t\"channel_full\", s.pendingChannel.IsFull(),\n\t\t\"full_reason\", s.pendingChannel.FullErr(),\n\t)\n}\n\n// processBlocks adds blocks from the blocks queue to the pending channel until\n// either the queue got exhausted or the channel is full.\nfunc (s *channelManager) processBlocks() error {\n\tvar (\n\t\tblocksAdded int\n\t\t_chFullErr  *ChannelFullError // throw away, just for type checking\n\t\tlatestL2ref eth.L2BlockRef\n\t)\n\tfor i, block := range s.blocks {\n\t\tl1info, err := s.pendingChannel.AddBlock(block)\n\t\tif errors.As(err, &_chFullErr) {\n\t\t\t// current block didn't get added because channel is already full\n\t\t\tbreak\n\t\t} else if err != nil {\n\t\t\treturn fmt.Errorf(\"adding block[%d] to channel builder: %w\", i, err)\n\t\t}\n\t\tblocksAdded += 1\n\t\tlatestL2ref = l2BlockRefFromBlockAndL1Info(block, l1info)\n\t\ts.metr.RecordL2BlockInChannel(block)\n\t\ts.log.Info(\"add block to channel\", \"channel id\", s.pendingChannel.ID(), \"block number\", block.Number())\n\t\t// current block got added but channel is now full\n\t\tif s.pendingChannel.IsFull() {\n\t\t\tbreak\n\t\t}\n\t}\n\n\tif blocksAdded == len(s.blocks) {\n\t\t// all blocks processed, reuse slice\n\t\ts.blocks = s.blocks[:0]\n\t} else {\n\t\t// remove processed blocks\n\t\ts.blocks = s.blocks[blocksAdded:]\n\t}\n\n\ts.metr.RecordL2BlocksAdded(latestL2ref,\n\t\tblocksAdded,\n\t\tlen(s.blocks),\n\t\ts.pendingChannel.InputBytes(),\n\t\ts.pendingChannel.ReadyBytes())\n\ts.log.Debug(\"Added blocks to channel\",\n\t\t\"blocks_added\", blocksAdded,\n\t\t\"blocks_pending\", len(s.blocks),\n\t\t\"channel_full\", s.pendingChannel.IsFull(),\n\t\t\"input_bytes\", s.pendingChannel.InputBytes(),\n\t\t\"ready_bytes\", s.pendingChannel.ReadyBytes(),\n\t)\n\treturn nil\n}\n\nfunc (s *channelManager) outputFrames() error {\n\tif err := s.pendingChannel.OutputFrames(); err != nil {\n\t\treturn fmt.Errorf(\"creating frames with channel builder: %w\", err)\n\t}\n\tif !s.pendingChannel.IsFull() {\n\t\treturn nil\n\t}\n\n\tinBytes, outBytes := s.pendingChannel.InputBytes(), s.pendingChannel.OutputBytes()\n\ts.metr.RecordChannelClosed(\n\t\ts.pendingChannel.ID(),\n\t\tlen(s.blocks),\n\t\ts.pendingChannel.NumFrames(),\n\t\tinBytes,\n\t\toutBytes,\n\t\ts.pendingChannel.FullErr(),\n\t)\n\n\tvar comprRatio float64\n\tif inBytes > 0 {\n\t\tcomprRatio = float64(outBytes) / float64(inBytes)\n\t}\n\ts.log.Info(\"Channel closed\",\n\t\t\"id\", s.pendingChannel.ID(),\n\t\t\"blocks_pending\", len(s.blocks),\n\t\t\"num_frames\", s.pendingChannel.NumFrames(),\n\t\t\"input_bytes\", inBytes,\n\t\t\"output_bytes\", outBytes,\n\t\t\"full_reason\", s.pendingChannel.FullErr(),\n\t\t\"compr_ratio\", comprRatio,\n\t)\n\treturn nil\n}\n\n// AddL2Block adds an L2 block to the internal blocks queue. It returns ErrReorg\n// if the block does not extend the last block loaded into the state. If no\n// blocks were added yet, the parent hash check is skipped.\nfunc (s *channelManager) AddL2Block(block *types.Block) error {\n\tif s.tip != (common.Hash{}) && s.tip != block.ParentHash() {\n\t\treturn ErrReorg\n\t}\n\n\ts.metr.RecordL2BlockInPendingQueue(block)\n\ts.blocks = append(s.blocks, block)\n\ts.tip = block.Hash()\n\n\treturn nil\n}\n\nfunc l2BlockRefFromBlockAndL1Info(block *types.Block, l1info derive.L1BlockInfo) eth.L2BlockRef {\n\treturn eth.L2BlockRef{\n\t\tHash:           block.Hash(),\n\t\tNumber:         block.NumberU64(),\n\t\tParentHash:     block.ParentHash(),\n\t\tTime:           block.Time(),\n\t\tL1Origin:       eth.BlockID{Hash: l1info.BlockHash, Number: l1info.Number},\n\t\tSequenceNumber: l1info.SequenceNumber,\n\t}\n}\n\n// Close closes the current pending channel, if one exists, outputs any remaining frames,\n// and prevents the creation of any new channels.\n// Any outputted frames still need to be published.\nfunc (s *channelManager) Close() error {\n\tif s.closed {\n\t\treturn nil\n\t}\n\n\ts.closed = true\n\n\t// Any pending state can be proactively cleared if there are no submitted transactions\n\tif len(s.confirmedTransactions) == 0 && len(s.pendingTransactions) == 0 {\n\t\ts.clearPendingChannel()\n\t}\n\n\tif s.pendingChannel == nil {\n\t\treturn nil\n\t}\n\n\ts.pendingChannel.Close()\n\n\treturn s.outputFrames()\n}"
    }
  ]
}