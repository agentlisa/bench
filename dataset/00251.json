{
  "Title": "Missing Connection Timeout for Contacting Mantle DA",
  "Content": "The implemented connection to Mantle DA is missing a defined timeout option. This might lead to issues when servers accept connections but fail to respond to calls. There are the following occurrences of connections to Mantle DA:\n\n\n* Connection to Mantle DA in [`getFramesByDataStoreId`](https://github.com/mantlenetworkio/mantle-v2/blob/e29d360904db5e5ec81888885f7b7250f8255895/op-node/rollup/da/datastore.go#L78) of OP-Node\n* Connection to Mantle DA Indexer in [`getFramesFromIndexerByDataStoreId`](https://github.com/mantlenetworkio/mantle-v2/blob/e29d360904db5e5ec81888885f7b7250f8255895/op-node/rollup/da/datastore.go#L99) of OP-Node\n* Connection to Mantle DA Disperser in [`callEncode`](https://github.com/mantlenetworkio/mantle-v2/blob/e29d360904db5e5ec81888885f7b7250f8255895/op-batcher/batcher/driver_da.go#L294) of OP-Batcher\n\n\nConsider adding a timeout mechanism to the above listed connections.\n\n\n***Update:** Acknowledged, will resolve.*\n\n\n",
  "Impact": "LOW",
  "Source": "",
  "Code": [
    {
      "filename": "op-node/rollup/da/datastore.go",
      "content": "package da\n\nimport (\n\t\"context\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"github.com/pkg/errors\"\n\t\"github.com/shurcooL/graphql\"\n\t\"google.golang.org/grpc\"\n\t\"google.golang.org/grpc/credentials/insecure\"\n\n\t\"github.com/Layr-Labs/datalayr/common/graphView\"\n\tpb \"github.com/Layr-Labs/datalayr/common/interfaces/interfaceRetrieverServer\"\n\n\t\"github.com/ethereum/go-ethereum/common/hexutil\"\n\t\"github.com/ethereum/go-ethereum/log\"\n)\n\nconst (\n\tPOLLING_INTERVAL     = 1 * time.Second\n\tMAX_RPC_MESSAGE_SIZE = 1024 * 1024 * 300\n)\n\ntype MantleDataStoreConfig struct {\n\tRetrieverSocket          string\n\tRetrieverTimeout         time.Duration\n\tGraphProvider            string\n\tDataStorePollingDuration time.Duration\n\tMantleDaIndexerSocket    string\n\tMantleDAIndexerEnable    bool\n}\n\ntype MantleDataStore struct {\n\tCtx           context.Context\n\tCfg           *MantleDataStoreConfig\n\tGraphClient   *graphView.GraphClient\n\tGraphqlClient *graphql.Client\n}\n\nfunc NewMantleDataStore(ctx context.Context, cfg *MantleDataStoreConfig) (*MantleDataStore, error) {\n\tgraphClient := graphView.NewGraphClient(cfg.GraphProvider, nil)\n\tgraphqlClient := graphql.NewClient(graphClient.GetEndpoint(), nil)\n\tmDatastore := &MantleDataStore{\n\t\tCtx:           ctx,\n\t\tCfg:           cfg,\n\t\tGraphClient:   graphClient,\n\t\tGraphqlClient: graphqlClient,\n\t}\n\treturn mDatastore, nil\n}\n\nfunc (mda *MantleDataStore) getDataStoreById(dataStoreId uint32) (*graphView.DataStore, error) {\n\tvar query struct {\n\t\tDataStore graphView.DataStoreGql `graphql:\"dataStore(id: $storeId)\"`\n\t}\n\tvariables := map[string]interface{}{\n\t\t\"storeId\": graphql.String(strconv.FormatUint(uint64(dataStoreId), 10)),\n\t}\n\terr := mda.GraphqlClient.Query(mda.Ctx, &query, variables)\n\tif err != nil {\n\t\tlog.Error(\"Query subgraph fail\", \"err\", err)\n\t\treturn nil, err\n\t}\n\tlog.Debug(\"Query dataStore success\",\n\t\t\"DurationDataStoreId\", query.DataStore.DurationDataStoreId,\n\t\t\"Confirmed\", query.DataStore.Confirmed,\n\t\t\"ConfirmTxHash\", query.DataStore.ConfirmTxHash)\n\tdataStore, err := query.DataStore.Convert()\n\tif err != nil {\n\t\tlog.Warn(\"DataStoreGql convert to DataStore fail\", \"err\", err)\n\t\treturn nil, err\n\t}\n\treturn dataStore, nil\n}\n\nfunc (mda *MantleDataStore) getFramesByDataStoreId(dataStoreId uint32) ([]byte, error) {\n\tconn, err := grpc.Dial(mda.Cfg.RetrieverSocket, grpc.WithTransportCredentials(insecure.NewCredentials()))\n\tif err != nil {\n\t\tlog.Error(\"Connect to da retriever fail\", \"err\", err)\n\t\treturn nil, err\n\t}\n\tdefer conn.Close()\n\tclient := pb.NewDataRetrievalClient(conn)\n\topt := grpc.MaxCallRecvMsgSize(MAX_RPC_MESSAGE_SIZE)\n\trequest := &pb.FramesAndDataRequest{\n\t\tDataStoreId: dataStoreId,\n\t}\n\treply, err := client.RetrieveFramesAndData(mda.Ctx, request, opt)\n\tif err != nil {\n\t\tlog.Error(\"Retrieve frames and data fail\", \"err\", err)\n\t\treturn nil, err\n\t}\n\tlog.Debug(\"Get reply data success\", \"replyLength\", len(reply.GetData()))\n\treturn reply.GetData(), nil\n}\n\nfunc (mda *MantleDataStore) getFramesFromIndexerByDataStoreId(dataStoreId uint32) ([]byte, error) {\n\tconn, err := grpc.Dial(mda.Cfg.MantleDaIndexerSocket, grpc.WithTransportCredentials(insecure.NewCredentials()))\n\tif err != nil {\n\t\tlog.Error(\"Connect to mantle da index retriever fail\", \"err\", err)\n\t\treturn nil, err\n\t}\n\tdefer conn.Close()\n\tclient := pb.NewDataRetrievalClient(conn)\n\topt := grpc.MaxCallRecvMsgSize(MAX_RPC_MESSAGE_SIZE)\n\trequest := &pb.FramesAndDataRequest{\n\t\tDataStoreId: dataStoreId,\n\t}\n\treply, err := client.RetrieveFramesAndData(mda.Ctx, request, opt)\n\tif err != nil {\n\t\tlog.Error(\"Retrieve frames and data fail\", \"err\", err)\n\t\treturn nil, err\n\t}\n\tlog.Debug(\"Get reply data from mantle da success\", \"replyLength\", len(reply.GetData()))\n\treturn reply.GetData(), nil\n}\n\nfunc (mda *MantleDataStore) RetrievalFramesFromDa(dataStoreId uint32) ([]byte, error) {\n\tpollingTimeout := time.NewTimer(mda.Cfg.DataStorePollingDuration)\n\tdefer pollingTimeout.Stop()\n\tintervalTicker := time.NewTicker(POLLING_INTERVAL)\n\tdefer intervalTicker.Stop()\n\tfor {\n\t\tselect {\n\t\tcase <-intervalTicker.C:\n\t\t\tif dataStoreId <= 0 {\n\t\t\t\tlog.Error(\"DataStoreId less than zero\", \"dataStoreId\", dataStoreId)\n\t\t\t\treturn nil, errors.New(\"dataStoreId less than 0\")\n\t\t\t}\n\t\t\tdataStore, err := mda.getDataStoreById(dataStoreId)\n\t\t\tif err != nil {\n\t\t\t\tlog.Warn(\"Get datastore by id fail\", \"err\", err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tlog.Info(\"Get dataStore success\",\n\t\t\t\t\"DurationDataStoreId\", dataStore.DurationDataStoreId,\n\t\t\t\t\"Confirmed\", dataStore.Confirmed,\n\t\t\t\t\"ConfirmTxHash\", hexutil.Encode(dataStore.ConfirmTxHash[:]))\n\t\t\tif !dataStore.Confirmed {\n\t\t\t\tlog.Warn(\"This batch is not confirmed\")\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tvar frames []byte\n\t\t\tif mda.Cfg.MantleDAIndexerEnable { // from mantle da indexer\n\t\t\t\tlog.Info(\"sync block data from mantle da indexer\")\n\t\t\t\tframes, err = mda.getFramesFromIndexerByDataStoreId(dataStoreId)\n\t\t\t\tif err != nil {\n\t\t\t\t\tlog.Warn(\"Get frames from indexer fail\", \"err\", err)\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t} else { // from mantle da retriever\n\t\t\t\tlog.Info(\"sync block data from mantle da retriever\")\n\t\t\t\tframes, err = mda.getFramesByDataStoreId(dataStoreId)\n\t\t\t\tif err != nil {\n\t\t\t\t\tlog.Warn(\"Get frames from mantle da retriever fail\", \"err\", err)\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t}\n\t\t\tif frames == nil {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\treturn frames, nil\n\t\tcase <-pollingTimeout.C:\n\t\t\treturn nil, errors.New(\"Get frame ticker exit\")\n\t\tcase err := <-mda.Ctx.Done():\n\t\t\tlog.Warn(\"Retrieval service shutting down\", \"err\", err)\n\t\t\treturn nil, errors.New(\"Retrieval service shutting down\")\n\t\t}\n\t}\n}"
    },
    {
      "filename": "op-node/rollup/da/datastore.go",
      "content": "package da\n\nimport (\n\t\"context\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"github.com/pkg/errors\"\n\t\"github.com/shurcooL/graphql\"\n\t\"google.golang.org/grpc\"\n\t\"google.golang.org/grpc/credentials/insecure\"\n\n\t\"github.com/Layr-Labs/datalayr/common/graphView\"\n\tpb \"github.com/Layr-Labs/datalayr/common/interfaces/interfaceRetrieverServer\"\n\n\t\"github.com/ethereum/go-ethereum/common/hexutil\"\n\t\"github.com/ethereum/go-ethereum/log\"\n)\n\nconst (\n\tPOLLING_INTERVAL     = 1 * time.Second\n\tMAX_RPC_MESSAGE_SIZE = 1024 * 1024 * 300\n)\n\ntype MantleDataStoreConfig struct {\n\tRetrieverSocket          string\n\tRetrieverTimeout         time.Duration\n\tGraphProvider            string\n\tDataStorePollingDuration time.Duration\n\tMantleDaIndexerSocket    string\n\tMantleDAIndexerEnable    bool\n}\n\ntype MantleDataStore struct {\n\tCtx           context.Context\n\tCfg           *MantleDataStoreConfig\n\tGraphClient   *graphView.GraphClient\n\tGraphqlClient *graphql.Client\n}\n\nfunc NewMantleDataStore(ctx context.Context, cfg *MantleDataStoreConfig) (*MantleDataStore, error) {\n\tgraphClient := graphView.NewGraphClient(cfg.GraphProvider, nil)\n\tgraphqlClient := graphql.NewClient(graphClient.GetEndpoint(), nil)\n\tmDatastore := &MantleDataStore{\n\t\tCtx:           ctx,\n\t\tCfg:           cfg,\n\t\tGraphClient:   graphClient,\n\t\tGraphqlClient: graphqlClient,\n\t}\n\treturn mDatastore, nil\n}\n\nfunc (mda *MantleDataStore) getDataStoreById(dataStoreId uint32) (*graphView.DataStore, error) {\n\tvar query struct {\n\t\tDataStore graphView.DataStoreGql `graphql:\"dataStore(id: $storeId)\"`\n\t}\n\tvariables := map[string]interface{}{\n\t\t\"storeId\": graphql.String(strconv.FormatUint(uint64(dataStoreId), 10)),\n\t}\n\terr := mda.GraphqlClient.Query(mda.Ctx, &query, variables)\n\tif err != nil {\n\t\tlog.Error(\"Query subgraph fail\", \"err\", err)\n\t\treturn nil, err\n\t}\n\tlog.Debug(\"Query dataStore success\",\n\t\t\"DurationDataStoreId\", query.DataStore.DurationDataStoreId,\n\t\t\"Confirmed\", query.DataStore.Confirmed,\n\t\t\"ConfirmTxHash\", query.DataStore.ConfirmTxHash)\n\tdataStore, err := query.DataStore.Convert()\n\tif err != nil {\n\t\tlog.Warn(\"DataStoreGql convert to DataStore fail\", \"err\", err)\n\t\treturn nil, err\n\t}\n\treturn dataStore, nil\n}\n\nfunc (mda *MantleDataStore) getFramesByDataStoreId(dataStoreId uint32) ([]byte, error) {\n\tconn, err := grpc.Dial(mda.Cfg.RetrieverSocket, grpc.WithTransportCredentials(insecure.NewCredentials()))\n\tif err != nil {\n\t\tlog.Error(\"Connect to da retriever fail\", \"err\", err)\n\t\treturn nil, err\n\t}\n\tdefer conn.Close()\n\tclient := pb.NewDataRetrievalClient(conn)\n\topt := grpc.MaxCallRecvMsgSize(MAX_RPC_MESSAGE_SIZE)\n\trequest := &pb.FramesAndDataRequest{\n\t\tDataStoreId: dataStoreId,\n\t}\n\treply, err := client.RetrieveFramesAndData(mda.Ctx, request, opt)\n\tif err != nil {\n\t\tlog.Error(\"Retrieve frames and data fail\", \"err\", err)\n\t\treturn nil, err\n\t}\n\tlog.Debug(\"Get reply data success\", \"replyLength\", len(reply.GetData()))\n\treturn reply.GetData(), nil\n}\n\nfunc (mda *MantleDataStore) getFramesFromIndexerByDataStoreId(dataStoreId uint32) ([]byte, error) {\n\tconn, err := grpc.Dial(mda.Cfg.MantleDaIndexerSocket, grpc.WithTransportCredentials(insecure.NewCredentials()))\n\tif err != nil {\n\t\tlog.Error(\"Connect to mantle da index retriever fail\", \"err\", err)\n\t\treturn nil, err\n\t}\n\tdefer conn.Close()\n\tclient := pb.NewDataRetrievalClient(conn)\n\topt := grpc.MaxCallRecvMsgSize(MAX_RPC_MESSAGE_SIZE)\n\trequest := &pb.FramesAndDataRequest{\n\t\tDataStoreId: dataStoreId,\n\t}\n\treply, err := client.RetrieveFramesAndData(mda.Ctx, request, opt)\n\tif err != nil {\n\t\tlog.Error(\"Retrieve frames and data fail\", \"err\", err)\n\t\treturn nil, err\n\t}\n\tlog.Debug(\"Get reply data from mantle da success\", \"replyLength\", len(reply.GetData()))\n\treturn reply.GetData(), nil\n}\n\nfunc (mda *MantleDataStore) RetrievalFramesFromDa(dataStoreId uint32) ([]byte, error) {\n\tpollingTimeout := time.NewTimer(mda.Cfg.DataStorePollingDuration)\n\tdefer pollingTimeout.Stop()\n\tintervalTicker := time.NewTicker(POLLING_INTERVAL)\n\tdefer intervalTicker.Stop()\n\tfor {\n\t\tselect {\n\t\tcase <-intervalTicker.C:\n\t\t\tif dataStoreId <= 0 {\n\t\t\t\tlog.Error(\"DataStoreId less than zero\", \"dataStoreId\", dataStoreId)\n\t\t\t\treturn nil, errors.New(\"dataStoreId less than 0\")\n\t\t\t}\n\t\t\tdataStore, err := mda.getDataStoreById(dataStoreId)\n\t\t\tif err != nil {\n\t\t\t\tlog.Warn(\"Get datastore by id fail\", \"err\", err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tlog.Info(\"Get dataStore success\",\n\t\t\t\t\"DurationDataStoreId\", dataStore.DurationDataStoreId,\n\t\t\t\t\"Confirmed\", dataStore.Confirmed,\n\t\t\t\t\"ConfirmTxHash\", hexutil.Encode(dataStore.ConfirmTxHash[:]))\n\t\t\tif !dataStore.Confirmed {\n\t\t\t\tlog.Warn(\"This batch is not confirmed\")\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tvar frames []byte\n\t\t\tif mda.Cfg.MantleDAIndexerEnable { // from mantle da indexer\n\t\t\t\tlog.Info(\"sync block data from mantle da indexer\")\n\t\t\t\tframes, err = mda.getFramesFromIndexerByDataStoreId(dataStoreId)\n\t\t\t\tif err != nil {\n\t\t\t\t\tlog.Warn(\"Get frames from indexer fail\", \"err\", err)\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t} else { // from mantle da retriever\n\t\t\t\tlog.Info(\"sync block data from mantle da retriever\")\n\t\t\t\tframes, err = mda.getFramesByDataStoreId(dataStoreId)\n\t\t\t\tif err != nil {\n\t\t\t\t\tlog.Warn(\"Get frames from mantle da retriever fail\", \"err\", err)\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t}\n\t\t\tif frames == nil {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\treturn frames, nil\n\t\tcase <-pollingTimeout.C:\n\t\t\treturn nil, errors.New(\"Get frame ticker exit\")\n\t\tcase err := <-mda.Ctx.Done():\n\t\t\tlog.Warn(\"Retrieval service shutting down\", \"err\", err)\n\t\t\treturn nil, errors.New(\"Retrieval service shutting down\")\n\t\t}\n\t}\n}"
    },
    {
      "filename": "op-batcher/batcher/driver_da.go",
      "content": "package batcher\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"io\"\n\t\"math/big\"\n\t\"sort\"\n\t\"time\"\n\n\t\"google.golang.org/grpc\"\n\t\"google.golang.org/grpc/credentials/insecure\"\n\n\tpb \"github.com/Layr-Labs/datalayr/common/interfaces/interfaceDL\"\n\n\t\"github.com/ethereum/go-ethereum/accounts/abi\"\n\tecommon \"github.com/ethereum/go-ethereum/common\"\n\t\"github.com/ethereum/go-ethereum/core/types\"\n\t\"github.com/ethereum/go-ethereum/log\"\n\t\"github.com/ethereum/go-ethereum/rlp\"\n\n\t\"github.com/ethereum-optimism/optimism/op-batcher/common\"\n\t\"github.com/ethereum-optimism/optimism/op-bindings/bindings\"\n\t\"github.com/ethereum-optimism/optimism/op-node/eth\"\n\t\"github.com/ethereum-optimism/optimism/op-service/txmgr\"\n)\n\nconst (\n\tEigenRollupMaxSize  = 1024 * 1024 * 300\n\tDaLoopRetryNum      = 10\n\tBytesPerCoefficient = 31\n)\n\nvar ErrInitDataStore = errors.New(\"init data store transaction failed\")\n\nfunc (l *BatchSubmitter) mantleDALoop() {\n\tdefer l.wg.Done()\n\tticker := time.NewTicker(l.PollInterval)\n\tdefer ticker.Stop()\n\n\tfor {\n\t\tselect {\n\t\tcase <-ticker.C:\n\t\t\terr := l.loadBlocksIntoState(l.shutdownCtx)\n\t\t\tif errors.Is(err, ErrReorg) {\n\t\t\t\terr := l.state.Close()\n\t\t\t\tif err != nil {\n\t\t\t\t\tl.log.Error(\"error closing the channel manager to handle a L2 reorg\", \"err\", err)\n\t\t\t\t}\n\t\t\t\tl.state.Clear()\n\t\t\t\tl.state.clearMantleDAStatus()\n\t\t\t\tcontinue\n\t\t\t} else if err != nil {\n\t\t\t\tl.log.Error(\"load block into state err\", \"err\", err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tl.publishStateToMantleDA()\n\n\t\tcase <-l.shutdownCtx.Done():\n\t\t\terr := l.state.Close()\n\t\t\tif err != nil {\n\t\t\t\tl.log.Error(\"error closing the channel manager\", \"err\", err)\n\t\t\t}\n\t\t\treturn\n\t\t}\n\t}\n}\n\n// publishStateToMantleDA loops through the block data loaded into `state` and\n// submits the associated data to the MantleDA in the form of channel frames.\n// batch frames in one rollup transaction to MantleDA\nfunc (l *BatchSubmitter) publishStateToMantleDA() {\n\n\tfor {\n\t\tisFull, err := l.appendNextRollupData(l.killCtx)\n\n\t\tif err != nil && err != io.EOF {\n\t\t\tl.log.Error(\"failed to  get next tx data\", \"err\", err)\n\t\t\treturn\n\t\t}\n\t\tif isFull {\n\t\t\tdone, err := l.loopRollupDa()\n\t\t\tif err != nil {\n\t\t\t\tl.log.Error(\"failed to rollup da to mantle da\", \"err\", err)\n\t\t\t\tl.log.Warn(\"reset state in channel manager\")\n\t\t\t\tl.reset()\n\t\t\t\treturn\n\t\t\t}\n\t\t\tif done {\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t\tif err == io.EOF {\n\t\t\treturn\n\t\t}\n\n\t}\n\n}\n\n// appendNextRollupData get next txData and cache the data in pendingTransactions, and determine whether the channel is full\nfunc (l *BatchSubmitter) appendNextRollupData(ctx context.Context) (bool, error) {\n\t// send all available transactions\n\tl1tip, err := l.l1Tip(ctx)\n\tif err != nil {\n\t\tl.log.Error(\"failed to query L1 tip\", \"error\", err)\n\t\treturn false, err\n\t}\n\tl.recordL1Tip(l1tip)\n\n\t// Collect next transaction data\n\ttxdata, err := l.state.TxData(l1tip.ID())\n\tif err != nil && err != io.EOF {\n\t\tl.log.Error(\"unable to get tx data\", \"err\", err)\n\t\treturn false, err\n\t}\n\tif err == nil {\n\t\tl.state.daPendingTxData[txdata.ID()] = txdata\n\t}\n\n\tif l.state.pendingChannel != nil && l.state.pendingChannel.IsFull() && !l.state.pendingChannel.HasFrame() {\n\t\treturn true, err\n\t}\n\treturn false, err\n}\n\n// When the channel is full, it starts fetching data from the channel's cache(pendingTransactions) and rollup to MantleDA.\n// A rollup consists of two transactions, InitDataStore and confirmStoredData, both of which are successful, meaning that the rollup is successful.\n// A rollup has a RollupMaxSize limit, if a channel's corresponding txData is larger than this limit, you need to commit it in several times.\nfunc (l *BatchSubmitter) loopRollupDa() (bool, error) {\n\tvar retry int32\n\tl.metr.RecordRollupRetry(0)\n\tl.metr.RecordDaRetry(0)\n\tfor {\n\t\t//it means that all the txData has been rollup\n\t\tif len(l.state.daPendingTxData) == 0 {\n\t\t\tl.log.Info(\"all txData of current channel have been rollup\")\n\t\t\treturn true, nil\n\t\t}\n\t\t//If current txsData has already been successfully uploaded to MantleDA(params is not nil), we don't need to re-upload.\n\t\tif l.state.params == nil {\n\t\t\ttransactionData, err := l.txAggregator()\n\t\t\tif err != nil {\n\t\t\t\tl.log.Error(\"loopRollupDa txAggregator err,need to try again\", \"retry time\", retry, \"err\", err)\n\t\t\t\tif l.isRetry(&retry) {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\treturn false, err\n\t\t\t}\n\t\t\terr = l.disperseStoreData(transactionData)\n\t\t\tif err != nil {\n\t\t\t\tl.log.Error(\"loopRollupDa disperseStoreData err,need to try again\", \"retry time\", retry, \"err\", err, \"retry time\", retry)\n\t\t\t\tif l.isRetry(&retry) {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\treturn false, err\n\t\t\t}\n\t\t}\n\n\t\tsendTx := func() error {\n\t\t\t//start to publish transaction\n\t\t\tcCtx, cancel := context.WithTimeout(l.killCtx, 2*time.Minute)\n\t\t\tdefer cancel()\n\t\t\tr, err := l.sendInitDataStoreTransaction(cCtx)\n\t\t\tif err != nil {\n\t\t\t\tl.log.Error(\"failed to send init datastore transaction,need to try again\", \"retry time\", retry, \"err\", err)\n\t\t\t\treturn err\n\t\t\t}\n\n\t\t\treceipt, err := l.handleInitDataStoreReceipt(cCtx, r)\n\t\t\tif err != nil {\n\t\t\t\tl.log.Error(\"failed to send confirm data transaction,need to try again\", \"retry time\", retry, \"err\", err)\n\t\t\t\treturn err\n\t\t\t}\n\t\t\terr = l.handleConfirmDataStoreReceipt(receipt)\n\t\t\tif err != nil {\n\t\t\t\tl.log.Error(\"failed to handle confirm data transaction receipt,need to try again\", \"retry time\", retry, \"err\", err)\n\t\t\t\treturn err\n\t\t\t}\n\t\t\treturn nil\n\t\t}\n\t\terr := sendTx()\n\t\tif err != nil {\n\t\t\tif l.isRetry(&retry) {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\treturn false, err\n\t\t}\n\n\t}\n}\n\nfunc (l *BatchSubmitter) isRetry(retry *int32) bool {\n\t*retry = *retry + 1\n\tl.metr.RecordRollupRetry(*retry)\n\tif *retry > DaLoopRetryNum {\n\t\tl.log.Error(\"rollup failed by 10 attempts, need to re-store data to mantle da\")\n\t\t*retry = 0\n\t\tl.state.params = nil\n\t\tl.state.initStoreDataReceipt = nil\n\t\tl.metr.RecordDaRetry(1)\n\t\treturn true\n\t}\n\ttime.Sleep(5 * time.Second)\n\treturn true\n}\n\nfunc (l *BatchSubmitter) txAggregator() ([]byte, error) {\n\tvar txsData [][]byte\n\tvar transactionByte []byte\n\tsortTxIds := make([]txID, 0, len(l.state.daPendingTxData))\n\tl.state.daUnConfirmedTxID = l.state.daUnConfirmedTxID[:0]\n\tfor k, _ := range l.state.daPendingTxData {\n\t\tsortTxIds = append(sortTxIds, k)\n\t}\n\tsort.Slice(sortTxIds, func(i, j int) bool {\n\t\treturn sortTxIds[i].frameNumber < sortTxIds[j].frameNumber\n\t})\n\tfor _, v := range sortTxIds {\n\t\ttxData, _ := l.state.daPendingTxData[v]\n\t\ttxsData = append(txsData, txData.Bytes())\n\t\ttxnBufBytes, err := rlp.EncodeToBytes(txsData)\n\t\tif err != nil {\n\t\t\tl.log.Error(\"op-batcher unable to encode txn\", \"err\", err)\n\t\t\treturn nil, err\n\t\t}\n\t\tif uint64(len(txnBufBytes)) >= l.RollupMaxSize {\n\t\t\tl.log.Info(\"op-batcher transactionByte size is more than RollupMaxSize\", \"rollupMaxSize\", l.RollupMaxSize, \"txnBufBytes\", len(txnBufBytes), \"transactionByte\", len(transactionByte))\n\t\t\tl.metr.RecordTxOverMaxLimit()\n\t\t\tbreak\n\t\t}\n\t\ttransactionByte = txnBufBytes\n\t\tl.state.daUnConfirmedTxID = append(l.state.daUnConfirmedTxID, v)\n\t\tl.log.Info(\"added frame to daUnConfirmedTxID\", \"id\", v.String())\n\t}\n\tnodesNumber, err := l.getMantleDANodesNumber()\n\tif err != nil {\n\t\tl.log.Warn(\"op-batcher get nodes number failed\", \"err\", err)\n\t\tnodesNumber = l.MantleDaNodes\n\t}\n\tl.log.Info(\"op-batcher transactionByte\", \"size\", len(transactionByte))\n\tif len(transactionByte) <= BytesPerCoefficient*nodesNumber {\n\t\tpaddingBytes := make([]byte, (BytesPerCoefficient*nodesNumber)-len(transactionByte))\n\t\ttransactionByte = append(transactionByte, paddingBytes...)\n\t}\n\treturn transactionByte, nil\n}\n\nfunc (l *BatchSubmitter) disperseStoreData(txsData []byte) error {\n\n\tparams, err := l.callEncode(txsData)\n\tif err != nil {\n\t\treturn err\n\t}\n\tl.log.Info(\"operator info\", \"numSys\", params.NumSys, \"numPar\", params.NumPar, \"totalOperatorsIndex\", params.TotalOperatorsIndex, \"numTotal\", params.NumTotal)\n\t//cache params\n\tl.state.params = params\n\n\treturn nil\n}\n\nfunc (l *BatchSubmitter) sendInitDataStoreTransaction(ctx context.Context) (*types.Receipt, error) {\n\t//If initStoreData transaction has already been successfully executed, we don't need to re-execute .\n\tif l.state.initStoreDataReceipt != nil {\n\t\tl.log.Info(\"init store data transaction has been published successfully, skip to send transaction again\")\n\t\treturn l.state.initStoreDataReceipt, nil\n\t}\n\tuploadHeader, err := common.CreateUploadHeader(l.state.params)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tdataStoreTxData, err := l.dataStoreTxData(\n\t\tl.DataLayrServiceManagerABI, uploadHeader, uint8(l.state.params.Duration), l.state.params.ReferenceBlockNumber, l.state.params.TotalOperatorsIndex,\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tcandidate := txmgr.TxCandidate{\n\t\tTo:     &l.DataLayrServiceManagerAddr,\n\t\tTxData: dataStoreTxData,\n\t}\n\treceipt, err := l.txMgr.Send(ctx, candidate)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tl.metr.RecordBatchTxInitDataSubmitted()\n\tl.metr.RecordInitReferenceBlockNumber(l.state.params.ReferenceBlockNumber)\n\treturn receipt, nil\n}\n\nfunc (l *BatchSubmitter) callEncode(data []byte) (*common.StoreParams, error) {\n\tconn, err := grpc.Dial(l.DisperserSocket, grpc.WithTransportCredentials(insecure.NewCredentials()))\n\tif err != nil {\n\t\tl.log.Error(\"op-batcher disperser cannot connect to\", \"disperserSocket\", l.DisperserSocket)\n\t\treturn nil, err\n\t}\n\tdefer conn.Close()\n\tc := pb.NewDataDispersalClient(conn)\n\tctx, cancel := context.WithTimeout(context.Background(), l.DisperserTimeout)\n\tdefer cancel()\n\trequest := &pb.EncodeStoreRequest{\n\t\tDuration: l.DataStoreDuration,\n\t\tData:     data,\n\t}\n\topt := grpc.MaxCallSendMsgSize(EigenRollupMaxSize)\n\treply, err := c.EncodeStore(ctx, request, opt)\n\tl.log.Info(\"op-batcher get store\", \"reply\", reply.String())\n\tif err != nil {\n\t\tl.log.Error(\"op-batcher get store\", \"err\", err)\n\t\treturn nil, err\n\t}\n\tl.log.Info(\"op-batcher get store end\")\n\tg := reply.GetStore()\n\tfeeBigInt := new(big.Int).SetBytes(g.Fee)\n\tparams := &common.StoreParams{\n\t\tReferenceBlockNumber: g.ReferenceBlockNumber,\n\t\tTotalOperatorsIndex:  g.TotalOperatorsIndex,\n\t\tOrigDataSize:         g.OrigDataSize,\n\t\tNumTotal:             g.NumTotal,\n\t\tQuorum:               g.Quorum,\n\t\tNumSys:               g.NumSys,\n\t\tNumPar:               g.NumPar,\n\t\tDuration:             g.Duration,\n\t\tKzgCommit:            g.KzgCommit,\n\t\tLowDegreeProof:       g.LowDegreeProof,\n\t\tDegree:               g.Degree,\n\t\tTotalSize:            g.TotalSize,\n\t\tOrder:                g.Order,\n\t\tFee:                  feeBigInt,\n\t\tHeaderHash:           g.HeaderHash,\n\t\tDisperser:            g.Disperser,\n\t}\n\treturn params, nil\n}\n\nfunc (l *BatchSubmitter) dataStoreTxData(abi *abi.ABI, uploadHeader []byte, duration uint8, blockNumber uint32, totalOperatorsIndex uint32) ([]byte, error) {\n\tl.log.Info(\"encode initDataStore\", \"feePayer\", l.txMgr.From(), \"confirmor\", l.txMgr.From(), \"duration\", duration, \"referenceBlockNumber\", blockNumber, \"totalOperatorsIndex\", totalOperatorsIndex)\n\n\treturn abi.Pack(\n\t\t\"initDataStore\",\n\t\tl.txMgr.From(),\n\t\tl.txMgr.From(),\n\t\tduration,\n\t\tblockNumber,\n\t\ttotalOperatorsIndex,\n\t\tuploadHeader)\n}\n\nfunc (l *BatchSubmitter) callDisperse(headerHash []byte, messageHash []byte) (*common.DisperseMeta, error) {\n\tconn, err := grpc.Dial(l.DisperserSocket, grpc.WithInsecure())\n\tif err != nil {\n\t\tl.log.Error(\"op-batcher dial disperserSocket\", \"err\", err)\n\t\treturn nil, err\n\t}\n\tdefer conn.Close()\n\tc := pb.NewDataDispersalClient(conn)\n\tctx, cancel := context.WithTimeout(context.Background(), l.DisperserTimeout)\n\tdefer cancel()\n\trequest := &pb.DisperseStoreRequest{\n\t\tHeaderHash:  headerHash,\n\t\tMessageHash: messageHash,\n\t}\n\treply, err := c.DisperseStore(ctx, request)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tsigs := reply.GetSigs()\n\taggSig := common.AggregateSignature{\n\t\tAggSig:            sigs.AggSig,\n\t\tStoredAggPubkeyG1: sigs.StoredAggPubkeyG1,\n\t\tUsedAggPubkeyG2:   sigs.UsedAggPubkeyG2,\n\t\tNonSignerPubkeys:  sigs.NonSignerPubkeys,\n\t}\n\tmeta := &common.DisperseMeta{\n\t\tSigs:            aggSig,\n\t\tApkIndex:        reply.GetApkIndex(),\n\t\tTotalStakeIndex: reply.GetTotalStakeIndex(),\n\t}\n\treturn meta, nil\n}\n\nfunc (l *BatchSubmitter) confirmStoredData(txHash []byte, ctx context.Context) (*types.Receipt, error) {\n\tevent, ok := l.GraphClient.PollingInitDataStore(\n\t\tctx,\n\t\ttxHash[:],\n\t\tl.GraphPollingDuration,\n\t)\n\tif !ok {\n\t\tl.log.Error(\"op-batcher could not get initDataStore\", \"ok\", ok)\n\t\treturn nil, errors.New(\"op-batcher could not get initDataStore\")\n\t}\n\tl.log.Info(\"PollingInitDataStore\", \"storeNumber\", event.StoreNumber)\n\tmeta, err := l.callDisperse(\n\t\tl.state.params.HeaderHash,\n\t\tevent.MsgHash[:],\n\t)\n\tif err != nil {\n\t\tl.log.Error(\"op-batcher call disperse fail\", \"err\", err)\n\t\treturn nil, err\n\t}\n\tif len(meta.Sigs.NonSignerPubkeys) != 0 {\n\t\tl.log.Error(\"op-batcher call disperse success. However, there are nodes that do not participate in the signature.\", \"number\", len(meta.Sigs.NonSignerPubkeys))\n\t\tl.metr.RecordDaNonSignerPubkeys(len(meta.Sigs.NonSignerPubkeys))\n\t\treturn nil, errors.New(\"disperse meta nonSignerPubkeys is not 0\")\n\t}\n\n\tcallData, err := common.MakeCalldata(l.state.params, *meta, event.StoreNumber, event.MsgHash)\n\tif err != nil {\n\t\tl.log.Error(\"op-batcher make call data fail\", \"err\", err)\n\t\treturn nil, err\n\t}\n\tsearchData := &bindings.IDataLayrServiceManagerDataStoreSearchData{\n\t\tDuration:  event.Duration,\n\t\tTimestamp: new(big.Int).SetUint64(uint64(event.InitTime)),\n\t\tIndex:     event.Index,\n\t\tMetadata: bindings.IDataLayrServiceManagerDataStoreMetadata{\n\t\t\tHeaderHash:           event.DataCommitment,\n\t\t\tDurationDataStoreId:  event.DurationDataStoreId,\n\t\t\tGlobalDataStoreId:    event.StoreNumber,\n\t\t\tReferenceBlockNumber: event.ReferenceBlockNumber,\n\t\t\tBlockNumber:          uint32(event.InitBlockNumber.Uint64()),\n\t\t\tFee:                  event.Fee,\n\t\t\tConfirmer:            ecommon.HexToAddress(event.Confirmer),\n\t\t\tSignatoryRecordHash:  [32]byte{},\n\t\t},\n\t}\n\n\tconfirmTxData, err := l.confirmDataTxData(l.DataLayrServiceManagerABI, callData, searchData)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tcandidate := txmgr.TxCandidate{\n\t\tTo:     &l.DataLayrServiceManagerAddr,\n\t\tTxData: confirmTxData,\n\t}\n\n\ttxReceipt, err := l.txMgr.Send(ctx, candidate)\n\tif err != nil {\n\t\tlog.Error(\"Tx manager send transaction fail\", \"err\", err)\n\t\treturn nil, err\n\t}\n\tl.metr.RecordConfirmedDataStoreId(event.StoreNumber)\n\treturn txReceipt, nil\n}\n\nfunc (l *BatchSubmitter) confirmDataTxData(abi *abi.ABI, callData []byte, searchData *bindings.IDataLayrServiceManagerDataStoreSearchData) ([]byte, error) {\n\treturn abi.Pack(\n\t\t\"confirmDataStore\",\n\t\tcallData,\n\t\tsearchData)\n}\n\nfunc (l *BatchSubmitter) handleInitDataStoreReceipt(ctx context.Context, txReceiptIn *types.Receipt) (*types.Receipt, error) {\n\tif txReceiptIn.Status == types.ReceiptStatusFailed {\n\t\tl.log.Error(\"init datastore tx successfully published but reverted\", \"txHash\", txReceiptIn.TxHash.String())\n\t\tl.metr.RecordBatchTxInitDataFailed()\n\t\treturn nil, ErrInitDataStore\n\t}\n\tl.metr.RecordBatchTxInitDataSuccess()\n\tl.log.Info(\"init datastore tx successfully published\", \"txHash\", txReceiptIn.TxHash.String())\n\tl.state.initStoreDataReceipt = txReceiptIn\n\t// start to confirmData\n\ttxReceiptOut, err := l.confirmStoredData(txReceiptIn.TxHash.Bytes(), ctx)\n\tif err != nil {\n\t\tl.log.Error(\"failed to confirm data\", \"err\", err)\n\t\treturn nil, err\n\t}\n\tl.metr.RecordBatchTxConfirmDataSubmitted()\n\treturn txReceiptOut, nil\n\n}\n\nfunc (l *BatchSubmitter) handleConfirmDataStoreReceipt(r *types.Receipt) error {\n\tif r.Status == types.ReceiptStatusFailed {\n\t\tl.log.Error(\"unable to publish confirm data store tx\", \"txHash\", r.TxHash.String())\n\t\tl.metr.RecordBatchTxConfirmDataFailed()\n\t\treturn errors.New(\"unable to publish confirm data store tx\")\n\t}\n\tl.log.Info(\"transaction confirmed\", \"txHash\", r.TxHash.String(), \"status\", r.Status, \"blockHash\", r.BlockHash.String(), \"blockNumber\", r.BlockNumber)\n\tl.metr.RecordBatchTxConfirmDataSuccess()\n\tl.recordConfirmedEigenDATx(r)\n\treturn nil\n}\n\nfunc (l *BatchSubmitter) recordConfirmedEigenDATx(receipt *types.Receipt) {\n\tl1block := eth.BlockID{Number: receipt.BlockNumber.Uint64(), Hash: receipt.BlockHash}\n\n\tfor _, id := range l.state.daUnConfirmedTxID {\n\t\tl.state.TxConfirmed(id, l1block)\n\t\tl.daTxDataConfirmed(id)\n\t}\n\tl.state.params = nil\n\tl.state.initStoreDataReceipt = nil\n\tl.state.metr.RecordRollupRetry(0)\n\tl.state.metr.RecordDaRetry(0)\n}\n\nfunc (l *BatchSubmitter) getMantleDANodesNumber() (int, error) {\n\toperators, err := l.GraphClient.QueryOperatorsByStatus()\n\tif err != nil {\n\t\tl.log.Error(\"op-batcher query mantle da operators fail\", \"err\", err)\n\t\treturn 0, err\n\t}\n\treturn len(operators), nil\n}\n\nfunc (l *BatchSubmitter) daTxDataConfirmed(id txID) {\n\tif _, ok := l.state.daPendingTxData[id]; !ok {\n\t\tl.log.Warn(\"daConfirmed, unknown txID of txData  marked as confirmed\", \"id\", id.String())\n\t\treturn\n\t}\n\tdelete(l.state.daPendingTxData, id)\n}\n\nfunc (l *BatchSubmitter) reset() {\n\tl.state.clearPendingChannel()\n\tl.state.clearMantleDAStatus()\n\tl.lastStoredBlock = eth.BlockID{}\n}"
    }
  ]
}