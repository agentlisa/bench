{
  "Title": "[G-36] Use bitwise operators instead of multiplication/division by value of 2 to the power of N",
  "Content": "\n**Files:** `L2ContractHelper.sol`, `Compressor.sol`, `Executor.sol`, `Merkle.sol`\n\n[File: code/system-contracts/contracts/Compressor.sol](https://github.com/code-423n4/2024-03-zksync/blob/main/code/system-contracts/contracts/Compressor.sol#L52)\n```solidity\n52:                 encodedData.length * 4 == _bytecode.length,\n[...]\n57:                 dictionary.length / 8 <= encodedData.length / 2,\n[...]\n62:                 uint256 indexOfEncodedChunk = uint256(encodedData.readUint16(encodedDataPointer)) * 8;\n[...]\n66:                 uint64 realChunk = _bytecode.readUint64(encodedDataPointer * 4);\n```\n\n[File: code/contracts/ethereum/contracts/state-transition/libraries/Merkle.sol](https://github.com/code-423n4/2024-03-zksync/blob/main/code/contracts/ethereum/contracts/state-transition/libraries/Merkle.sol#L33)\n```solidity\n33:             _index /= 2;\n```\n[File: code/contracts/ethereum/contracts/common/libraries/L2ContractHelper.sol](https://github.com/code-423n4/2024-03-zksync/blob/main/code/contracts/ethereum/contracts/common/libraries/L2ContractHelper.sol#L25)\n```solidity\n25:         uint256 bytecodeLenInWords = _bytecode.length / 32;\n26:         require(bytecodeLenInWords < 2 ** 16, \"pp\"); // bytecode length must be less than 2^16 words\n```\n\n[File: code/contracts/ethereum/contracts/state-transition/chain-deps/facets/Executor.sol](https://github.com/code-423n4/2024-03-zksync/blob/main/code/contracts/ethereum/contracts/state-transition/chain-deps/facets/Executor.sol#L581)\n```solidity\n581:             blobAuxOutputWords[i * 2] = _blobHashes[i];\n582:             blobAuxOutputWords[i * 2 + 1] = _blobCommitments[i];\n```\n",
  "Impact": "GAS",
  "Source": "https://code4rena.com/reports/2024-03-zksync",
  "Code": [
    {
      "filename": "code/system-contracts/contracts/Compressor.sol",
      "content": "// SPDX-License-Identifier: MIT\n\npragma solidity 0.8.20;\n\nimport {ICompressor, OPERATION_BITMASK, LENGTH_BITS_OFFSET, MAX_ENUMERATION_INDEX_SIZE} from \"./interfaces/ICompressor.sol\";\nimport {ISystemContract} from \"./interfaces/ISystemContract.sol\";\nimport {Utils} from \"./libraries/Utils.sol\";\nimport {UnsafeBytesCalldata} from \"./libraries/UnsafeBytesCalldata.sol\";\nimport {EfficientCall} from \"./libraries/EfficientCall.sol\";\nimport {L1_MESSENGER_CONTRACT, INITIAL_WRITE_STARTING_POSITION, COMPRESSED_INITIAL_WRITE_SIZE, STATE_DIFF_ENTRY_SIZE, STATE_DIFF_ENUM_INDEX_OFFSET, STATE_DIFF_FINAL_VALUE_OFFSET, STATE_DIFF_DERIVED_KEY_OFFSET, DERIVED_KEY_LENGTH, VALUE_LENGTH, ENUM_INDEX_LENGTH, KNOWN_CODE_STORAGE_CONTRACT} from \"./Constants.sol\";\n\n/**\n * @author Matter Labs\n * @custom:security-contact security@matterlabs.dev\n * @notice Contract with code pertaining to compression for zkEVM; at the moment this is used for bytecode compression\n * and state diff compression validation.\n * @dev Every deployed bytecode/published state diffs in zkEVM should be publicly restorable from the L1 data availability.\n * For this reason, the user may request the sequencer to publish the original bytecode and mark it as known.\n * Or the user may compress the bytecode and publish it instead (fewer data onchain!). At the end of every L1 Batch\n * we publish pubdata, part of which contains the state diffs that occurred within the batch.\n */\ncontract Compressor is ICompressor, ISystemContract {\n    using UnsafeBytesCalldata for bytes;\n\n    /// @notice Verify the compressed bytecode and publish it on the L1.\n    /// @param _bytecode The original bytecode to be verified against.\n    /// @param _rawCompressedData The compressed bytecode in a format of:\n    ///    - 2 bytes: the length of the dictionary\n    ///    - N bytes: the dictionary\n    ///    - M bytes: the encoded data\n    /// @return bytecodeHash The hash of the original bytecode.\n    /// @dev The dictionary is a sequence of 8-byte chunks, each of them has the associated index.\n    /// @dev The encoded data is a sequence of 2-byte chunks, each of them is an index of the dictionary.\n    /// @dev The compression algorithm works as follows:\n    ///     1. The original bytecode is split into 8-byte chunks.\n    ///     Since the bytecode size is always a multiple of 32, this is always possible.\n    ///     2. For each 8-byte chunk in the original bytecode:\n    ///         * If the chunk is not already in the dictionary, it is added to the dictionary array.\n    ///         * If the dictionary becomes overcrowded (2^16 + 1 elements), the compression process will fail.\n    ///         * The 2-byte index of the chunk in the dictionary is added to the encoded data.\n    /// @dev Currently, the method may be called only from the bootloader because the server is not ready to publish bytecodes\n    /// in internal transactions. However, in the future, we will allow everyone to publish compressed bytecodes.\n    /// @dev Read more about the compression: https://github.com/matter-labs/zksync-era/blob/main/docs/guides/advanced/compression.md\n    function publishCompressedBytecode(\n        bytes calldata _bytecode,\n        bytes calldata _rawCompressedData\n    ) external payable onlyCallFromBootloader returns (bytes32 bytecodeHash) {\n        unchecked {\n            (bytes calldata dictionary, bytes calldata encodedData) = _decodeRawBytecode(_rawCompressedData);\n\n            require(\n                encodedData.length * 4 == _bytecode.length,\n                \"Encoded data length should be 4 times shorter than the original bytecode\"\n            );\n\n            require(\n                dictionary.length / 8 <= encodedData.length / 2,\n                \"Dictionary should have at most the same number of entries as the encoded data\"\n            );\n\n            for (uint256 encodedDataPointer = 0; encodedDataPointer < encodedData.length; encodedDataPointer += 2) {\n                uint256 indexOfEncodedChunk = uint256(encodedData.readUint16(encodedDataPointer)) * 8;\n                require(indexOfEncodedChunk < dictionary.length, \"Encoded chunk index is out of bounds\");\n\n                uint64 encodedChunk = dictionary.readUint64(indexOfEncodedChunk);\n                uint64 realChunk = _bytecode.readUint64(encodedDataPointer * 4);\n\n                require(encodedChunk == realChunk, \"Encoded chunk does not match the original bytecode\");\n            }\n        }\n\n        bytecodeHash = Utils.hashL2Bytecode(_bytecode);\n        L1_MESSENGER_CONTRACT.sendToL1(_rawCompressedData);\n        KNOWN_CODE_STORAGE_CONTRACT.markBytecodeAsPublished(bytecodeHash);\n    }\n\n    /// @notice Verifies that the compression of state diffs has been done correctly for the {_stateDiffs} param.\n    /// @param _numberOfStateDiffs The number of state diffs being checked.\n    /// @param _enumerationIndexSize Number of bytes used to represent an enumeration index for repeated writes.\n    /// @param _stateDiffs Encoded full state diff structs. See the first dev comment below for encoding.\n    /// @param _compressedStateDiffs The compressed state diffs\n    /// @dev We don't verify that the size of {_stateDiffs} is equivalent to {_numberOfStateDiffs} * STATE_DIFF_ENTRY_SIZE since that check is\n    ///      done within the L1Messenger calling contract.\n    /// @return stateDiffHash Hash of the encoded (uncompressed) state diffs to be committed to via system log.\n    /// @dev This check assumes that the ordering of state diffs are sorted by (address, key) for the encoded state diffs and\n    ///      then the compressed are sorted the same but with all the initial writes coming before the repeated writes.\n    /// @dev state diff:   [20bytes address][32bytes key][32bytes derived key][8bytes enum index][32bytes initial value][32bytes final value]\n    /// @dev The compression format:\n    ///     - 2 bytes: number of initial writes\n    ///     - N bytes initial writes\n    ///         - 32 bytes derived key\n    ///         - 1 byte metadata:\n    ///             - first 5 bits: length in bytes of compressed value\n    ///             - last 3 bits: operation\n    ///                 - 0 -> Nothing (32 bytes)\n    ///                 - 1 -> Add\n    ///                 - 2 -> Subtract\n    ///                 - 3 -> Transform (< 32 bytes)\n    ///         - Len Bytes: Compressed Value\n    ///     - M bytes repeated writes\n    ///         - {_enumerationIndexSize} bytes for enumeration index\n    ///         - 1 byte metadata:\n    ///             - first 5 bits: length in bytes of compressed value\n    ///             - last 3 bits: operation\n    ///                 - 0 -> Nothing (32 bytes)\n    ///                 - 1 -> Add\n    ///                 - 2 -> Subtract\n    ///                 - 3 -> Transform (< 32 bytes)\n    ///         - Len Bytes: Compressed Value\n    function verifyCompressedStateDiffs(\n        uint256 _numberOfStateDiffs,\n        uint256 _enumerationIndexSize,\n        bytes calldata _stateDiffs,\n        bytes calldata _compressedStateDiffs\n    ) external payable onlyCallFrom(address(L1_MESSENGER_CONTRACT)) returns (bytes32 stateDiffHash) {\n        // We do not enforce the operator to use the optimal, i.e. the minimally possible _enumerationIndexSize.\n        // We do enforce however, that the _enumerationIndexSize is not larger than 8 bytes long, which is the\n        // maximal ever possible size for enumeration index.\n        require(_enumerationIndexSize <= MAX_ENUMERATION_INDEX_SIZE, \"enumeration index size is too large\");\n\n        uint256 numberOfInitialWrites = uint256(_compressedStateDiffs.readUint16(0));\n\n        uint256 stateDiffPtr = 2;\n        uint256 numInitialWritesProcessed = 0;\n\n        // Process initial writes\n        for (uint256 i = 0; i < _numberOfStateDiffs * STATE_DIFF_ENTRY_SIZE; i += STATE_DIFF_ENTRY_SIZE) {\n            bytes calldata stateDiff = _stateDiffs[i:i + STATE_DIFF_ENTRY_SIZE];\n            uint64 enumIndex = stateDiff.readUint64(84);\n            if (enumIndex != 0) {\n                // It is a repeated write, so we skip it.\n                continue;\n            }\n\n            numInitialWritesProcessed++;\n\n            bytes32 derivedKey = stateDiff.readBytes32(52);\n            uint256 initValue = stateDiff.readUint256(92);\n            uint256 finalValue = stateDiff.readUint256(124);\n            require(derivedKey == _compressedStateDiffs.readBytes32(stateDiffPtr), \"iw: initial key mismatch\");\n            stateDiffPtr += 32;\n\n            uint8 metadata = uint8(bytes1(_compressedStateDiffs[stateDiffPtr]));\n            stateDiffPtr++;\n            uint8 operation = metadata & OPERATION_BITMASK;\n            uint8 len = operation == 0 ? 32 : metadata >> LENGTH_BITS_OFFSET;\n            _verifyValueCompression(\n                initValue,\n                finalValue,\n                operation,\n                _compressedStateDiffs[stateDiffPtr:stateDiffPtr + len]\n            );\n            stateDiffPtr += len;\n        }\n\n        require(numInitialWritesProcessed == numberOfInitialWrites, \"Incorrect number of initial storage diffs\");\n\n        // Process repeated writes\n        for (uint256 i = 0; i < _numberOfStateDiffs * STATE_DIFF_ENTRY_SIZE; i += STATE_DIFF_ENTRY_SIZE) {\n            bytes calldata stateDiff = _stateDiffs[i:i + STATE_DIFF_ENTRY_SIZE];\n            uint64 enumIndex = stateDiff.readUint64(84);\n            if (enumIndex == 0) {\n                continue;\n            }\n\n            uint256 initValue = stateDiff.readUint256(92);\n            uint256 finalValue = stateDiff.readUint256(124);\n            uint256 compressedEnumIndex = _sliceToUint256(\n                _compressedStateDiffs[stateDiffPtr:stateDiffPtr + _enumerationIndexSize]\n            );\n            require(enumIndex == compressedEnumIndex, \"rw: enum key mismatch\");\n            stateDiffPtr += _enumerationIndexSize;\n\n            uint8 metadata = uint8(bytes1(_compressedStateDiffs[stateDiffPtr]));\n            stateDiffPtr += 1;\n            uint8 operation = metadata & OPERATION_BITMASK;\n            uint8 len = operation == 0 ? 32 : metadata >> LENGTH_BITS_OFFSET;\n            _verifyValueCompression(\n                initValue,\n                finalValue,\n                operation,\n                _compressedStateDiffs[stateDiffPtr:stateDiffPtr + len]\n            );\n            stateDiffPtr += len;\n        }\n\n        require(stateDiffPtr == _compressedStateDiffs.length, \"Extra data in _compressedStateDiffs\");\n\n        stateDiffHash = EfficientCall.keccak(_stateDiffs);\n    }\n\n    /// @notice Decode the raw compressed data into the dictionary and the encoded data.\n    /// @param _rawCompressedData The compressed bytecode in a format of:\n    ///    - 2 bytes: the bytes length of the dictionary\n    ///    - N bytes: the dictionary\n    ///    - M bytes: the encoded data\n    function _decodeRawBytecode(\n        bytes calldata _rawCompressedData\n    ) internal pure returns (bytes calldata dictionary, bytes calldata encodedData) {\n        unchecked {\n            // The dictionary length can't be more than 2^16, so it fits into 2 bytes.\n            uint256 dictionaryLen = uint256(_rawCompressedData.readUint16(0));\n            dictionary = _rawCompressedData[2:2 + dictionaryLen * 8];\n            encodedData = _rawCompressedData[2 + dictionaryLen * 8:];\n        }\n    }\n\n    /// @notice Verify value compression was done correct given initial value, final value, operation, and compressed value\n    /// @param _initialValue Previous value of key/enumeration index.\n    /// @param _finalValue Updated value of key/enumeration index.\n    /// @param _operation The operation that was performed on value.\n    /// @param _compressedValue The slice of calldata with compressed value either representing the final\n    /// value or difference between initial and final value. It should be of arbitrary length less than or equal to 32 bytes.\n    /// @dev It is the responsibility of the caller of this function to ensure that the `_compressedValue` has length no longer than 32 bytes.\n    /// @dev Operation id mapping:\n    /// 0 -> Nothing (32 bytes)\n    /// 1 -> Add\n    /// 2 -> Subtract\n    /// 3 -> Transform (< 32 bytes)\n    function _verifyValueCompression(\n        uint256 _initialValue,\n        uint256 _finalValue,\n        uint256 _operation,\n        bytes calldata _compressedValue\n    ) internal pure {\n        uint256 convertedValue = _sliceToUint256(_compressedValue);\n\n        unchecked {\n            if (_operation == 0 || _operation == 3) {\n                require(convertedValue == _finalValue, \"transform or no compression: compressed and final mismatch\");\n            } else if (_operation == 1) {\n                require(\n                    _initialValue + convertedValue == _finalValue,\n                    \"add: initial plus converted not equal to final\"\n                );\n            } else if (_operation == 2) {\n                require(\n                    _initialValue - convertedValue == _finalValue,\n                    \"sub: initial minus converted not equal to final\"\n                );\n            } else {\n                revert(\"unsupported operation\");\n            }\n        }\n    }\n\n    /// @notice Converts a calldata slice into uint256. It is the responsibility of the caller to ensure that\n    /// the _calldataSlice has length no longer than 32 bytes\n    /// @param _calldataSlice The calldata slice to convert to uint256\n    /// @return number The uint256 representation of the calldata slice\n    function _sliceToUint256(bytes calldata _calldataSlice) internal pure returns (uint256 number) {\n        number = uint256(bytes32(_calldataSlice));\n        number >>= (256 - (_calldataSlice.length * 8));\n    }\n}"
    },
    {
      "filename": "code/contracts/ethereum/contracts/state-transition/libraries/Merkle.sol",
      "content": "// SPDX-License-Identifier: MIT\n\npragma solidity 0.8.20;\n\nimport {UncheckedMath} from \"../../common/libraries/UncheckedMath.sol\";\n\n/// @author Matter Labs\n/// @custom:security-contact security@matterlabs.dev\nlibrary Merkle {\n    using UncheckedMath for uint256;\n\n    /// @dev Calculate Merkle root by the provided Merkle proof.\n    /// NOTE: When using this function, check that the _path length is equal to the tree height to prevent shorter/longer paths attack\n    /// @param _path Merkle path from the leaf to the root\n    /// @param _index Leaf index in the tree\n    /// @param _itemHash Hash of leaf content\n    /// @return The Merkle root\n    function calculateRoot(\n        bytes32[] calldata _path,\n        uint256 _index,\n        bytes32 _itemHash\n    ) internal pure returns (bytes32) {\n        uint256 pathLength = _path.length;\n        require(pathLength > 0, \"xc\");\n        require(pathLength < 256, \"bt\");\n        require(_index < (1 << pathLength), \"px\");\n\n        bytes32 currentHash = _itemHash;\n        for (uint256 i; i < pathLength; i = i.uncheckedInc()) {\n            currentHash = (_index % 2 == 0)\n                ? _efficientHash(currentHash, _path[i])\n                : _efficientHash(_path[i], currentHash);\n            _index /= 2;\n        }\n\n        return currentHash;\n    }\n\n    /// @dev Keccak hash of the concatenation of two 32-byte words\n    function _efficientHash(bytes32 _lhs, bytes32 _rhs) private pure returns (bytes32 result) {\n        assembly {\n            mstore(0x00, _lhs)\n            mstore(0x20, _rhs)\n            result := keccak256(0x00, 0x40)\n        }\n    }\n}"
    },
    {
      "filename": "code/contracts/ethereum/contracts/common/libraries/L2ContractHelper.sol",
      "content": "// SPDX-License-Identifier: MIT\n\npragma solidity 0.8.20;\n\n/**\n * @author Matter Labs\n * @custom:security-contact security@matterlabs.dev\n * @notice Helper library for working with L2 contracts on L1.\n */\nlibrary L2ContractHelper {\n    /// @dev The prefix used to create CREATE2 addresses.\n    bytes32 private constant CREATE2_PREFIX = keccak256(\"zksyncCreate2\");\n\n    /// @notice Validate the bytecode format and calculate its hash.\n    /// @param _bytecode The bytecode to hash.\n    /// @return hashedBytecode The 32-byte hash of the bytecode.\n    /// Note: The function reverts the execution if the bytecode has non expected format:\n    /// - Bytecode bytes length is not a multiple of 32\n    /// - Bytecode bytes length is not less than 2^21 bytes (2^16 words)\n    /// - Bytecode words length is not odd\n    function hashL2Bytecode(bytes memory _bytecode) internal pure returns (bytes32 hashedBytecode) {\n        // Note that the length of the bytecode must be provided in 32-byte words.\n        require(_bytecode.length % 32 == 0, \"pq\");\n\n        uint256 bytecodeLenInWords = _bytecode.length / 32;\n        require(bytecodeLenInWords < 2 ** 16, \"pp\"); // bytecode length must be less than 2^16 words\n        require(bytecodeLenInWords % 2 == 1, \"ps\"); // bytecode length in words must be odd\n        hashedBytecode = sha256(_bytecode) & 0x00000000FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF;\n        // Setting the version of the hash\n        hashedBytecode = (hashedBytecode | bytes32(uint256(1 << 248)));\n        // Setting the length\n        hashedBytecode = hashedBytecode | bytes32(bytecodeLenInWords << 224);\n    }\n\n    /// @notice Validates the format of the given bytecode hash.\n    /// @dev Due to the specification of the L2 bytecode hash, not every 32 bytes could be a legit bytecode hash.\n    /// @dev The function reverts on invalid bytecode hash formam.\n    /// @param _bytecodeHash The hash of the bytecode to validate.\n    function validateBytecodeHash(bytes32 _bytecodeHash) internal pure {\n        uint8 version = uint8(_bytecodeHash[0]);\n        require(version == 1 && _bytecodeHash[1] == bytes1(0), \"zf\"); // Incorrectly formatted bytecodeHash\n\n        require(bytecodeLen(_bytecodeHash) % 2 == 1, \"uy\"); // Code length in words must be odd\n    }\n\n    /// @notice Returns the length of the bytecode associated with the given hash.\n    /// @param _bytecodeHash The hash of the bytecode.\n    /// @return codeLengthInWords The length of the bytecode in words.\n    function bytecodeLen(bytes32 _bytecodeHash) internal pure returns (uint256 codeLengthInWords) {\n        codeLengthInWords = uint256(uint8(_bytecodeHash[2])) * 256 + uint256(uint8(_bytecodeHash[3]));\n    }\n\n    /// @notice Computes the create2 address for a Layer 2 contract.\n    /// @param _sender The address of the sender.\n    /// @param _salt The salt value to use in the create2 address computation.\n    /// @param _bytecodeHash The contract bytecode hash.\n    /// @param _constructorInputHash The hash of the constructor input data.\n    /// @return The create2 address of the contract.\n    /// NOTE: L2 create2 derivation is different from L1 derivation!\n    function computeCreate2Address(\n        address _sender,\n        bytes32 _salt,\n        bytes32 _bytecodeHash,\n        bytes32 _constructorInputHash\n    ) internal pure returns (address) {\n        bytes32 senderBytes = bytes32(uint256(uint160(_sender)));\n        bytes32 data = keccak256(\n            bytes.concat(CREATE2_PREFIX, senderBytes, _salt, _bytecodeHash, _constructorInputHash)\n        );\n\n        return address(uint160(uint256(data)));\n    }\n}"
    },
    {
      "filename": "code/contracts/ethereum/contracts/state-transition/chain-deps/facets/Executor.sol",
      "content": "// SPDX-License-Identifier: MIT\n\npragma solidity 0.8.20;\n\nimport {ZkSyncStateTransitionBase} from \"./ZkSyncStateTransitionBase.sol\";\nimport {COMMIT_TIMESTAMP_NOT_OLDER, COMMIT_TIMESTAMP_APPROXIMATION_DELTA, EMPTY_STRING_KECCAK, L2_TO_L1_LOG_SERIALIZE_SIZE, MAX_L2_TO_L1_LOGS_COMMITMENT_BYTES, PACKED_L2_BLOCK_TIMESTAMP_MASK, PUBLIC_INPUT_SHIFT, POINT_EVALUATION_PRECOMPILE_ADDR} from \"../../../common/Config.sol\";\nimport {IExecutor, L2_LOG_ADDRESS_OFFSET, L2_LOG_KEY_OFFSET, L2_LOG_VALUE_OFFSET, SystemLogKey, LogProcessingOutput, PubdataSource, BLS_MODULUS, PUBDATA_COMMITMENT_SIZE, PUBDATA_COMMITMENT_CLAIMED_VALUE_OFFSET, PUBDATA_COMMITMENT_COMMITMENT_OFFSET, MAX_NUMBER_OF_BLOBS, TOTAL_BLOBS_IN_COMMITMENT} from \"../../chain-interfaces/IExecutor.sol\";\nimport {PriorityQueue, PriorityOperation} from \"../../libraries/PriorityQueue.sol\";\nimport {UncheckedMath} from \"../../../common/libraries/UncheckedMath.sol\";\nimport {UnsafeBytes} from \"../../../common/libraries/UnsafeBytes.sol\";\nimport {VerifierParams} from \"../../chain-interfaces/IVerifier.sol\";\nimport {L2_BOOTLOADER_ADDRESS, L2_TO_L1_MESSENGER_SYSTEM_CONTRACT_ADDR, L2_SYSTEM_CONTEXT_SYSTEM_CONTRACT_ADDR, L2_PUBDATA_CHUNK_PUBLISHER_ADDR} from \"../../../common/L2ContractAddresses.sol\";\nimport {PubdataPricingMode} from \"../ZkSyncStateTransitionStorage.sol\";\nimport {IStateTransitionManager} from \"../../IStateTransitionManager.sol\";\n\n// While formally the following import is not used, it is needed to inherit documentation from it\nimport {IZkSyncStateTransitionBase} from \"../../chain-interfaces/IZkSyncStateTransitionBase.sol\";\n\n/// @title zkSync hyperchain Executor contract capable of processing events emitted in the zkSync hyperchain protocol.\n/// @author Matter Labs\n/// @custom:security-contact security@matterlabs.dev\ncontract ExecutorFacet is ZkSyncStateTransitionBase, IExecutor {\n    using UncheckedMath for uint256;\n    using PriorityQueue for PriorityQueue.Queue;\n\n    /// @inheritdoc IZkSyncStateTransitionBase\n    string public constant override getName = \"ExecutorFacet\";\n\n    /// @dev Process one batch commit using the previous batch StoredBatchInfo\n    /// @dev returns new batch StoredBatchInfo\n    /// @notice Does not change storage\n    function _commitOneBatch(\n        StoredBatchInfo memory _previousBatch,\n        CommitBatchInfo calldata _newBatch,\n        bytes32 _expectedSystemContractUpgradeTxHash\n    ) internal view returns (StoredBatchInfo memory) {\n        require(_newBatch.batchNumber == _previousBatch.batchNumber + 1, \"f\"); // only commit next batch\n\n        uint8 pubdataSource = uint8(bytes1(_newBatch.pubdataCommitments[0]));\n        require(pubdataSource == uint8(PubdataSource.Calldata) || pubdataSource == uint8(PubdataSource.Blob), \"us\");\n\n        // Check that batch contain all meta information for L2 logs.\n        // Get the chained hash of priority transaction hashes.\n        LogProcessingOutput memory logOutput = _processL2Logs(_newBatch, _expectedSystemContractUpgradeTxHash);\n\n        bytes32[] memory blobCommitments = new bytes32[](MAX_NUMBER_OF_BLOBS);\n        bytes32[] memory blobHashes = new bytes32[](MAX_NUMBER_OF_BLOBS);\n        if (s.feeParams.pubdataPricingMode == PubdataPricingMode.Validium) {\n            // skipping data validation for validium, we just check that the data is empty\n            require(logOutput.pubdataHash == 0x00, \"v0h\");\n            require(_newBatch.pubdataCommitments.length == 1);\n        } else if (pubdataSource == uint8(PubdataSource.Blob)) {\n            // We want only want to include the actual blob linear hashes when we send pubdata via blobs.\n            // Otherwise we should be using bytes32(0)\n            blobHashes[0] = logOutput.blob1Hash;\n            blobHashes[1] = logOutput.blob2Hash;\n            // In this scenario, pubdataCommitments is a list of: opening point (16 bytes) || claimed value (32 bytes) || commitment (48 bytes) || proof (48 bytes)) = 144 bytes\n            blobCommitments = _verifyBlobInformation(_newBatch.pubdataCommitments[1:], blobHashes);\n        } else if (pubdataSource == uint8(PubdataSource.Calldata)) {\n            // In this scenario pubdataCommitments is actual pubdata consisting of l2 to l1 logs, l2 to l1 message, compressed smart contract bytecode, and compressed state diffs\n            require(\n                logOutput.pubdataHash ==\n                    keccak256(_newBatch.pubdataCommitments[1:_newBatch.pubdataCommitments.length - 32]),\n                \"wp\"\n            );\n            blobHashes[0] = logOutput.blob1Hash;\n            blobCommitments[0] = bytes32(\n                _newBatch.pubdataCommitments[_newBatch.pubdataCommitments.length - 32:_newBatch\n                    .pubdataCommitments\n                    .length]\n            );\n        }\n\n        require(_previousBatch.batchHash == logOutput.previousBatchHash, \"l\");\n        // Check that the priority operation hash in the L2 logs is as expected\n        require(logOutput.chainedPriorityTxsHash == _newBatch.priorityOperationsHash, \"t\");\n        // Check that the number of processed priority operations is as expected\n        require(logOutput.numberOfLayer1Txs == _newBatch.numberOfLayer1Txs, \"ta\");\n\n        // Check the timestamp of the new batch\n        _verifyBatchTimestamp(logOutput.packedBatchAndL2BlockTimestamp, _newBatch.timestamp, _previousBatch.timestamp);\n\n        // Create batch commitment for the proof verification\n        bytes32 commitment = _createBatchCommitment(_newBatch, logOutput.stateDiffHash, blobCommitments, blobHashes);\n\n        return\n            StoredBatchInfo(\n                _newBatch.batchNumber,\n                _newBatch.newStateRoot,\n                _newBatch.indexRepeatedStorageChanges,\n                _newBatch.numberOfLayer1Txs,\n                _newBatch.priorityOperationsHash,\n                logOutput.l2LogsTreeRoot,\n                _newBatch.timestamp,\n                commitment\n            );\n    }\n\n    /// @notice checks that the timestamps of both the new batch and the new L2 block are correct.\n    /// @param _packedBatchAndL2BlockTimestamp - packed batch and L2 block timestamp in a format of batchTimestamp * 2**128 + l2BatchTimestamp\n    /// @param _expectedBatchTimestamp - expected batch timestamp\n    /// @param _previousBatchTimestamp - the timestamp of the previous batch\n    function _verifyBatchTimestamp(\n        uint256 _packedBatchAndL2BlockTimestamp,\n        uint256 _expectedBatchTimestamp,\n        uint256 _previousBatchTimestamp\n    ) internal view {\n        // Check that the timestamp that came from the system context is expected\n        uint256 batchTimestamp = _packedBatchAndL2BlockTimestamp >> 128;\n        require(batchTimestamp == _expectedBatchTimestamp, \"tb\");\n\n        // While the fact that _previousBatchTimestamp < batchTimestamp is already checked on L2,\n        // we double check it here for clarity\n        require(_previousBatchTimestamp < batchTimestamp, \"h3\");\n\n        uint256 lastL2BlockTimestamp = _packedBatchAndL2BlockTimestamp & PACKED_L2_BLOCK_TIMESTAMP_MASK;\n\n        // All L2 blocks have timestamps within the range of [batchTimestamp, lastL2BlockTimestamp].\n        // So here we need to only double check that:\n        // - The timestamp of the batch is not too small.\n        // - The timestamp of the last L2 block is not too big.\n        require(block.timestamp - COMMIT_TIMESTAMP_NOT_OLDER <= batchTimestamp, \"h1\"); // New batch timestamp is too small\n        require(lastL2BlockTimestamp <= block.timestamp + COMMIT_TIMESTAMP_APPROXIMATION_DELTA, \"h2\"); // The last L2 block timestamp is too big\n    }\n\n    /// @dev Check that L2 logs are proper and batch contain all meta information for them\n    /// @dev The logs processed here should line up such that only one log for each key from the\n    ///      SystemLogKey enum in Constants.sol is processed per new batch.\n    /// @dev Data returned from here will be used to form the batch commitment.\n    function _processL2Logs(\n        CommitBatchInfo calldata _newBatch,\n        bytes32 _expectedSystemContractUpgradeTxHash\n    ) internal pure returns (LogProcessingOutput memory logOutput) {\n        // Copy L2 to L1 logs into memory.\n        bytes memory emittedL2Logs = _newBatch.systemLogs;\n\n        // Used as bitmap to set/check log processing happens exactly once.\n        // See SystemLogKey enum in Constants.sol for ordering.\n        uint256 processedLogs;\n\n        // linear traversal of the logs\n        for (uint256 i = 0; i < emittedL2Logs.length; i = i.uncheckedAdd(L2_TO_L1_LOG_SERIALIZE_SIZE)) {\n            // Extract the values to be compared to/used such as the log sender, key, and value\n            (address logSender, ) = UnsafeBytes.readAddress(emittedL2Logs, i + L2_LOG_ADDRESS_OFFSET);\n            (uint256 logKey, ) = UnsafeBytes.readUint256(emittedL2Logs, i + L2_LOG_KEY_OFFSET);\n            (bytes32 logValue, ) = UnsafeBytes.readBytes32(emittedL2Logs, i + L2_LOG_VALUE_OFFSET);\n\n            // Ensure that the log hasn't been processed already\n            require(!_checkBit(processedLogs, uint8(logKey)), \"kp\");\n            processedLogs = _setBit(processedLogs, uint8(logKey));\n\n            // Need to check that each log was sent by the correct address.\n            if (logKey == uint256(SystemLogKey.L2_TO_L1_LOGS_TREE_ROOT_KEY)) {\n                require(logSender == L2_TO_L1_MESSENGER_SYSTEM_CONTRACT_ADDR, \"lm\");\n                logOutput.l2LogsTreeRoot = logValue;\n            } else if (logKey == uint256(SystemLogKey.TOTAL_L2_TO_L1_PUBDATA_KEY)) {\n                require(logSender == L2_TO_L1_MESSENGER_SYSTEM_CONTRACT_ADDR, \"ln\");\n                logOutput.pubdataHash = logValue;\n            } else if (logKey == uint256(SystemLogKey.STATE_DIFF_HASH_KEY)) {\n                require(logSender == L2_TO_L1_MESSENGER_SYSTEM_CONTRACT_ADDR, \"lb\");\n                logOutput.stateDiffHash = logValue;\n            } else if (logKey == uint256(SystemLogKey.PACKED_BATCH_AND_L2_BLOCK_TIMESTAMP_KEY)) {\n                require(logSender == L2_SYSTEM_CONTEXT_SYSTEM_CONTRACT_ADDR, \"sc\");\n                logOutput.packedBatchAndL2BlockTimestamp = uint256(logValue);\n            } else if (logKey == uint256(SystemLogKey.PREV_BATCH_HASH_KEY)) {\n                require(logSender == L2_SYSTEM_CONTEXT_SYSTEM_CONTRACT_ADDR, \"sv\");\n                logOutput.previousBatchHash = logValue;\n            } else if (logKey == uint256(SystemLogKey.CHAINED_PRIORITY_TXN_HASH_KEY)) {\n                require(logSender == L2_BOOTLOADER_ADDRESS, \"bl\");\n                logOutput.chainedPriorityTxsHash = logValue;\n            } else if (logKey == uint256(SystemLogKey.NUMBER_OF_LAYER_1_TXS_KEY)) {\n                require(logSender == L2_BOOTLOADER_ADDRESS, \"bk\");\n                logOutput.numberOfLayer1Txs = uint256(logValue);\n            } else if (logKey == uint256(SystemLogKey.BLOB_ONE_HASH_KEY)) {\n                require(logSender == L2_PUBDATA_CHUNK_PUBLISHER_ADDR, \"pc\");\n                logOutput.blob1Hash = logValue;\n            } else if (logKey == uint256(SystemLogKey.BLOB_TWO_HASH_KEY)) {\n                require(logSender == L2_PUBDATA_CHUNK_PUBLISHER_ADDR, \"pd\");\n                logOutput.blob2Hash = logValue;\n            } else if (logKey == uint256(SystemLogKey.EXPECTED_SYSTEM_CONTRACT_UPGRADE_TX_HASH_KEY)) {\n                require(logSender == L2_BOOTLOADER_ADDRESS, \"bu\");\n                require(_expectedSystemContractUpgradeTxHash == logValue, \"ut\");\n            } else {\n                revert(\"ul\");\n            }\n        }\n\n        // We only require 9 logs to be checked, the 10th is if we are expecting a protocol upgrade\n        // Without the protocol upgrade we expect 9 logs: 2^9 - 1 = 511\n        // With the protocol upgrade we expect 8 logs: 2^10 - 1 = 1023\n        if (_expectedSystemContractUpgradeTxHash == bytes32(0)) {\n            require(processedLogs == 511, \"b7\");\n        } else {\n            require(processedLogs == 1023, \"b8\");\n        }\n    }\n\n    /// @inheritdoc IExecutor\n    function commitBatches(\n        StoredBatchInfo memory _lastCommittedBatchData,\n        CommitBatchInfo[] calldata _newBatchesData\n    ) external nonReentrant onlyValidator {\n        _commitBatches(_lastCommittedBatchData, _newBatchesData);\n    }\n\n    /// @inheritdoc IExecutor\n    function commitBatchesSharedBridge(\n        uint256, // _chainId\n        StoredBatchInfo memory _lastCommittedBatchData,\n        CommitBatchInfo[] calldata _newBatchesData\n    ) external nonReentrant onlyValidator {\n        _commitBatches(_lastCommittedBatchData, _newBatchesData);\n    }\n\n    function _commitBatches(\n        StoredBatchInfo memory _lastCommittedBatchData,\n        CommitBatchInfo[] calldata _newBatchesData\n    ) internal {\n        // check that we have the right protocol version\n        // three comments:\n        // 1. A chain has to keep their protocol version up to date, as processing a block requires the latest or previous protocol version\n        // to solve this we will need to add the feature to create batches with only the protocol upgrade tx, without any other txs.\n        // 2. A chain might become out of sync if it launches while we are in the middle of a protocol upgrade. This would mean they cannot process their genesis upgrade\n        // as thier protocolversion would be outdated, and they also cannot process the protocol upgrade tx as they have a pending upgrade.\n        // 3. The protocol upgrade is increased in the BaseZkSyncUpgrade, in the executor only the systemContractsUpgradeTxHash is checke"
    }
  ]
}